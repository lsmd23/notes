# 线性回归
- 问题举例：根据日期、温度等数据，推测每家每户的用电量
	- 需要从一组数据中，提取出关键性的特征，并根据这些数据，进行结果的预测
- 数学建模：
	- 参考[[L4 学习(A)#线性回归模型|人工智能导论——线性回归模型]]
	- 用矩阵表示这个训练误差：$$\hat{\epsilon}(\pmb w)=\sum_{i=1}^n(\pmb w^Tx_i-y_i)^2=||\pmb{Xw}-\pmb y||^2$$其中：$$\pmb X=\left[ \begin{array}{c} \pmb x_1^T\\ \pmb x_2^T \\ ... \\ \pmb x_n^T\end{array} \right], \pmb y =\left[ \begin{array}{c} y_1\\ y_2 \\ ... \\ y_n\end{array} \right]$$ 
	- 误差存在最小二乘的解析解，对其进行求导可以得到：$$\begin{align}\nabla_{\pmb w} \hat{\epsilon}(\pmb w) &= 2\pmb X^T( \pmb {Xw} - \pmb y) = 0\\ &\Rightarrow \pmb X^T\pmb{Xw} = \pmb X^T\pmb y\\ &\Rightarrow\pmb w = (\pmb X^T\pmb X)^{-1}\pmb X^T\pmb y\\\end{align}$$其复杂度为：$O(d^2(d+n))$，计算上过于复杂，因此常不关心其解析解
	- 实际学习过程中采用的是梯度下降的方法：[[L4 学习(A)#^d205b8|人工智能导论——梯度下降算法]]
## 统计视角：最大似然估计
- 参数化建模：假设存在一个参数化的概率分布：$\{p(\pmb z;\theta)|\theta\in\Theta\}$，其参数为$\theta$
	- 从已知分布依独立同分布采样的过程，即为模型的生成过程
	- 反之，已知数据，估计参数的过程，即为模型的学习过程$$\hat{\theta}=\arg \max_{\theta\in\Theta}\sum_{i=1}^n\log p(\pmb {z}_i;\theta)$$
- 统计视角的线性模型：
	- 建模似然：$$p(D;\hat{\theta})=\prod_{i=1}^np(\pmb x_i,y_i;\hat{\theta})=\prod_{i=1}^np(y_i|\pmb x_i;\hat{\theta})p(\pmb x_i;\hat{\theta})$$
	- 在**判别式模型**中，经验分布为：$p(\pmb x_i)=\frac{1}{n}$，则可以得到：$$p(D;\hat{\theta})=\prod_{i=1}^np(y_i|\pmb x_i;\hat{\theta})\frac{1}{n}∝\prod_{i=1}^np(y_i|\pmb x_i;\hat{\theta})$$
	- **高斯噪声假设**：假设$y$服从条件分布：$y\sim N(\pmb w^T\pmb x,\sigma^2)$，则线性回归模型建模的是条件期望值：$\mathbb E[y|\pmb x;\pmb w,\sigma^2]=\pmb w^T\pmb x$（也即做**线性假设**）
	- 代入，可以求得似然为：$$\log p(D_n;\pmb w,\sigma)= \frac{n}{2}\log\frac{1}{2\pi\sigma^2}-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\pmb w^T\pmb x_i)^2$$即可得到最小二乘形式的损失函数
	- 扩展：此处采用了高斯噪声假设，也即认为噪声的分布是一个高斯分布。类似的，考虑性质相近的t分布（长尾分布），也可以做对应的回归
		- 这种模型更多的考虑了和条件期望值的偏差较大的事件，需要根据实际数据进行更合适的假设
# 非线性回归
- 问题：实际问题中，变量之间往往不是简单的线性关系
- 基函数：
	- 定义：定义在样本向量$\pmb x$上的函数$\phi_j(\pmb x)$，用于对线性的结果进行扩展
	- 特征映射与空间：一组这样的函数$\Phi$将原来的$d$维样本向量映射为$\tilde{d}$维的向量$\pmb z$
		- 例：$\pmb z'=(1,x_2,x_1^2,x_1x_2)$
		- 径向基函数：以高斯函数的方式描述的基函数$$\phi_j(x)=\{\exp{(\frac{-||\pmb{x}-\pmb{\mu}_j||_2^2}{2\sigma^2}}):j=1,...,\tilde{d}\}$$描述的是一个向量在某个分量上的均值附近的集中程度
- *局部加权线性回归：*
	- 由于每个函数在微分意义下都可以视为线性的函数，利用这种思想对目标函数进行拟合
	- 定义新的损失函数：$$\hat{\epsilon}(h)=\sum_{i=1}^n{(h(\pmb{x}_i)-y_i)^2K(\frac{||\pmb x_i-\pmb x||_2}{\tau})}$$其中$K$为核函数，$\tau$为带宽参数
		- 核函数：用于描述距离的权重函数，一般取高斯核函数：$$K(u)=\exp(-\frac{u^2}{2})$$
		- 如图：![[Pasted image 20250929160527.png]]
# 正则化
- 过拟合问题：当基函数过多时，模型会过于复杂，导致训练误差很小，但测试误差很大
	- 例：![[Pasted image 20250929161103.png]]
- 正则化：在损失函数中添加正则项
	- 二范数正则化线性回归：
		- [[L4 学习(A)#^92f4ce|人工智能导论——二范数正则化]]
		- 这样的回归称为**岭回归**
	- 一范数正则化线性回归：将二范数改为一范数即可，即为绝对值之和
		- [[L4 学习(A)#^fad2a2|人工智能导论——一范数正则化]]
		- 这样的回归称为**Lasso回归（套索回归）**
	- 正则化的作用：
		- 避免过拟合
		- 避免损失函数失去全局最小值的性质
		- 一范数正则化的解具有稀疏性，可以进行**特征筛选**；二范数则可以用于做**权重衰减**以避免过拟合![[Pasted image 20250929163136.png]]
- 次梯度：
	- 当损失函数不可导时，可以使用次梯度的方式进行优化
	- 定义：函数$f$在点$\pmb x$处的次梯度$\pmb g$满足：$$f(\pmb z)\geq f(\pmb x)+\pmb g^T(\pmb z-\pmb x),\forall \pmb z$$
	- 性质：当$f$可导时，次梯度等于梯度；当$f$不可导时，次梯度不唯一，按照次梯度做梯度下降，效果完全一致
	- 应用：用于对一范数正则化后的不处处可导的损失函数做梯度下降
		- 计算：对于一范数正则化的损失函数，其次梯度为：$$\nabla \hat{\epsilon}(\pmb w)=2\sum_{i=1}^n(\pmb w^T\pmb x_i-y_i)\pmb x_i+\lambda sign(\pmb w)$$其中$sign(w_j)=\begin{cases}1&w_j>0\\ -1&w_j<0\\ [-1,1]&w_j=0\end{cases}$
- *近端梯度下降：*
	- 用于处理损失函数中包含不可导的正则项的情况
	- 思想：将损失函数分为可导部分和不可导部分，先对可导部分做梯度下降，再对不可导部分做近端映射
## 统计视角：最大后验估计
- 最大后验估计：在最大似然估计的基础上，加入了对参数的先验分布的假设
	- 形式：$$\hat{\theta}=\arg \max_{\theta\in\Theta}\sum_{i=1}^n\log p(\pmb {z}_i|\theta)+\log p(\theta)$$
	- 解释：加入了对参数的先验分布的假设，可以避免过拟合的问题
- 后验估计与正则化：
	- 线性回归的最大后验估计：假设参数$\pmb w$服从高斯分布$N(\pmb 0,\frac{\sigma^2}{\lambda}\pmb I)$的先验分布，则可以得到：$$\log p(D;\pmb w,\sigma)= \frac{n}{2}\log\frac{1}{2\pi\sigma^2}-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\pmb w^T\pmb x_i)^2-\frac{\lambda}{2\sigma^2}||\pmb w||_2^2$$
	- 由此可以看出，在损失函数中加入二范数正则项的形式等价于使用高斯分布为先验的最大后验估计
	- 类似地，假设参数$\pmb w$服从拉普拉斯分布，则可以得到一范数正则化的形式
		- 例：![[Pasted image 20250929164943.png]]
# 线性分类
