# 无模型强化学习
- 在MDP模型中，通过[[L6 强化学习(A)#规划求解MDP|规划求解]]的方法得到一个最优策略，这个过程是已知转移概率矩阵$\mathcal{P}$和奖励函数$\mathcal{R}$的情况下进行的
- 然而，在实际应用中，往往无法直接获得这些信息（MDP模型未知，或已知但空间过大难以计算），因此需要通过与环境的交互来学习最优策略，这就是无模型强化学习的方法
	- 同样有基于价值的方法和基于策略的方法两大类
- 方法：
	- 动态规划：通过动态规划的方法，将原来的遍历模拟求取价值等的操作转化成低复杂度的迭代计算
	- 蒙特卡洛方法：通过多次采样的方式，估计价值函数
	- 时间差分学习：结合动态规划和蒙特卡洛方法的优点，通过自举的方式进行学习
	- 示意图：![[Pasted image 20251208164204.png]]
# 蒙特卡洛方法
- 通过采样的方法估计价值函数、转移概率等，从而完成优化过程
- 蒙特卡洛方法（Monte Carlo Method, MC）：
	- 条件：情节式任务（episodic task），即每个模拟都有一个明确的**终止**状态
	- 采样：依照策略$\pi$，从初状态$S_0$出发，采样一系列的奖励函数和状态转移，直到到达终止状态，形成一个完整的情节：$$S_0,A_0,R_1,S_1,A_1,R_2,...,S_{T-1},A_{T-1},R_T\sim \pi$$
	- 计算期望：对这个采样的链的任意一个状态$S_t$，计算一个回报函数：$$G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...+\gamma^{T-t-1}R_T$$状态价值函数则计算为：$$v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]$$
	- 统计意义上，采样计算得到的数值是真实价值函数的**无偏估计**，但由于采样的每个过程都依赖于策略的随机性，因此其方差较大
- 蒙特卡洛预测（Monte Carlo Prediction）：
	- 目标：估计一个给定策略$\pi$的状态价值函数$v_\pi(s)$，且尽可能降低方差
	- 方法：
		1. 首次访问MC：在一个长的采样链中，对每个首次遇到的状态，都使用其后面的采样链，计算当状态的一个回报值用于估计
			- 在第$t$步，状态$s$被采样得到
			- 更新采样数$N(t)\leftarrow N(t)+1$
			- 更新回报函数估计值$G(s))\leftarrow G(s)+G_t$
			- 估计的价值函数由均值计算得到：$$V(s)= \frac{G(s)}{N(s)}$$
		2. 每次访问MC：对采样链中，每个采样的状态，都在线的更新其估计值
			- 数学基础：均值可以写成如下的动态更新形式$$\mu_k = \frac{1}{k} \sum_{j=1}^k x_j = \frac{1}{k} \left( x_k + \sum_{j=1}^{k-1} x_j \right) = \frac{1}{k} \left( x_k + (k-1)\mu_{k-1} \right) \\ = \mu_{k-1} + \frac{1}{k} \left( x_k - \mu_{k-1} \right) = \mu_{k-1} + \alpha \left( x_k - \mu_{k-1} \right)$$
			- 由此，可以得到在线的更新策略为：$V(S_t)\leftarrow V(S_t) + \alpha \left( G_t - V(S_t) \right)$，其中$\alpha$为学习率
- 蒙特卡洛规划求解：
	- 需要使用蒙特卡洛的方式，解决策略评估和策略改进两个过程的计算
	- 确定策略的蒙特卡洛规划：
		- 可以用上述的两种办法，估计出状态价值函数$v_\pi(s)$，再使用贪心的方式，用采样的动作更新价值函数$q_\pi(s,a)$，对策略进行改进
		- 问题：从$v_\pi(s)$到$q_\pi(s,a)$的计算依赖MDP的状态转移矩阵，这一过程依然依赖采样计算
		- 解决：直接通过采样，计算状态动作价值函数$q_\pi(s,a)$，从而避免对状态转移矩阵的依赖，实现完全的无模型强化学习
		- 确定策略的局限性：
			- 依赖大量的蒙特卡洛采样，实现复杂度较高
			- 每个策略下的采样都需要独立进行，无法共享采样数据，如果状态空间很大，则很难采样遍各种状态动作的轨迹
	- 基于$\epsilon$-贪心的蒙特卡洛规划：
		- 向策略引入不确定性，使其可以依靠不确定性，在采样的过程中完成学习，避免确定采样导致有的状态-动作对$(s,a)$无法被采样到的问题







# 时间差分学习

