# 无模型强化学习
- 在MDP模型中，通过[[L6 强化学习(A)#规划求解MDP|规划求解]]的方法得到一个最优策略，这个过程是已知转移概率矩阵$\mathcal{P}$和奖励函数$\mathcal{R}$的情况下进行的
- 然而，在实际应用中，往往无法直接获得这些信息（MDP模型未知，或已知但空间过大难以计算），因此需要通过与环境的交互来学习最优策略，这就是无模型强化学习的方法
	- 同样有基于价值的方法和基于策略的方法两大类
- 方法：
	- 动态规划：通过动态规划的方法，将原来的遍历模拟求取价值等的操作转化成低复杂度的迭代计算
	- 蒙特卡洛方法：通过多次采样的方式，估计价值函数
	- 时间差分学习：结合动态规划和蒙特卡洛方法的优点，通过自举的方式进行学习
	- 示意图：![[Pasted image 20251208164204.png]]
# 蒙特卡洛方法
- 通过采样的方法估计价值函数、转移概率等，从而完成优化过程
- 蒙特卡洛方法（Monte Carlo Method, MC）：
	- 条件：情节式任务（episodic task），即每个模拟都有一个明确的**终止**状态
	- 采样：依照策略$\pi$，从初状态$S_0$出发，采样一系列的奖励函数和状态转移，直到到达终止状态，形成一个完整的情节：$$S_0,A_0,R_1,S_1,A_1,R_2,...,S_{T-1},A_{T-1},R_T\sim \pi$$
	- 计算期望：对这个采样的链的任意一个状态$S_t$，计算一个回报函数：$$G_t = R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+...+\gamma^{T-t-1}R_T$$状态价值函数则计算为：$$v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]$$
	- 统计意义上，采样计算得到的数值是真实价值函数的**无偏估计**，但由于采样的每个过程都依赖于策略的随机性，因此其方差较大
- 蒙特卡洛预测（Monte Carlo Prediction）：
	- 目标：估计一个给定策略$\pi$的状态价值函数$v_\pi(s)$，且尽可能降低方差
	- 方法：
		1. 首次访问MC：在一个长的采样链中，对每个首次遇到的状态，都使用其后面的采样链，计算当状态的一个回报值用于估计
			- 在第$t$步，状态$s$被采样得到
			- 更新采样数$N(t)\leftarrow N(t)+1$
			- 更新回报函数估计值$G(s))\leftarrow G(s)+G_t$
			- 估计的价值函数由均值计算得到：$$V(s)= \frac{G(s)}{N(s)}$$
		2. 每次访问MC：对采样链中，每个采样的状态，都在线的更新其估计值
			- 数学基础：均值可以写成如下的动态更新形式$$\mu_k = \frac{1}{k} \sum_{j=1}^k x_j = \frac{1}{k} \left( x_k + \sum_{j=1}^{k-1} x_j \right) = \frac{1}{k} \left( x_k + (k-1)\mu_{k-1} \right) \\ = \mu_{k-1} + \frac{1}{k} \left( x_k - \mu_{k-1} \right) = \mu_{k-1} + \alpha \left( x_k - \mu_{k-1} \right)$$
			- 由此，可以得到在线的更新策略为：$V(S_t)\leftarrow V(S_t) + \alpha \left( G_t - V(S_t) \right)$，其中$\alpha$为学习率
- 蒙特卡洛规划求解：
	- 需要使用蒙特卡洛的方式，解决策略评估和策略改进两个过程的计算
	- 确定策略的蒙特卡洛规划：
		- 可以用上述的两种办法，估计出状态价值函数$v_\pi(s)$，再使用贪心的方式，用采样的动作更新价值函数$q_\pi(s,a)$，对策略进行改进
		- 问题：从$v_\pi(s)$到$q_\pi(s,a)$的计算依赖MDP的状态转移矩阵，这一过程依然依赖采样计算
		- 解决：直接通过采样，计算状态动作价值函数$q_\pi(s,a)$，从而避免对状态转移矩阵的依赖，实现完全的无模型强化学习
		- 确定策略的局限性：
			- 依赖大量的蒙特卡洛采样，实现复杂度较高
			- 每个策略下的采样都需要独立进行，无法共享采样数据，如果状态空间很大，则很难采样遍各种状态动作的轨迹
	- **基于$\epsilon$-贪心的蒙特卡洛规划**：
		- 向策略引入不确定性，使其可以依靠不确定性，在采样的过程中完成学习，避免确定采样导致有的状态-动作对$(s,a)$无法被采样到的问题
		- 蒙特卡洛控制：![[Pasted image 20251215152330.png]]
			- 初始化：初始化一个$\epsilon$-贪心策略$\pi$，初始化状态动作价值函数$Q(s,a)$为任意值，并初始化回报表$\text{Returns}(s,a)=\varnothing$，用于存储每个状态动作对的回报值
			- 采样一条有终点的情节：依照当前策略$\pi$，采样一条完整的情节$S_0,A_0,R_1,S_1,A_1,R_2,...,S_{T-1},A_{T-1},R_T$，直到到达终止状态
			- 对于每一步$t=T-1,T-2,\cdots,0$，计算其奖励函数$G \leftarrow \gamma G + R_{t+1}$
				- 将计算的奖励函数$G$添加到回报表项：$\text{Returns}(S_t,A_t) \leftarrow \text{Returns}(S_t,A_t) \cup \{G\}$
				- 取回报表项的平均值为该状态动作对的价值函数：$Q(S_t,A_t) \leftarrow \text{average}(\text{Returns}(S_t,A_t))$
				- 取最优动作：$A^* \leftarrow \arg\max_a Q(S_t,a)$
				- 根据$\epsilon$定义非确定策略：$$\pi(a|S_t) = \begin{cases} 1-\epsilon + \frac{\epsilon}{|\mathcal{A}(S_t)|}, & a = A^* \\ \frac{\epsilon}{|\mathcal{A}(S_t)|}, & a \neq A^* \end{cases}$$
		- 数学保证：
			- $\epsilon$-贪心策略提升定理：对任意的$\epsilon$-贪心策略$\pi$，进行一步策略改进后的策略$\pi'$，满足$v_{\pi'}(s) \geq v_\pi(s)$，即策略不会变差
				- 证明：
			- 







# 时间差分学习
## TD(0)学习
- 基本思想：基于一步的交互估计价值函数
	- 单步交互是在策略$\pi$下采样得到的四元组：$(S_t,A_t,R_{t+1},S_{t+1})$
	- 价值函数可以由贝尔曼方程，转化为和一步交互有关的定义式：$$v(s) = \mathbb{E}[G_t|S_t=s] = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1})|S_t=s]$$
	- 因此，可以用
- 实现：![[Pasted image 20251215155355.png]]
	- 
## TD($\lambda$)学习
- 基本思想：在方差和偏差之间权衡
	- N步回报：
- 实现：![[Pasted image 20251215155401.png]]
	- 
- 优化：基于均摊思想的优化
	- 问题：
	- $\lambda-\text{Return}$（$\lambda$-回报）：前向视角
		- 用均摊方法估计计算当前状态之后的多个时间步的回报
	- 资格迹（Eligibility Trace）：后向视角
		- 通过记录过去的状态访问情况，来调整当前的价值函数估计
		- 数学保证：
	- 优化后实现：![[Pasted image 20251215161638.png]]
## Sarsa算法
- 基本思想：将时间差分学习中的状态价值函数扩展到状态动作价值函数，并使用$\epsilon$-贪心策略进行学习，即可得到Sarsa算法
	- 单步Sarsa交互是在策略$\pi$下采样得到的五元组：$(S_t,A_t,R_{t+1},S_{t+1},A_{t+1})$
	- 由其推导出的贝尔曼方程具有相同的形式，因此可以使用类似的时间差分方法进行学习
- 实现：![[Pasted image 20251215162719.png]]
- 均摊N步Sarsa算法
	- 基本思想：将Sarsa算法扩展到N步的采样交互
	- 均摊优化：
		- $\text{Sarsa}(\lambda)$算法：结合资格迹的思想，对N步Sarsa进行均摊优化
			- $\lambda$-回报的计算：前向视角
			- 资格迹的计算：后向视角
		- 实现：![[Pasted image 20251215163128.png]]


## Q-学习
- 基本思想：异策略的（Off-policy）学习
	- 概述：借用另一个智能体的策略$\mu$，来学习目标策略$\pi$的价值函数。如果另一个策略本身已经是一个较好的策略，则可以更快地学习到目标策略
	- 对单步贝尔曼方程的$Q$迭代：$$Q(S_t,A_t)\leftarrow Q(S_t,A_t) + \alpha ( R_{t+1} + \gamma  Q(S_{t+1},A') - Q(S_t,A_t) )$$此时，虽然状态动作价值函数$Q(S_t,A_t)$和瞬时奖励$R_{t+1}$是由行为策略$\mu$采样得到的，但后续的动作$A'$是由目标策略$\pi$选择的
	- 一般，目标策略$\pi$是一个贪心的策略：$$\begin{align*}\pi(S_{t+1})= \arg\max_{a'} Q(S_{t+1},a') \\ R_{t+1} + \gamma  Q(S_{t+1},A') = R_{t+1} + \gamma \max_{a'} Q(S_{t+1},a') \end{align*}$$而行为策略$\mu$是一个$\epsilon$-贪心策略
- 实现：![[Pasted image 20251215164009.png]]
- 问题：Q-学习容易产生过高的估计问题
# 价值函数近似
- 问题：在状态空间和动作空间过大的情况下，无法使用表格的方式存储和更新价值函数，如果推理的过程依赖于表格的未被采样到的状态动作对，则无法进行泛化
- 解决：使用函数近似的方法，借助一个监督学习的学习器，来近似表示和更新价值函数
	- 
- **经验回放**：利用历史采样数据，进行离线的批量训练
- 深度Q-学习：![[Pasted image 20251215170010.png]]