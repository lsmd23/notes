# 无模型强化学习
- 在MDP模型中，通过[[L6 强化学习(A)#规划求解MDP|规划求解]]的方法得到一个最优策略，这个过程是知道转移概率矩阵$\mathcal{P}$和奖励函数$\mathcal{R}$的情况下进行的
- 然而，在实际应用中，往往无法直接获得这些信息（MDP模型未知，或已知但空间过大难以计算），因此需要通过与环境的交互来学习最优策略，这就是无模型强化学习的方法
	- 同样由基于价值的方法和基于策略的方法两大类
- 方法：
	- 动态规划：通过动态规划的方法，将原来的遍历模拟求取价值等的操作转化成低复杂度的迭代计算
	- 蒙特卡洛方法：通过多次采样的方式，估计价值函数
	- 时间差分学习：结合动态规划和蒙特卡洛方法的优点，通过自举的方式进行学习
	- 示意图：![[Pasted image 20251208164204.png]]
# 蒙特卡洛方法








# 时间差分学习

