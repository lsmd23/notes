# 概率近似正确(PAC)
- 统计视角的学习：
	- 样本与数据：所有的数据$(\pmb x,y)$都是统计中的一个样本，是从某个总体分布$\mathcal{D}_\mathcal{X\times Y}$中**独立同分布**采样得到的，采样得到的样本集合也即为数据集$D_n=\{(\pmb x_i,y_i):i=1,2,...,n\}$
	- 分布外的样本：在实际问题上，会有未知的不符合独立同分布的样本出现在测试集中，此时，模型的**泛化能力**就显得尤为重要
* 泛化：
	* 泛化能力：学习器在训练集之外的数据上表现良好的能力。学习不光是拟合原有的训练数据，还要有能力去预测未知的数据
	* 泛化能力差的模型就会发生过拟合现象
* 误差与偏差：
	* 期望误差：假设函数$h$在总体分布$\mathcal{D}$上的误差，定义为$$\epsilon_{\mathcal{D}}(h)=\mathbb{E}_{(\pmb x,y)\sim \mathcal{D}}[l(h(\pmb x),y)]$$也即损失函数在总体分布上的期望
		* 该误差无法直接计算，因为总体分布$\mathcal{D}$未知
	* 经验误差：假设函数$h$在数据集$D_n$上的误差，定义为$$\hat\epsilon_{D_n}(h)=\frac{1}{n}\sum_{i=1}^n l(h(\pmb x_i),y_i)$$也即损失函数在数据集上的平均值
		* 经验误差是对期望误差的无偏估计：$$\mathbb{E}_{D_n\sim \mathcal{D}^n}[\hat\epsilon_{D_n}(h)]=\epsilon_{\mathcal{D}}(h)$$
	* 贝叶斯误差：假设函数空间$\mathcal{H}$中最优的假设函数$h^*$在总体分布$\mathcal{D}$上的误差，定义为$$\epsilon^*=\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)$$
		* 贝叶斯决策函数：$h^*=\arg\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)$
		* 代入贝叶斯决策函数的期望误差即为贝叶斯误差
	* 偏差-方差分解：
		* 对于回归问题，假设损失函数为平方误差损失函数$l(h(\pmb x),y)=(h(\pmb x)-y)^2$，则假设函数$h$的期望误差可以分解为偏差和方差两部分：$$\mathbb{E}_{D_n\sim \mathcal{D}^n}[\epsilon_{\mathcal{D}}(h)]=\text{Bias}^2+\text{Variance}+\sigma^2$$
		* 其中，偏差表示假设函数$h$的期望输出与真实输出之间的差距$$\text{Bias}^2=\mathbb{E}_{\pmb x\sim \mathcal{D}_\mathcal{X}}[(\mathbb{E}_{D_n\sim \mathcal{D}^n}[h(\pmb x)]-f(\pmb x))^2]$$方差表示假设函数$h$在不同数据集上的输出变化程度，也即对自己期望的偏离程度$$\text{Variance}=\mathbb{E}_{\pmb x\sim \mathcal{D}_\mathcal{X}}[\mathbb{E}_{D_n\sim \mathcal{D}^n}[(h(\pmb x)-\mathbb{E}_{D_n\sim \mathcal{D}^n}[h(\pmb x)])^2]]$$$\sigma^2$表示数据中的噪声
	* 近似误差与估计误差：
		* 近似误差：假设函数空间$\mathcal{H}$中最优的假设函数$h^*$在总体分布$\mathcal{D}$上的误差，即贝叶斯误差$\epsilon^*$
		* 估计误差：学习算法选取的假设函数$\hat h$与最优假设函数$h^*$在总体分布$\mathcal{D}$上的误差差值$$\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)$$
		* 示意图：![[Pasted image 20251027165117.png]]
		* 期望误差可以表示为近似误差与估计误差之和：$$\mathbb{E}_{D_n\sim \mathcal{D}^n}[\epsilon_{\mathcal{D}}(\hat h)]=\epsilon^*+\mathbb{E}_{D_n\sim \mathcal{D}^n}[\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)]$$









---