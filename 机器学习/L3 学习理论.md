# 概率近似正确(PAC)
- 统计视角的学习：
	- 样本与数据：所有的数据$(\pmb x,y)$都是统计中的一个样本，是从某个总体分布$\mathcal{D}_\mathcal{X\times Y}$中**独立同分布**采样得到的，采样得到的样本集合也即为数据集$D_n=\{(\pmb x_i,y_i):i=1,2,...,n\}$
	- 分布外的样本：在实际问题上，会有未知的不符合独立同分布的样本出现在测试集中，此时，模型的**泛化能力**就显得尤为重要
* **泛化（Generalization）**：
	* 泛化能力：学习器在训练集之外的数据上表现良好的能力。学习不光是拟合原有的训练数据，还要有能力去预测未知的数据
	* 泛化能力差的模型就会发生过拟合现象
* 误差与偏差：
	* **期望误差**（Expected Error）：假设函数$h$在总体分布$\mathcal{D}$上的误差，定义为$$\epsilon_{\mathcal{D}}(h)=\mathbb{E}_{(\pmb x,y)\sim \mathcal{D}}[l(h(\pmb x),y)]$$也即损失函数在总体分布上的期望
		* 期望误差越小，模型的泛化能力越强
		* 该误差无法直接计算，因为总体分布$\mathcal{D}$未知
	* **经验误差**（Empirical Error）：假设函数$h$在数据集$D_n$上的误差，定义为$$\hat\epsilon_{D_n}(h)=\frac{1}{n}\sum_{i=1}^n l(h(\pmb x_i),y_i)$$也即损失函数在数据集上的平均值
		* 经验误差是对期望误差的无偏估计：$$\mathbb{E}_{D_n\sim \mathcal{D}^n}[\hat\epsilon_{D_n}(h)]=\epsilon_{\mathcal{D}}(h)$$
	* 贝叶斯误差：假设函数空间$\mathcal{H}$中最优的假设函数$h^*$在总体分布$\mathcal{D}$上的误差，定义为$$\epsilon^*=\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)$$
		* 贝叶斯决策函数：$h^*=\arg\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)$
		* 代入贝叶斯决策函数的期望误差即为贝叶斯误差
	* 偏差-方差分解：
		* 对于回归问题，假设损失函数为平方误差损失函数$l(h(\pmb x),y)=(h(\pmb x)-y)^2$，则假设函数$h$的期望误差可以分解为偏差和方差两部分：$$\mathbb{E}_{D_n\sim \mathcal{D}^n}[\epsilon_{\mathcal{D}}(h)]=\text{Bias}^2+\text{Variance}+\sigma^2$$
		* 其中，偏差表示假设函数$h$的期望输出与真实输出之间的差距$$\text{Bias}^2=\mathbb{E}_{\pmb x\sim \mathcal{D}_\mathcal{X}}[(\mathbb{E}_{D_n\sim \mathcal{D}^n}[h(\pmb x)]-f(\pmb x))^2]$$方差表示假设函数$h$在不同数据集上的输出变化程度，也即对自己期望的偏离程度$$\text{Variance}=\mathbb{E}_{\pmb x\sim \mathcal{D}_\mathcal{X}}[\mathbb{E}_{D_n\sim \mathcal{D}^n}[(h(\pmb x)-\mathbb{E}_{D_n\sim \mathcal{D}^n}[h(\pmb x)])^2]]$$$\sigma^2$表示数据中的噪声
	* 近似误差与估计误差：
		* 近似误差（Approximation Error）：也叫逼近误差。假设函数空间$\mathcal{H}$中最优的假设函数$h^*$在总体分布$\mathcal{D}$上的误差，即贝叶斯误差$\epsilon^*$
		* 估计误差（Estimation Error）：学习算法选取的假设函数$\hat h$与最优假设函数$h^*$在总体分布$\mathcal{D}$上的误差差值$$\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)$$
		* 示意图：![[Pasted image 20251027165117.png]]
		* 期望误差可以表示为近似误差与估计误差之和：$$\epsilon(h)-\epsilon^*(f)=[\epsilon(h)-\epsilon(h^*)]+\left[\epsilon^*(h)-\epsilon^*(f)\right]$$前一项即为估计误差，后一项即为近似误差
* 泛化界（Generalization Bound）：
	* 估计误差$\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)$（具有随机性的一个泛函），随数据集$D$的不同而不同，但可以通过概率论的方法，给出其上界的概率保证
	* **近似正确**事件：随机事件，使得估计误差不超过某个阈值$\epsilon$，即$$\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)\leq \epsilon$$所对应的事件
	* 泛化保证：保证概率不等式$$\mathbb{P}_{D_n\sim \mathcal{D}^n}\left(\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)\geq \epsilon\right)\leq \delta$$也即，以概率至少$1-\delta$，估计误差不超过$\epsilon$
- 概率近似正确（PAC）学习框架：
	- 定义：称假设空间$\mathcal{H}$是**概率近似正确可学习**的，如果存在一个学习算法$\mathcal{A}$，和一个多项式函数$\text{poly}()$，对于任意的$\epsilon>0$和$\delta>0$，对任意样本分布$\mathcal{D}$在$\mathcal{X}$上，对任意目标假设$h\in \mathcal{H}$，当样本复杂度$n$满足$n\geq\text{poly}(\frac{1}{\epsilon},\frac{1}{\delta},|\mathcal{H}|)$时，有$$\mathbb{P}_{D_n\sim \mathcal{D}^n}\left(\epsilon_{\mathcal{D}}(\hat h_{\mathcal{D}^n})-\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)\geq \epsilon\right)\leq \delta$$
		* $\hat h_{\mathcal{D}^n}$：学习算法$\mathcal{A}$在数据集$D_n$上学得的假设函数
		* $|\mathcal{H}|$：假设空间$\mathcal{H}$的容量
		* $\epsilon_{\mathcal{D}}(\hat h_{\mathcal{D}^n})$：假设函数$\hat h_{\mathcal{D}^n}$在总体分布$\mathcal{D}$上的误差，也即期望误差
		* $\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)$：假设空间$\mathcal{H}$中最优的假设函数$h^*$在总体分布$\mathcal{D}$上的误差，也即贝叶斯误差
		* 意义：当样本复杂度$n$足够大时，以概率至少$1-\delta$，学习算法$\mathcal{A}$在数据集$D_n$上学得的假设函数$\hat h_{\mathcal{D}^n}$的期望误差不超过贝叶斯误差加上$\epsilon$
	* 高效PAC学习：如果学习算法$\mathcal{A}$在数据集$D_n$上的计算复杂度也是多项式时间的，则称假设空间$\mathcal{H}$是**高效概率近似正确可学习**的
# 概率统计基础知识
- 联合界（Union Bound）：
	- 定义：对于任意事件$A_1,A_2,...,A_k$，有$$\mathbb{P}\left(\bigcup_{i=1}^k A_i\right)\leq \sum_{i=1}^k \mathbb{P}(A_i)$$
	- 说明：联合事件发生的概率不超过各个事件发生概率之和
	- 用途：并集的概率上界估算
- 逆变换（Inversion）：
	- 若$P(X \geq \epsilon) \leq f(\epsilon)$，则对任意$\delta > 0$，有概率至少$1 - \delta$满足$X \leq f^{-1}(\delta)$
	- 应用：可实现$\epsilon$和$\delta$的转换，例如在泛化误差分析中，将$P(|\hat{\mathcal{E}}_{\mathcal{D}_n}(h) - \mathcal{E}(h)| \geq \varepsilon) \leq \delta$中的误差参数和概率参数相互推导
- 琴森不等式（Jensen's Inequality）：
	- 定义：对于随机变量$X$和凸函数$\phi$，有$$\phi(\mathbb{E}[X]) \leq \mathbb{E}[\phi(X)]$$
	- 说明：凸函数作用于期望值不超过期望值作用于凸函数
	- 应用：在概率论和统计学中，用于处理随机变量的非线性变换
- 集中不等式（Concentration Inequalities）：
	- 定义：界定任意分布的随机变量与其期望值偏离程度的概率界限，也即求$P(|X - \mathbb{E}[X]| \geq \epsilon)$的上界
	- 马尔可夫不等式（Markov's Inequality）：
		- 定义：对于非负随机变量$X$和任意$\epsilon > 0$，有$$P(X \geq \epsilon) \leq \frac{\mathbb{E}[X]}{\epsilon}$$
		- 说明：随机变量大于某个值的概率不超过其期望与该值之比
	- 切比雪夫不等式（Chebyshev's Inequality）：
		- 定义：对于随机变量$X$，其期望为$\mu$，方差为$\sigma^2$，以及任意$\epsilon > 0$，有$$P(|X - \mu| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2}$$
		- 说明：随机变量偏离其期望值超过某个距离的概率不超过方差与该距离平方之比
	- 弱大数定律（Weak Law of Large Numbers）：
		- 定义：对于一组独立同分布的随机变量$X_1, X_2, ..., X_n$，其期望为$\mu$，则样本均值$\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$收敛于$\mu$，即对于任意$\epsilon > 0$，有$$P(|\bar{X}_n - \mu| \geq \epsilon) \to 0 \quad \text{当 } n \to \infty$$
		- 说明：随着样本数量增加，样本均值越来越接近总体均值
	- 切尔诺夫界（Chernoff Bound）：
		- 定义：取$\phi(t)=e^{tX}$，则对于任意$t>0$，有$$P(X \geq \epsilon) \leq \frac{\mathbb{E}[e^{tX}]}{e^{t\epsilon}}$$通过选择最优的$t$，可以得到更紧的概率界限
		- 取随机变量$V=Z-\mathbb{E}[Z]$，则对于任意$t>0$，有$$P(V \geq \epsilon) \leq \frac{\mathbb{E}[e^{tV}]}{e^{t\epsilon}}$$
		- 独立同分布随机变量：若$Z_1,Z_2,...,Z_n$为独立同分布随机变量，记$X=\sum_{i=1}^n Z_i$，则$$P(X - \mathbb{E}[X] \geq \epsilon) \leq \frac{\mathbb{E}\left[e^{t \sum_{i=1}^n (Z_i - \mathbb{E}Z_i)}\right]}{e^{t \epsilon}} = \frac{\prod_{i=1}^n \mathbb{E}\left[e^{t (Z_i - \mathbb{E}Z_i)}\right]}{e^{\lambda \epsilon}}$$
	- Hoeffding引理：设$V$是有界随机变量，满足$\mathbb{E}[V] = 0$且$a \leq V \leq b$（$b > a$），则对任意$\lambda > 0$，有： $$\mathbb{E}\left[e^{\lambda V}\right] \leq e^{\frac{\lambda^2(b-a)^2}{8}}$$
		- 证明思路：利用凸函数的性质，将$e^{\lambda V}$表示为端点处值的凸组合，然后取期望并应用Jensen不等式，最后通过中值定理得到结果
	- Hoeffding不等式（Hoeffding's Inequality）：
		- 定义：设$X_1,X_2,...,X_n$为独立随机变量，且每个变量均有界，即$X_i$的取值范围为$[a_i,b_i]$，记$V_i=X_i-\mathbb{E}[X_i]$，则对于任意$\epsilon > 0$，有$$P\left(\sum_{i=1}^n V_i \geq \epsilon\right) \leq \exp\left(-\frac{2\epsilon^2}{\sum_{i=1}^n (b_i - a_i)^2}\right)$$
		- 定理：设$X_1, X_2, \dots, X_n \in \mathcal{X}$是独立随机变量，函数$f: \mathcal{X}^n \to \mathbb{R}$对输入变化具有“稳定性”：即对所有$i \in [1,n]$，满足 $$\sup_{x_1,x_2,\dots,x_n,x_i'} |f(x_1, \dots, x_i, \dots, x_n) - f(x_1, \dots, x_i', \dots, x_n)| \leq c_i$$ 则对任意$\varepsilon > 0$，有 $$P\left(|f(X_1, \dots, X_n) - \mathbb{E}f(X_1, \dots, X_n)| \geq \varepsilon\right) \leq 2 \exp\left(-\frac{2\varepsilon^2}{\sum_{i=1}^n c_i^2}\right)$$
			- 说明：该不等式表明，如果一个函数对每个输入变量的变化都不敏感，那么该函数的输出值集中在其期望值附近
			- 联系：当$f$为样本均值函数时，McDiarmid不等式（也即上式）退化为Hoeffding不等式
# 有限假设空间的泛化界
- 泛化界的推导：
	- 单假设泛化界：
		- 随机变量$X_i=\ell(h(\pmb x_i),y_i)$，则不等式左侧即为$n(\hat\varepsilon(h)-\varepsilon(h))$，也即估计误差和期望误差的差值
		- 若采用0-1损失函数，则$\ell(h(\pmb x_i),y_i)\in[0,1]$，则$b_i-a_i=1$，则有$$P(\hat\varepsilon(h)-\varepsilon(h) \geq \epsilon) \leq 2e^{-2n\epsilon^2}$$
		- 逆变换：对任意$\delta>0$，$\delta = P(\hat\varepsilon(h)-\varepsilon(h) \geq \epsilon) \leq 2e^{-2n\epsilon^2}$，则$$\epsilon = \sqrt{\frac{1}{2n}\ln\frac{2}{\delta}}$$即以概率至少$1-\delta$，有$$\varepsilon(h) \leq \hat\varepsilon(h) + \sqrt{\frac{1}{2n}\ln\frac{2}{\delta}}$$
	- 多假设泛化界：
		- 学习算法实际上是在样本集空间$\mathcal{D}^n$上映射到假设空间$\mathcal{H}$上的映射，因此需要对所有假设函数同时成立，也即推导**一致界**：$$P\left(\exists h \in \mathcal{H}, \hat\varepsilon(h)-\varepsilon(h) \geq \epsilon\right)$$
		- 有限假设空间的一致泛化界定理：
			- 内容：若$\mathcal{H}$是有限假设空间（$|\mathcal{H}| < \infty$），则对任意$\delta > 0$，以概率至少$1 - \delta$，对所有$h \in \mathcal{H}$，有： $$\mathcal{E}(h) \leq \hat{\mathcal{E}}_{\mathcal{D}_n}(h) + \sqrt{\frac{\log|\mathcal{H}| + \log\frac{2}{\delta}}{2n}}$$
			- 证明：取反事件，利用事件本身的定义，对假设空间中的每个假设函数应用单假设泛化界，然后利用联合界得到一致泛化界
		- 说明：
			- 一致泛化界定理的示意图：![[Pasted image 20251103170004.png]]
			- 误差上界和样本复杂度呈现$O(\frac{1}{\sqrt{n}})$的收敛速度
			- 误差上界和假设空间的容量呈现对数关系，也即编码假设的比特数





---