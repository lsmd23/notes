# 概率近似正确(PAC)
- 统计视角的学习：
	- 样本与数据：所有的数据$(\pmb x,y)$都是统计中的一个样本，是从某个总体分布$\mathcal{D}_\mathcal{X\times Y}$中**独立同分布**采样得到的，采样得到的样本集合也即为数据集$D_n=\{(\pmb x_i,y_i):i=1,2,...,n\}$
	- 分布外的样本：在实际问题上，会有未知的不符合独立同分布的样本出现在测试集中，此时，模型的**泛化能力**就显得尤为重要
* **泛化（Generalization）**：
	* 泛化能力：学习器在训练集之外的数据上表现良好的能力。学习不光是拟合原有的训练数据，还要有能力去预测未知的数据
	* 泛化能力差的模型就会发生过拟合现象
* 误差与偏差：
	* **期望误差**（Expected Error）：假设函数$h$在总体分布$\mathcal{D}$上的误差，定义为$$\epsilon_{\mathcal{D}}(h)=\mathbb{E}_{(\pmb x,y)\sim \mathcal{D}}[l(h(\pmb x),y)]$$也即损失函数在总体分布上的期望
		* 期望误差越小，模型的泛化能力越强
		* 该误差无法直接计算，因为总体分布$\mathcal{D}$未知
	* **经验误差**（Empirical Error）：假设函数$h$在数据集$D_n$上的误差，定义为$$\hat\epsilon_{D_n}(h)=\frac{1}{n}\sum_{i=1}^n l(h(\pmb x_i),y_i)$$也即损失函数在数据集上的平均值
		* 经验误差是对期望误差的无偏估计：$$\mathbb{E}_{D_n\sim \mathcal{D}^n}[\hat\epsilon_{D_n}(h)]=\epsilon_{\mathcal{D}}(h)$$
	* 贝叶斯误差：假设函数空间$\mathcal{H}$中最优的假设函数$h^*$在总体分布$\mathcal{D}$上的误差，定义为$$\epsilon^*=\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)$$
		* 贝叶斯决策函数：$h^*=\arg\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)$
		* 代入贝叶斯决策函数的期望误差即为贝叶斯误差
	* 偏差-方差分解：
		* 对于回归问题，假设损失函数为平方误差损失函数$l(h(\pmb x),y)=(h(\pmb x)-y)^2$，则假设函数$h$的期望误差可以分解为偏差和方差两部分：$$\mathbb{E}_{D_n\sim \mathcal{D}^n}[\epsilon_{\mathcal{D}}(h)]=\text{Bias}^2+\text{Variance}+\sigma^2$$
		* 其中，偏差表示假设函数$h$的期望输出与真实输出之间的差距$$\text{Bias}^2=\mathbb{E}_{\pmb x\sim \mathcal{D}_\mathcal{X}}[(\mathbb{E}_{D_n\sim \mathcal{D}^n}[h(\pmb x)]-f(\pmb x))^2]$$方差表示假设函数$h$在不同数据集上的输出变化程度，也即对自己期望的偏离程度$$\text{Variance}=\mathbb{E}_{\pmb x\sim \mathcal{D}_\mathcal{X}}[\mathbb{E}_{D_n\sim \mathcal{D}^n}[(h(\pmb x)-\mathbb{E}_{D_n\sim \mathcal{D}^n}[h(\pmb x)])^2]]$$$\sigma^2$表示数据中的噪声
	* 近似误差与估计误差：
		* 近似误差（Approximation Error）：也叫逼近误差。假设函数空间$\mathcal{H}$中最优的假设函数$h^*$在总体分布$\mathcal{D}$上的误差，即贝叶斯误差$\epsilon^*$
		* 估计误差（Estimation Error）：学习算法选取的假设函数$\hat h$与最优假设函数$h^*$在总体分布$\mathcal{D}$上的误差差值$$\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)$$
		* 示意图：![[Pasted image 20251027165117.png]]
		* 期望误差可以表示为近似误差与估计误差之和：$$\epsilon(h)-\epsilon^*(f)=[\epsilon(h)-\epsilon(h^*)]+\left[\epsilon^*(h)-\epsilon^*(f)\right]$$前一项即为估计误差，后一项即为近似误差
* 泛化界（Generalization Bound）：
	* 估计误差$\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)$（具有随机性的一个泛函），随数据集$D$的不同而不同，但可以通过概率论的方法，给出其上界的概率保证
	* **近似正确**事件：随机事件，使得估计误差不超过某个阈值$\epsilon$，即$$\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)\leq \epsilon$$所对应的事件
	* 泛化保证：保证概率不等式$$\mathbb{P}_{D_n\sim \mathcal{D}^n}\left(\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)\geq \epsilon\right)\leq \delta$$也即，以概率至少$1-\delta$，估计误差不超过$\epsilon$
- 概率近似正确（PAC）学习框架：
	- 定义：称假设空间$\mathcal{H}$是**概率近似正确可学习**的，如果存在一个学习算法$\mathcal{A}$，和一个多项式函数$\text{poly}()$，对于任意的$\epsilon>0$和$\delta>0$，对任意样本分布$\mathcal{D}$在$\mathcal{X}$上，对任意目标假设$h\in \mathcal{H}$，当样本复杂度$n$满足$n\geq\text{poly}(\frac{1}{\epsilon},\frac{1}{\delta},|\mathcal{H}|)$时，有$$\mathbb{P}_{D_n\sim \mathcal{D}^n}\left(\epsilon_{\mathcal{D}}(\hat h_{\mathcal{D}^n})-\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)\geq \epsilon\right)\leq \delta$$
		* $\hat h_{\mathcal{D}^n}$：学习算法$\mathcal{A}$在数据集$D_n$上学得的假设函数
		* $|\mathcal{H}|$：假设空间$\mathcal{H}$的容量
		* $\epsilon_{\mathcal{D}}(\hat h_{\mathcal{D}^n})$：假设函数$\hat h_{\mathcal{D}^n}$在总体分布$\mathcal{D}$上的误差，也即期望误差
		* $\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)$：假设空间$\mathcal{H}$中最优的假设函数$h^*$在总体分布$\mathcal{D}$上的误差，也即贝叶斯误差
		* 意义：当样本复杂度$n$足够大时，以概率至少$1-\delta$，学习算法$\mathcal{A}$在数据集$D_n$上学得的假设函数$\hat h_{\mathcal{D}^n}$的期望误差不超过贝叶斯误差加上$\epsilon$
	* 高效PAC学习：如果学习算法$\mathcal{A}$在数据集$D_n$上的计算复杂度也是多项式时间的，则称假设空间$\mathcal{H}$是**高效概率近似正确可学习**的
# 概率统计基础知识
- 联合界（Union Bound）：
	- 定义：对于任意事件$A_1,A_2,...,A_k$，有$$\mathbb{P}\left(\bigcup_{i=1}^k A_i\right)\leq \sum_{i=1}^k \mathbb{P}(A_i)$$
	- 说明：联合事件发生的概率不超过各个事件发生概率之和
	- 用途：并集的概率上界估算
- 逆变换（Inversion）：
	- 若$P(X \geq \epsilon) \leq f(\epsilon)$，则对任意$\delta > 0$，有概率至少$1 - \delta$满足$X \leq f^{-1}(\delta)$
	- 应用：可实现$\epsilon$和$\delta$的转换，例如在泛化误差分析中，将$P(|\hat{\mathcal{E}}_{\mathcal{D}_n}(h) - \mathcal{E}(h)| \geq \varepsilon) \leq \delta$中的误差参数和概率参数相互推导
- 琴森不等式（Jensen's Inequality）：
	- 定义：对于随机变量$X$和凸函数$\phi$，有$$\phi(\mathbb{E}[X]) \leq \mathbb{E}[\phi(X)]$$
	- 说明：凸函数作用于期望值不超过期望值作用于凸函数
	- 应用：在概率论和统计学中，用于处理随机变量的非线性变换
- 集中不等式（Concentration Inequalities）：
	- 定义：界定任意分布的随机变量与其期望值偏离程度的概率界限，也即求$P(|X - \mathbb{E}[X]| \geq \epsilon)$的上界
	- 马尔可夫不等式（Markov's Inequality）：
		- 定义：对于非负随机变量$X$和任意$\epsilon > 0$，有$$P(X \geq \epsilon) \leq \frac{\mathbb{E}[X]}{\epsilon}$$
		- 说明：随机变量大于某个值的概率不超过其期望与该值之比
	- 切比雪夫不等式（Chebyshev's Inequality）：
		- 定义：对于随机变量$X$，其期望为$\mu$，方差为$\sigma^2$，以及任意$\epsilon > 0$，有$$P(|X - \mu| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2}$$
		- 说明：随机变量偏离其期望值超过某个距离的概率不超过方差与该距离平方之比
	- 弱大数定律（Weak Law of Large Numbers）：
		- 定义：对于一组独立同分布的随机变量$X_1, X_2, ..., X_n$，其期望为$\mu$，则样本均值$\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$收敛于$\mu$，即对于任意$\epsilon > 0$，有$$P(|\bar{X}_n - \mu| \geq \epsilon) \to 0 \quad \text{当 } n \to \infty$$
		- 说明：随着样本数量增加，样本均值越来越接近总体均值






---