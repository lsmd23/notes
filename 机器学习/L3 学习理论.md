# 概率近似正确(PAC)
- 统计视角的学习：
	- 样本与数据：所有的数据$(\pmb x,y)$都是统计中的一个样本，是从某个总体分布$\mathcal{D}_\mathcal{X\times Y}$中**独立同分布**采样得到的，采样得到的样本集合也即为数据集$D_n=\{(\pmb x_i,y_i):i=1,2,...,n\}$
	- 分布外的样本：在实际问题上，会有未知的不符合独立同分布的样本出现在测试集中，此时，模型的**泛化能力**就显得尤为重要
* **泛化（Generalization）**：
	* 泛化能力：学习器在训练集之外的数据上表现良好的能力。学习不光是拟合原有的训练数据，还要有能力去预测未知的数据
	* 泛化能力差的模型就会发生过拟合现象
* 误差与偏差：
	* **期望误差**（Expected Error）：假设函数$h$在总体分布$\mathcal{D}$上的误差，定义为$$\epsilon_{\mathcal{D}}(h)=\mathbb{E}_{(\pmb x,y)\sim \mathcal{D}}[l(h(\pmb x),y)]$$也即损失函数在总体分布上的期望
		* 期望误差越小，模型的泛化能力越强
		* 该误差无法直接计算，因为总体分布$\mathcal{D}$未知
	* **经验误差**（Empirical Error）：假设函数$h$在数据集$D_n$上的误差，定义为$$\hat\epsilon_{D_n}(h)=\frac{1}{n}\sum_{i=1}^n l(h(\pmb x_i),y_i)$$也即损失函数在数据集上的平均值
		* 经验误差是对期望误差的无偏估计：$$\mathbb{E}_{D_n\sim \mathcal{D}^n}[\hat\epsilon_{D_n}(h)]=\epsilon_{\mathcal{D}}(h)$$
	* 贝叶斯误差：假设函数空间$\mathcal{H}$中最优的假设函数$h^*$在总体分布$\mathcal{D}$上的误差，定义为$$\epsilon^*=\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)$$
		* 贝叶斯决策函数：$h^*=\arg\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)$
		* 代入贝叶斯决策函数的期望误差即为贝叶斯误差
	* 偏差-方差分解：
		* 对于回归问题，假设损失函数为平方误差损失函数$l(h(\pmb x),y)=(h(\pmb x)-y)^2$，则假设函数$h$的期望误差可以分解为偏差和方差两部分：$$\mathbb{E}_{D_n\sim \mathcal{D}^n}[\epsilon_{\mathcal{D}}(h)]=\text{Bias}^2+\text{Variance}+\sigma^2$$
		* 其中，偏差表示假设函数$h$的期望输出与真实输出之间的差距$$\text{Bias}^2=\mathbb{E}_{\pmb x\sim \mathcal{D}_\mathcal{X}}[(\mathbb{E}_{D_n\sim \mathcal{D}^n}[h(\pmb x)]-f(\pmb x))^2]$$方差表示假设函数$h$在不同数据集上的输出变化程度，也即对自己期望的偏离程度$$\text{Variance}=\mathbb{E}_{\pmb x\sim \mathcal{D}_\mathcal{X}}[\mathbb{E}_{D_n\sim \mathcal{D}^n}[(h(\pmb x)-\mathbb{E}_{D_n\sim \mathcal{D}^n}[h(\pmb x)])^2]]$$$\sigma^2$表示数据中的噪声
	* 近似误差与估计误差：
		* 近似误差（Approximation Error）：也叫逼近误差。假设函数空间$\mathcal{H}$中最优的假设函数$h^*$在总体分布$\mathcal{D}$上的误差，即贝叶斯误差$\epsilon^*$
		* 估计误差（Estimation Error）：学习算法选取的假设函数$\hat h$与最优假设函数$h^*$在总体分布$\mathcal{D}$上的误差差值$$\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)$$
		* 示意图：![[Pasted image 20251027165117.png]]
		* 期望误差可以表示为近似误差与估计误差之和：$$\epsilon(h)-\epsilon^*(f)=[\epsilon(h)-\epsilon(h^*)]+\left[\epsilon^*(h)-\epsilon^*(f)\right]$$前一项即为估计误差，后一项即为近似误差
* 泛化界（Generalization Bound）：
	* 估计误差$\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)$（具有随机性的一个泛函），随数据集$D$的不同而不同，但可以通过概率论的方法，给出其上界的概率保证
	* **近似正确**事件：随机事件，使得估计误差不超过某个阈值$\epsilon$，即$$\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)\leq \epsilon$$所对应的事件
	* 泛化保证：保证概率不等式$$\mathbb{P}_{D_n\sim \mathcal{D}^n}\left(\epsilon_{\mathcal{D}}(\hat h)-\epsilon_{\mathcal{D}}(h^*)\geq \epsilon\right)\leq \delta$$也即，以概率至少$1-\delta$，估计误差不超过$\epsilon$
- 概率近似正确（PAC）学习框架：
	- 定义：称假设空间$\mathcal{H}$是**概率近似正确可学习**的，如果存在一个学习算法$\mathcal{A}$，和一个多项式函数$\text{poly}()$，对于任意的$\epsilon>0$和$\delta>0$，任意在$\mathcal{X}$上的样本分布$\mathcal{D}$，任意目标假设$h\in \mathcal{H}$，当样本复杂度$n$满足$n\geq\text{poly}(\frac{1}{\epsilon},\frac{1}{\delta},|\mathcal{H}|)$时，有$$\mathbb{P}_{D_n\sim \mathcal{D}^n}\left(\epsilon_{\mathcal{D}}(\hat h_{\mathcal{D}^n})-\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)\geq \epsilon\right)\leq \delta$$
		* $\hat h_{\mathcal{D}^n}$：学习算法$\mathcal{A}$在数据集$D_n$上学得的假设函数
		* $|\mathcal{H}|$：假设空间$\mathcal{H}$的容量
		* $\epsilon_{\mathcal{D}}(\hat h_{\mathcal{D}^n})$：假设函数$\hat h_{\mathcal{D}^n}$在总体分布$\mathcal{D}$上的误差，也即期望误差
		* $\min_{h\in \mathcal{H}}\epsilon_{\mathcal{D}}(h)$：假设空间$\mathcal{H}$中最优的假设函数$h^*$在总体分布$\mathcal{D}$上的误差，也即贝叶斯误差
		* 意义：当样本复杂度$n$足够大时，以概率至少$1-\delta$，学习算法$\mathcal{A}$在数据集$D_n$上学得的假设函数$\hat h_{\mathcal{D}^n}$的期望误差不超过贝叶斯误差加上$\epsilon$
	* 高效PAC学习：如果学习算法$\mathcal{A}$在数据集$D_n$上的计算复杂度也是多项式时间的，则称假设空间$\mathcal{H}$是**高效概率近似正确可学习**的
# 概率统计基础知识
- 联合界（Union Bound）：
	- 定义：对于任意事件$A_1,A_2,...,A_k$，有$$\mathbb{P}\left(\bigcup_{i=1}^k A_i\right)\leq \sum_{i=1}^k \mathbb{P}(A_i)$$
	- 说明：联合事件发生的概率不超过各个事件发生概率之和
	- 用途：并集的概率上界估算
- 逆变换（Inversion）：
	- 若$P(X \geq \epsilon) \leq f(\epsilon)$，则对任意$\delta > 0$，有概率至少$1 - \delta$满足$X \leq f^{-1}(\delta)$
	- 应用：可实现$\epsilon$和$\delta$的转换，例如在泛化误差分析中，将$P(|\hat{\mathcal{E}}_{\mathcal{D}_n}(h) - \mathcal{E}(h)| \geq \varepsilon) \leq \delta$中的误差参数和概率参数相互推导
- 琴森不等式（Jensen's Inequality）：
	- 定义：对于随机变量$X$和凸函数$\phi$，有$$\phi(\mathbb{E}[X]) \leq \mathbb{E}[\phi(X)]$$
	- 说明：凸函数作用于期望值不超过期望值作用于凸函数
	- 应用：在概率论和统计学中，用于处理随机变量的非线性变换
- 集中不等式（Concentration Inequalities）：
	- 定义：界定任意分布的随机变量与其期望值偏离程度的概率界限，也即求$P(|X - \mathbb{E}[X]| \geq \epsilon)$的上界
	- 马尔可夫不等式（Markov's Inequality）：
		- 定义：对于非负随机变量$X$和任意$\epsilon > 0$，有$$P(X \geq \epsilon) \leq \frac{\mathbb{E}[X]}{\epsilon}$$
		- 说明：随机变量大于某个值的概率不超过其期望与该值之比
	- 切比雪夫不等式（Chebyshev's Inequality）：
		- 定义：对于随机变量$X$，其期望为$\mu$，方差为$\sigma^2$，以及任意$\epsilon > 0$，有$$P(|X - \mu| \geq \epsilon) \leq \frac{\sigma^2}{\epsilon^2}$$
		- 说明：随机变量偏离其期望值超过某个距离的概率不超过方差与该距离平方之比
	- 弱大数定律（Weak Law of Large Numbers）：
		- 定义：对于一组独立同分布的随机变量$X_1, X_2, ..., X_n$，其期望为$\mu$，则样本均值$\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i$收敛于$\mu$，即对于任意$\epsilon > 0$，有$$P(|\bar{X}_n - \mu| \geq \epsilon) \to 0 \quad \text{当 } n \to \infty$$
		- 说明：随着样本数量增加，样本均值越来越接近总体均值
	- 切尔诺夫界（Chernoff Bound）：
		- 定义：取$\phi(t)=e^{tX}$，则对于任意$t>0$，有$$P(X \geq \epsilon) \leq \frac{\mathbb{E}[e^{tX}]}{e^{t\epsilon}}$$通过选择最优的$t$，可以得到更紧的概率界限
		- 取随机变量$V=Z-\mathbb{E}[Z]$，则对于任意$t>0$，有$$P(V \geq \epsilon) \leq \frac{\mathbb{E}[e^{tV}]}{e^{t\epsilon}}$$
		- 独立同分布随机变量：若$Z_1,Z_2,...,Z_n$为独立同分布随机变量，记$X=\sum_{i=1}^n Z_i$，则$$P(X - \mathbb{E}[X] \geq \epsilon) \leq \frac{\mathbb{E}\left[e^{t \sum_{i=1}^n (Z_i - \mathbb{E}Z_i)}\right]}{e^{t \epsilon}} = \frac{\prod_{i=1}^n \mathbb{E}\left[e^{t (Z_i - \mathbb{E}Z_i)}\right]}{e^{\lambda \epsilon}}$$
	- Hoeffding引理：设$V$是有界随机变量，满足$\mathbb{E}[V] = 0$且$a \leq V \leq b$（$b > a$），则对任意$\lambda > 0$，有： $$\mathbb{E}\left[e^{\lambda V}\right] \leq e^{\frac{\lambda^2(b-a)^2}{8}}$$
		- 证明思路：利用凸函数的性质，将$e^{\lambda V}$表示为端点处值的凸组合，然后取期望并应用Jensen不等式，最后通过中值定理得到结果
	- Hoeffding不等式（Hoeffding's Inequality）： ^0fddb4
		- 定理：设$X_1,X_2,...,X_n$为独立随机变量，且每个变量均有界，即$X_i$的取值范围为$[a_i,b_i]$，记$V_i=X_i-\mathbb{E}[X_i]$，则对于任意$\epsilon > 0$，有$$P\left(\left|\sum_{i=1}^n V_i \right|\geq \epsilon\right) \leq 2\exp\left(-\frac{2\epsilon^2}{\sum_{i=1}^n (b_i - a_i)^2}\right)$$
	- McDiarmid不等式（McDiarmid's Inequality）： ^e3f1a7
		- 定理：设$X_1, X_2, \dots, X_n \in \mathcal{X}$是独立随机变量，函数$f: \mathcal{X}^n \to \mathbb{R}$对输入变化具有“稳定性”：即对所有$i \in [1,n]$，满足 $$\sup_{x_1,x_2,\dots,x_n,x_i'} |f(x_1, \dots, x_i, \dots, x_n) - f(x_1, \dots, x_i', \dots, x_n)| \leq c_i$$ 则对任意$\varepsilon > 0$，有 $$P\left(|f(X_1, \dots, X_n) - \mathbb{E}f(X_1, \dots, X_n)| \geq \varepsilon\right) \leq 2 \exp\left(-\frac{2\varepsilon^2}{\sum_{i=1}^n c_i^2}\right)$$
			- 说明：该不等式表明，如果一个函数对每个输入变量的变化都不敏感，那么该函数的输出值集中在其期望值附近
			- 联系：当$f$为样本均值函数时，McDiarmid不等式（也即上式）退化为Hoeffding不等式
# 有限假设空间的泛化界
- 泛化界的推导：
	- 单假设泛化界：
		- 随机变量$X_i=\ell(h(\pmb x_i),y_i)$，则不等式左侧即为$n(\hat\varepsilon(h)-\varepsilon(h))$，也即估计误差和期望误差的差值
		- 若采用0-1损失函数，则$\ell(h(\pmb x_i),y_i)\in[0,1]$，则$b_i-a_i=1$，则有$$P(\hat\varepsilon(h)-\varepsilon(h) \geq \epsilon) \leq 2e^{-2n\epsilon^2}$$
		- 逆变换：对任意$\delta>0$，$\delta = P(\hat\varepsilon(h)-\varepsilon(h) \geq \epsilon) \leq 2e^{-2n\epsilon^2}$，则$$\epsilon = \sqrt{\frac{1}{2n}\ln\frac{2}{\delta}}$$即以概率至少$1-\delta$，有$$\varepsilon(h) \leq \hat\varepsilon(h) + \sqrt{\frac{1}{2n}\ln\frac{2}{\delta}}$$
	- 多假设泛化界：
		- 学习算法实际上是在样本集空间$\mathcal{D}^n$上映射到假设空间$\mathcal{H}$上的映射，因此需要对所有假设函数同时成立，也即推导**一致界**：$$P\left(\exists h \in \mathcal{H}, \hat\varepsilon(h)-\varepsilon(h) \geq \epsilon\right)=P\left(\sup_{h \in \mathcal{H}}[\hat\varepsilon(h)-\varepsilon(h)] \geq \epsilon\right)$$
		- 有限假设空间的一致泛化界定理：
			- 内容：若$\mathcal{H}$是有限假设空间（$|\mathcal{H}| < \infty$），则对任意$\delta > 0$，以概率至少$1 - \delta$，对所有$h \in \mathcal{H}$，有： $$\mathcal{E}(h) \leq \hat{\mathcal{E}}_{\mathcal{D}_n}(h) + \sqrt{\frac{\log|\mathcal{H}| + \log\frac{2}{\delta}}{2n}}$$
			- 证明：取反事件，利用事件本身的定义，对假设空间中的每个假设函数应用单假设泛化界，然后利用联合界得到一致泛化界
		- 说明：
			- 一致泛化界定理的示意图：![[Pasted image 20251103170004.png]]
			- 误差上界和样本复杂度呈现$O(\frac{1}{\sqrt{n}})$的收敛速度
			- 误差上界和假设空间的容量呈现对数关系，也即编码假设的比特数
			- 奥卡姆剃刀原理（Occam's Razor）：在多个假设函数都能很好拟合训练数据的情况下，选择假设空间容量更小的假设函数，通常能获得更好的泛化能力
- 离散化技巧：
	- 计算机中的数值表示是有限精度的，因此可以将连续参数离散化为有限个取值
	- 假设某个64位的机器，其可以表示$2^{64}$个不同的实数值。若假设空间由$d$个实数参数组成，则离散化后的假设空间容量为$|\mathcal{H}| = (2^{64})^d = 2^{64d}$
	- 应用上述的泛化界，则有$$\mathcal{E}(h) \leq \hat{\mathcal{E}}_{\mathcal{D}_n}(h) + \sqrt{\frac{64d + \log\frac{2}{\delta}}{2n}}$$也即，以概率至少$1-\delta$，对所有$h \in \mathcal{H}$，有上述不等式成立
- 泛化界定理与PAC可学习性：
	- 定理：**若假设空间$\mathcal{H}$是有限假设空间（$|\mathcal{H}| < \infty$），则$\mathcal{H}$是概率近似正确可学习的**
	- 证明：
		- 学习算法$\mathcal{A}:\mathcal{D}^n \to h_{\mathcal{D}^n} = \arg\min_{h \in \mathcal{H}} \hat{\mathcal{E}}_{\mathcal{D}^n}(h)$
		- 分解误差：$$\mathcal{E}(h_{\mathcal{D}^n}) - \min_{h \in \mathcal{H}} \mathcal{E}(h) = \mathcal{E}(h_{\mathcal{D}^n}) -\mathcal{E}(h^*)=[\mathcal{E}(h_{\mathcal{D}^n}) - \hat{\mathcal{E}}_{\mathcal{D}^n}(h_{\mathcal{D}^n})] + [\hat{\mathcal{E}}_{\mathcal{D}^n}(h_{\mathcal{D}^n}) - \mathcal{E}(h^*)]$$
		- 由于$\hat{\mathcal{E}}_{\mathcal{D}^n}(h_{\mathcal{D}^n}) \leq \hat{\mathcal{E}}_{\mathcal{D}^n}(h^*)$（因为$h_{\mathcal{D}^n}$是经验误差最小的假设函数），有：$$\mathcal{E}(h_{\mathcal{D}^n}) - \mathcal{E}(h^*) \leq [\mathcal{E}(h_{\mathcal{D}^n}) - \hat{\mathcal{E}}_{\mathcal{D}^n}(h_{\mathcal{D}^n})] + [\hat{\mathcal{E}}_{\mathcal{D}^n}(h^*) - \mathcal{E}(h^*)]$$
		- 加上绝对值，求上确界，可得：$$\mathcal{E}(h_{\mathcal{D}^n}) - \mathcal{E}(h^*) \leq 2 \sup_{h \in \mathcal{H}} |\mathcal{E}(h) - \hat{\mathcal{E}}_{\mathcal{D}^n}(h)|$$
		- 代入有限假设空间的一致泛化界定理，有：$$P\left(\mathcal{E}(h_{\mathcal{D}^n}) - \mathcal{E}(h^*) \geq \varepsilon\right) \leq P\left( \sup_{h \in \mathcal{H}} |\mathcal{E}(h) - \hat{\mathcal{E}}_{\mathcal{D}^n}(h)| \geq \frac{\varepsilon}{2}\right) =2|\mathcal{H}|e^{-\frac{n\varepsilon^2}{2}}=\delta$$
		- 也即，当样本复杂度$n$满足$n \geq \frac{2}{\varepsilon^2} \log \frac{2|\mathcal{H}|}{\delta}$时，有$$P\left(\mathcal{E}(h_{\mathcal{D}^n}) - \mathcal{E}(h^*) \geq \varepsilon\right) \leq \delta$$
# 无限假设空间的泛化界
- 在实际问题中，假设空间通常是无限的，例如线性分类器的假设空间$\mathcal{H}=\{\pmb w\cdot \pmb x + b: \pmb w \in \mathbb{R}^d, b \in \mathbb{R}\}$。但无限假设空间之间也有复杂度差异，因此，有某种方式度量无限假设空间的复杂度
- 拟合能力的衡量：
	- 对于二分类任务，考虑一组不带标签的数据集$n$
	- 考虑所有的可能的标签形式（共有$2^n$种），每种标签形式称为一个**对分**（dichotomy）
	- 若空间$\mathcal{H}$总能通过空间中的某个假设函数对每一种标签形式都能正确分类，称这个数据集被假设空间$\mathcal{H}$**打散**（shattered）
## 拉德马赫复杂度(Radhamacher Complexity)
- 背景：
	- 考虑一个可以被二元分类的数据集$\{\pmb x_i,y_i\}$，其中$y_i \in \{-1, +1\}$。此时，假设函数$h$的输出可以表示为$h(\pmb x_i) \in \{-1, +1\}$
	- 使用0-1损失函数，则其经验误差可以表示为$$\hat\varepsilon_{D_n}(h)=\frac{1}{n}\sum_{i=1}^n \pmb{1}(h(\pmb x_i) \neq y_i)=\frac{1}{n}\sum_{i=1}^n \frac{1 - h(\pmb x_i)y_i}{2}=\frac{1}{2} - \frac{1}{2n}\sum_{i=1}^n h(\pmb x_i)y_i$$
		- 显然，对一个可以完美分类的假设函数$h$，有$\hat\varepsilon_{D_n}(h)=0$，也即$\sum_{i=1}^n h(\pmb x_i)y_i=n$
		- 因此，若某个假设空间中存在可以完美分类数据集的假设函数$h$，则有$$\sup_{h \in \mathcal{H}} \frac{1}{n}\sum_{i=1}^n h(\pmb x_i)y_i = 1$$可见，这个上确界可以反映一个假设空间对某组带标签数据的拟合能力
	- 对任意一组无标签的数据（大小为$n$），为其打上任意的标签，则标签的可能情况有$2^n$种。考虑在这$2^n$种标签下，上述的上确界的某种“平均值”，即可以反映假设空间对某个大小的数据集的拟合能力（无论标签如何）
- 定义：
	- $\mathcal{G}$为数域$\mathcal{Z}$到$[0,1]$的函数族，拉德马赫变量$\sigma_i$为在$\{-1,+1\}$上均匀分布的独立随机伯努利变量
	- 经验拉德马赫复杂度（Empirical Rademacher Complexity）：对于数据集$S=\{z_1,z_2,...,z_n\}$，定义为$$\hat{\mathcal{R}}_{S_n}(\mathcal{G})=\mathbb{E}_\sigma\left[\sup_{g \in \mathcal{G}} \frac{1}{n} \sum_{i=1}^n \sigma_i g(z_i)\right]$$
		* 该复杂度反映了函数族$\mathcal{G}$在数据集$S$上的拟合能力，复杂度越大，表示函数族越容易拟合随机噪声
	* （期望）拉德马赫复杂度（Expected Rademacher Complexity）：由于经验拉德马赫复杂度依赖于具体的数据集$S$，因此定义其期望形式为：$$\mathcal{R}_n(\mathcal{G})=\mathbb{E}_{S_n \sim \mathcal{D}^n}[\hat{\mathcal{R}}_{S_n}(\mathcal{G})]$$
		* 该复杂度反映了函数族$\mathcal{G}$在所有可能的数据集上的平均拟合能力
* 性质：
	1. 随数据集规模递减，也即：$$\mathcal{R}_{n+1}(\mathcal{G}) \leq \mathcal{R}_{n}(\mathcal{G})$$该结论是平凡的（数据集规模越大，打散数据集的难度越高），证明略去
	2. 有界性：对于一般的假设空间$\mathcal{H}$，有$$0 < \mathcal{R}_n(\mathcal{H}) < 1$$该值越大，表示假设空间的拟合（打散）能力越强
- 拉德马赫泛化界：
	- 定理：对函数族$\mathcal{G}: \mathcal{Z} \to [0,1]$，任意$\delta > 0$，以概率至少$1 - \delta$，对所有$g \in \mathcal{G}$，有： $$\mathbb{E}_{\pmb z\sim D}[g(\pmb z)] \leq \frac{1}{n} \sum_{i=1}^n g(\pmb z_i) + 2 \mathcal{R}_n(\mathcal{G}) + \sqrt{\frac{\log\frac{1}{\delta}}{2n}} \tag{1}$$且$$\mathbb{E}_{\pmb z\sim D}[g(\pmb z)] \leq \frac{1}{n} \sum_{i=1}^n g(\pmb z_i) + 2 \hat{\mathcal{R}}_n(\mathcal{G}) + 3\sqrt{\frac{\log\frac{2}{\delta}}{2n}} \tag{2}$$
	- 含义：该定理给出了函数族$\mathcal{G}$的期望误差（左侧）与经验误差（右侧第一项）之间的关系，也即**函数族$\mathcal{G}$的期望误差和经验误差的差值有一个上界**，该上界可以由**假设空间的拉德马赫复杂度**（右侧第二项）和一个与**置信度相关的项**（右侧第三项）共同决定
	- 证明：
		- 定义上确界函数：$$\Phi(\mathcal{S}) = \sup_{g \in \mathcal{G}} (\mathbb{E}_D[g] - \hat{\mathbb{E}}_{\mathcal{S}}[g]) \text{ where } \mathcal{S} = (\boldsymbol{z}_1, \boldsymbol{z}_2, ..., \boldsymbol{z}_n)$$
		- 证明该上确界函数是稳定的：
			- 假设样本集$\mathcal{S}'$与$\mathcal{S}$仅在$\boldsymbol{z}_i$处不同，则只需验证：$|\Phi(\mathcal{S}) - \Phi(\mathcal{S}')| \leq c_i$
			- 由定义：$$\Phi(\mathcal{S}) = \sup_{g \in \mathcal{G}} (\mathbb{E}_D[g] - \hat{\mathbb{E}}_{\mathcal{S}}[g]),\quad \mathcal{S} = (\boldsymbol{z}_1, ..., \boldsymbol{z}_n)$$当$\mathcal{S}$变为仅$\boldsymbol{z}_i$不同的$\mathcal{S}'$时： $$ \begin{align*} \Phi(\mathcal{S}) - \Phi(\mathcal{S}') &= \sup_{g \in \mathcal{G}} (\mathbb{E}_D[g] - \hat{\mathbb{E}}_{\mathcal{S}}[g]) - \sup_{g \in \mathcal{G}} (\mathbb{E}_D[g] - \hat{\mathbb{E}}_{\mathcal{S}'}[g]) \\ &\leq \sup_{g \in \mathcal{G}} \left[(\mathbb{E}_D[g] - \hat{\mathbb{E}}_{\mathcal{S}}[g]) - (\mathbb{E}_D[g] - \hat{\mathbb{E}}_{\mathcal{S}'}[g])\right] \quad \text{（上确界的次可加性）} \\ &= \sup_{g \in \mathcal{G}} \left[\hat{\mathbb{E}}_{\mathcal{S}'}[g] - \hat{\mathbb{E}}_{\mathcal{S}}[g]\right] \\ &= \sup_{g \in \mathcal{G}} \left\{\frac{1}{n}\sum_{\boldsymbol{z} \in \mathcal{S}'} g(\boldsymbol{z}) - \frac{1}{n}\sum_{\boldsymbol{z} \in \mathcal{S}} g(\boldsymbol{z})\right\} \\ &= \frac{1}{n}\sup_{g \in \mathcal{G}} \left\{g(\boldsymbol{z}_i') - g(\boldsymbol{z}_i)\right\} \\ &\leq \frac{1}{n} \quad \text{（因} \ g:\mathcal{Z} \to [0,1] \text{有界）} \end{align*} $$
		- 应用[[L3 学习理论#^e3f1a7|McDiarmid不等式]]，则有：$$P\left(\Phi(\mathcal{S}) - \mathbb{E}_{\mathcal{S}}\Phi(\mathcal{S}) \geq \varepsilon\right) \leq \exp\left(-\frac{2\varepsilon^2}{\sum_{i=1}^n 1/n^2}\right) = \exp(-2n\varepsilon^2) $$
		- 因此，以至少 $1 - \delta/2$ 的概率： $$ \Phi(\mathcal{S}) \leq \mathbb{E}_{\mathcal{S}}[\Phi(\mathcal{S})] + \sqrt{\frac{\log(2/\delta)}{2n}}$$
		- 下证$\mathbb{E}_{\mathcal{S}}[\Phi(\mathcal{S})]$这一期望有上界： $$ \begin{align*} \mathbb{E}_{\mathcal{S}}[\Phi(\mathcal{S})] &= \mathbb{E}_{\mathcal{S}}\left[\sup_{g \in \mathcal{G}} (\mathbb{E}_D[g] - \hat{\mathbb{E}}_{\mathcal{S}}[g])\right] \\ &\stackrel{\text{重采样技巧}}{=} \mathbb{E}_{\mathcal{S}}\left[\sup_{g \in \mathcal{G}} \mathbb{E}_{\mathcal{S}' \sim D^n} (\hat{\mathbb{E}}_{\mathcal{S}'}[g] - \hat{\mathbb{E}}_{\mathcal{S}}[g])\right] \quad (\text{因} \ \mathbb{E}_D[g] = \mathbb{E}_{\mathcal{S}' \sim D^n}\hat{\mathbb{E}}_{\mathcal{S}'}[g]) \\ &\stackrel{\text{次可加性}}{\leq} \mathbb{E}_{\mathcal{S},\mathcal{S}'}\left[\sup_{g \in \mathcal{G}} (\hat{\mathbb{E}}_{\mathcal{S}'}[g] - \hat{\mathbb{E}}_{\mathcal{S}}[g])\right] \\ &= \mathbb{E}_{\mathcal{S},\mathcal{S}'}\left[\sup_{g \in \mathcal{G}} \frac{1}{n}\sum_{i=1}^n \left(g(\boldsymbol{z}_i') - g(\boldsymbol{z}_i)\right)\right] \\ &\stackrel{\text{若}\ \sigma_i=-1,\ \text{交换}\ \boldsymbol{z}_i\text{与}\ \boldsymbol{z}_i'}{=} \mathbb{E}_{\sigma,\mathcal{S},\mathcal{S}'}\left[\sup_{g \in \mathcal{G}} \frac{1}{n}\sum_{i=1}^n \sigma_i\left(g(\boldsymbol{z}_i') - g(\boldsymbol{z}_i)\right)\right] \\ &\stackrel{次可加性}{\leq} \mathbb{E}_{\sigma,\mathcal{S}'}\left[\sup_{g \in \mathcal{G}} \frac{1}{n}\sum_{i=1}^n \sigma_i g(\boldsymbol{z}_i')\right] + \mathbb{E}_{\sigma,\mathcal{S}}\left[\sup_{g \in \mathcal{G}} -\frac{1}{n}\sum_{i=1}^n \sigma_i g(\boldsymbol{z}_i)\right] \\ &= 2\mathbb{E}_{\sigma,\mathcal{S}}\left[\sup_{g \in \mathcal{G}} \frac{1}{n}\sum_{i=1}^n \sigma_i g(\boldsymbol{z}_i)\right] = 2\mathcal{R}_n(\mathcal{G}) \end{align*} $$
		- 联立上述两式，可以得到定理中式(1)的联合界结论
		- 经验复杂度上界：同样可以使用McDiarmid不等式：
			- 经验复杂度函数对样本是稳定的：$$|\hat{\mathcal{R}}_{\mathcal{S}'}(\mathcal{G}) - \hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G})| \leq \left| \frac{1}{n} \mathbb{E}_{\sigma} \left[ \sup_{g \in \mathcal{G}} \{\sigma_i g(\boldsymbol{z}_i) - \sigma_i g(\boldsymbol{z}_i')\} \right] \right| \leq \frac{1}{n}$$
			- 应用McDiarmid不等式，以至少$1 - \delta/2$的概率，有： $$\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G}) \leq \mathbb{E}_{\mathcal{S}}[\hat{\mathcal{R}}_{\mathcal{S}}(\mathcal{G})] + \sqrt{\frac{\log(2/\delta)}{2n}} = \mathcal{R}_n(\mathcal{G}) + \sqrt{\frac{\log(2/\delta)}{2n}}$$
		- 使用概率学基本知识，放缩到上界，可以联立两个概率不等式和一个恒成立不等式，得到式(2)的结论
- 应用于二分类任务的泛化界：
	- 设假设空间$\mathcal{H}$为二分类函数族，使用0-1损失函数作为函数族$\mathcal{G}$，即$g(\pmb z) = \ell(h(\pmb x), y) = \pmb{1}(h(\pmb x) \neq y)$，则有：$$\mathcal{R}_n(\mathcal{G}) = \frac{1}{2} \mathcal{R}_n(\mathcal{H})$$
		- 证明：展开期望的定义，进行变量提取换元即可，略去
- 问题：
	- 即使是经验拉德马赫复杂度，对其计算也是十分困难的，因为每次计算需要找到最优的假设函数，也即运行一次学习算法。在一个$2^n$个标签形式的集合上运行学习算法，计算复杂度是不可接受的（也即NP难问题）
	- 因此，需要对拉德马赫复杂度进行上界估计，从而得到可计算的泛化界
## 增长函数(Growth Function)
- 定义：对假设空间$\mathcal{H}$，增长函数$\Pi_{\mathcal{H}}:\mathbb{N} \to \mathbb{N}$定义为：$$\forall n\in \mathbb{N},\Pi_{\mathcal{H}}(n) = \max_{\{x_1,x_2,...,x_n\} \in \mathcal{X}} |\{(h(x_1), h(x_2), ..., h(x_n)) : h \in \mathcal{H}\}|$$
	- 含义：增长函数衡量了假设空间$\mathcal{H}$在任意$n$个样本点上能够实现的不同分类方式的最大数量
		- 例：对如下的一个线性的假设空间，可直观看出，其增长函数值为8![[Pasted image 20251110193712.png]]
	- 显然，对$n$个数据点构成的数据集，若增长函数值为$2^n$，则表示假设空间$\mathcal{H}$可以对这$n$个数据点实现所有可能的分类方式（也即打散），对应假设空间的拉德马赫复杂度为1
- Massart引理(Massart's Lemma)：
	- 定理：设$\mathcal{A} \subset \mathbb{R}^n$为有限集合，且$\forall \pmb a \in \mathcal{A}, ||\pmb a||_2 \leq r$，则有：$$\mathbb{E}_\sigma\left[\sup_{\pmb a \in \mathcal{A}} \frac{1}{n} \sum_{i=1}^n \sigma_i a_i\right] \leq r \sqrt{\frac{2 \log |\mathcal{A}|}{n}}$$
	- 证明思路略去
	* 含义：该引理提供了一个上界，用于估计在有限集合$\mathcal{A}$上，随机符号加权和的期望最大值
* 增长函数界：
	* 定理：设函数族$\mathcal{G}: \mathcal{Z} \to [0,1]$，则其拉德马赫复杂度有如下上界：$$\mathcal{R}_{n}(\mathcal{G}) \leq \sqrt{\frac{2 \log \Pi_{\mathcal{G}}(n)}{n}}$$
	* 证明略去
* 增长函数泛化界：
	* 定理：对假设空间$\mathcal{H}$，其中的假设函数均为二分类函数$h: \mathcal{X} \to \{-1,+1\}$，则对任意$\delta > 0$，以概率至少$1 - \delta$，对所有$h \in \mathcal{H}$，有： $$\mathcal{E}_D(h) \leq \hat{\mathcal{E}}_{D_n}(h) + \sqrt{\frac{2 \log \Pi_{\mathcal{H}}(n)}{n}} + \sqrt{\frac{\log\frac{1}{\delta}}{2n}}$$
	* 基于假设空间大小的泛化界：由于$\Pi_{\mathcal{H}}(n) \leq |\mathcal{H}|$，因此增长函数泛化界可以退化为基于假设空间大小的泛化界：$$ \mathcal{E}_D(h) \leq \hat{\mathcal{E}}_{D_n}(h) + \sqrt{\frac{2 \log |\mathcal{H}|}{n}} + \sqrt{\frac{\log\frac{1}{\delta}}{2n}}$$
	* 含义：增长函数是一个假设空间更加本质的特征，相比于假设空间的大小，增长函数能够更准确地反映假设空间的复杂度。因此，基于增长函数的泛化界通常比基于假设空间大小的泛化界更紧
	* 问题：实际的假设空间都是无限大的，其增长函数也难以计算。因此，需要寻找更加可计算的假设空间复杂度度量方法
## VC维度(Vapnik-Chervonenkis Dimension)
- 定义：
	- 对假设空间$\mathcal{H}$，定义VC维度为：$$\text{VCdim}(\mathcal{H}) = \max\{n \in \mathbb{N} : \Pi_{\mathcal{H}}(n) = 2^n\}$$
	- 含义：VC维度衡量了假设空间$\mathcal{H}$能够打散的最大数据集规模
	- 计算：只需证明，对某个正+整数$d$，存在某组$d$个数据点可以被假设空间$\mathcal{H}$打散，而对任意$d+1$个数据点，假设空间$\mathcal{H}$都不能打散，则有$\text{VCdim}(\mathcal{H}) = d$
		- 例：考虑一个一维空间内的线段分类器，在线段上的点分类为正类，线段外的点分类为负类，则该假设空间的VC维度为2![[Pasted image 20251110201128.png]]
		- 例：考虑一个二维空间内的线性分类器，则该假设空间的VC维度为3![[Pasted image 20251110201148.png]]
		- 例：考虑一维空间内的带参正弦函数分类器，比如$h(x) = \text{sign}(\sin(wx))$，则该假设空间的VC维度为无穷大![[Pasted image 20251110201228.png]]
		- 更高级的知识可以证明，对线性分类器，其VC维度与输入空间的维度有关，具体为$\text{VCdim}(\mathcal{H}) = d + 1$，其中$d$为输入空间的维度
- Sauer引理(Sauer's Lemma)：
	- 定理：设假设空间$\mathcal{H}$的VC维度为$d$，则其增长函数有如下上界：$$\forall n \in \mathbb{N}, \Pi_{\mathcal{H}}(n) \leq \sum_{i=0}^d \binom{n}{i}$$
	- 证明思路略去
	- 含义：该引理提供了一个上界，用于估计假设空间$\mathcal{H}$在任意$n$个样本点上能够实现的不同分类方式的最大数量，基于VC维度$d$，该上界反映了假设空间的复杂度
	- 阶数形式：
		- 当$n \leq d$时，$\Pi_{\mathcal{H}}(n) = 2^n$
		- 当$n > d$时，$\Pi_{\mathcal{H}}(n) \leq \left(\frac{en}{d}\right)^d=O(n^d)$
		- 也即，增长函数在$n \leq d$时呈指数增长，而在$n > d$时呈多项式增长
- VC维度泛化界：
	- 定理：设假设空间$\mathcal{H}$的VC维度为$d$，则对任意$\delta > 0$，以概率至少$1 - \delta$，对所有$h \in \mathcal{H}$，有： $$\mathcal{E}_D(h) \leq \hat{\mathcal{E}}_{D_n}(h) + \sqrt{\frac{2d \log\frac{en}{d}}{n}} + \sqrt{\frac{\log\frac{1}{\delta}}{2n}}$$
	- 阶数形式：$$\mathcal{E}_D(h) \leq \hat{\mathcal{E}}_{D_n}(h) + O\left(\sqrt{\frac{\log \frac{n}{d}}{\frac{n}{d}}}\right)$$
	- 含义：该泛化界表明，假设空间的VC维度$d$越小，样本的复杂度$n$越大，泛化误差上界越紧，从而假设函数的泛化能力越强
---
[[L4 决策树]]