# 机器学习
## 机器学习框架
- 例：垃圾邮件的分类——二分类问题
	- **特征抽取/学习**：将输入的数据映射到向量空间中，便于后续的数据处理
	- 学习的过程即为找到假设函数以拟合实际函数的过程：![[Pasted image 20250915155136.png]]
		- **假设空间**：一般选用参数化的模型：$h_\theta(x)$，同时函数应当具有一些良好的数学性质（即**正则性**）：连续、光滑等
		- **损失函数**：刻画假设函数与实际情况的差异。如平方损失函数：$l(y,h(x))=(y-h(x))^2$
		- **训练**：最小化训练误差的过程：$$\hat{\epsilon}(h)=\min_\theta\sum_{i=1}^nl(h_\theta(\pmb x_i),y_i)$$
- 分类：![[Pasted image 20250922152137.png]]
	- 监督学习：所有数据的$(\pmb x,\pmb y)$都已知
		- 分为**分类**问题（对离散标签）和**回归**问题（对连续的数值）
	- 无监督学习：数据的$(\pmb x,\pmb y)$中$\pmb y$都未知
		- 分为聚类问题（对离散标签）和**嵌入**问题（对向量值）
	- 强化学习：根据外界的反馈实时调整策略的学习过程
- **“No Free Lunch Theorem”**：
	- 不同的模型在不同问题上有不同的表现，没有先验意义下更好的模型
	- 数学表述：![[Pasted image 20250915163751.png]]
	- 模型的选择：[[L4 学习(A)#模型选择|人工智能导论——模型选择]]
		- 数学描述：![[Pasted image 20250915165600.png]]
# K-近邻算法(KNN)
- 适用问题：无监督的聚类问题
- 思路：对输入数据，将其视为高维空间的向量，对某数据，简单的选取K个与之相邻的数据，按照标签给出投票，从而得到一个决策面
	- ![[Pasted image 20250922152844.png]]
- 性质：
	- 是一个**非参数化**的方法
	- 数学证明可以得到，$\lim_{n\rightarrow \infty}\epsilon_\text{KNN}(n)\leq 2\epsilon^*(1-\epsilon^*)$，在大样本下，这是一个好的学习器
- 学习器的调整优化：	
	- k是这个学习器的超参数，k过小则过拟合，过大则欠拟合
	- 归纳偏好：相近的点的标签更可能相似，且各个维度都是平权的
		- 修正：标准化，确保各个维度的分布都接近，选用标准的高斯分布，将每个维度上的数据都归一化到标准高斯分布
	- 距离选择：
		- 距离可以有多种公式，都属于超参数调优的范围
		- 例：![[Pasted image 20250922155800.png]]
	- 加权KNN：对每个数据，在K近邻计算时给予不同的权重，用于更精确的确定决策面
	- 最近中心方法：计算每个类的类中心，之后以类中心的近邻来进行类别划分
- ## **维度灾难**：
	- 对KNN算法，其具有$O(nd)$的复杂度
	- 通过KD-Tree的数据结构（[[L8 高级搜索树#KD树|数据结构——KD树]]），在$n \gg 2^d$时，可以得到比较好的加速效果
		- 但这一条件过于苛刻，因而在实际应用中，简单的KNN算法会遇到计算上的困难
	- 由于空间的指数特性，在高维的线性空间中，数据的分布是稀疏的
		- 高维空间下原点到最近的点的