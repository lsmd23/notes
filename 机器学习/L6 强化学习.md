# 强化学习概述
- 强化学习（Reinforcement Learning, RL）是一种机器学习范式，旨在训练智能体通过与环境交互来学习最优策略，以最大化累积奖励
- 强化学习范式被广泛应用于游戏、机器人控制、自动驾驶等领域，涉及多个学科的知识，如计算机科学、控制论、神经科学等
- 核心特点：
	- 序列决策问题：智能体需要在多个时间步中做出决策
	- 无监督学习：智能体通过试错方式学习，只有奖励信号指导，而非依赖于标注数据
	- 反馈延迟：奖励可能在多个时间步后才收到，增加了学习的复杂性
	- 时间影响：当前的行动可能影响未来的奖励和状态
		- 因此，RL的数据不满足独立同分布（i.i.d.）假设，其泛化问题更为复杂
- 建模：
	- 行为模式：
		- 对智能体，在时间步$t$
			- 接受对环境的观察$O_t$（环境状态的部分或全部信息）
			- 接受奖励信号$R_t$（衡量当前状态的好坏）
			- 基于观察和奖励，选择动作$A_t$（影响环境状态的决策）
			- 对环境
			- 接受智能体的动作$A_t$
			- 根据动作更新环境状态，产生新的观察$O_{t+1}$和奖励$R_{t+1}$
		- 示意图：![[Pasted image 20251201153233.png]]
	- 历史（History）：
		- 历史$H_t$表示智能体在时间步$t$之前的所有观察、奖励和动作序列：$$H_t = (O_1, R_1, A_1, O_2, R_2, A_2, ...,A_{t-1}, O_t, R_t)$$
		- 历史包含了智能体与环境交互的完整信息
	- 状态（State）：状态$S_t$是对历史$H_t$的某种压缩或抽象：$S_t = f(H_t)$，保留了对未来决策最有用的信息
	- 奖励（Reward）：奖励$R_t$是智能体在时间步$t$收到的标量反馈信号，表示当前状态的好坏
		- 强化学习的目标是最大化累积奖励，有时可能会牺牲短期奖励以获得长期收益
	- 可观测性：
		- 将智能体定义的状态$S_t$，记为$S_t^a$，环境定义的状态记为$S_t^e$
		- 如果$O_t=S_t^a = S_t^e$，则称其为完备可观测环境（Fully Observable Environment）
		- 如果$S_t^a\neq S_t^e$，则称其为部分可观测环境（Partially Observable Environment）
	- RL智能体：由以下几个主要组件组成
		- 策略（Policy）：定义智能体的行为模式
			- 确定性策略：$a = \pi(s)$
			- 随机策略：$\pi(a|s)=\mathbb{P}[A_t=a|S_t=s]$
		- 价值函数（Value Function）：评估状态或状态-动作对的长期价值
			- 状态价值函数：$v_\pi(s)$
			- 动作价值函数：$Q_\pi(s,a)$
		- 模型（Model）：对环境动态的预测
			- 状态转移概率：$\mathcal{P}_{ss'}^a=\mathbb{P}[S_{t+1}=s'|S_t=s,A_t=a]$
			- 奖励函数：$\mathcal{R}_s^a=\mathbb{E}[R_{t+1}|S_t=s,A_t=a]$
		- 例：![[Pasted image 20251201155146.png]]
# 强化学习的数学框架
## 马尔可夫奖励过程
- 马尔可夫过程（Markov Process, MP）：
	- 定义：状态$S_t$是马尔可夫的，当且仅当$$\mathbb{P}[S_{t+1}|S_t] = \mathbb{P}[S_{t+1}|S_1, S_2, ..., S_t]$$也即，未来状态只依赖于当前状态，而与过去状态无关
	- 状态迁移概率：$\mathcal{P}_{ss'}=\mathbb{P}[S_{t+1}=s'|S_t=s]$
	- 马尔可夫过程（MP）是一个元组$<\mathcal{S}, \mathcal{P}>$：
		- $\mathcal{S}$：状态空间，包含所有可能的状态，$\mathcal{S}=\{s_1,s_2,...,s_n\}$
		- $\mathcal{P}$：状态转移概率矩阵，描述从一个状态转移到另一个状态的概率，$$\mathcal{P}=\begin{bmatrix}\mathcal{P}_{11} & \mathcal{P}_{1 2} & ... & \mathcal{P}_{1 n} \\ \mathcal{P}_{2 1} & \mathcal{P}_{2 2} & ... & \mathcal{P}_{2 n} \\ ... & ... & ... & ... \\ \mathcal{P}_{n 1} & \mathcal{P}_{n 2} & ... & \mathcal{P}_{n n} \\ \end{bmatrix}$$
	- 例：![[Pasted image 20251201160457.png]]
- 马尔可夫奖励过程（Markov Reward Process, MRP）：
	- 定义：马尔可夫奖励过程是一个元组$<\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma>$，其中
		- $\mathcal{S}$：状态空间
		- $\mathcal{P}$：状态转移概率矩阵
		- $\mathcal{R}$：奖励函数，$\mathcal{R}_s=\mathbb{E}[R_{t+1}|S_t=s]$
			- 从某个状态出发，下一步获得的期望奖励
			- 某种意义上刻画了状态的短期内“好坏”
		- $\gamma$：折扣因子，$0\leq \gamma \leq 1$，用于权衡短期和长期奖励的相对重要性
	- 例：在上述例子中，每个状态上定义一个奖励![[Pasted image 20251201160656.png]]
	- 回报函数（Return）：回报$G_t$表示从时间步$t$开始，智能体未来所能获得的累积奖励，定义为$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
		- 折扣因子$\gamma$用于权衡短期和长期奖励的重要性，当$\gamma=0$时，智能体只关注即时奖励，当$\gamma$接近1时，智能体更加重视长期奖励
		- 例：![[Pasted image 20251201161906.png]]
		- 由于回报函数是奖励的无穷和，为了确保其收敛，通常要求奖励是有界的，并且折扣因子$\gamma$小于1
		- 由于奖励是一个随机变量，因此回报函数也是一个随机变量
	- 价值函数（Value Function）：价值函数用于刻画状态的长期“好坏”
		- 状态价值函数（State Value Function）：$$v(s) = \mathbb{E}[G_t | S_t = s]$$表示在状态$s$下，智能体未来所能获得的期望累积奖励
	- 贝尔曼方程（Bellman Equation）：对状态价值函数，将其分解为动态规划的形式$$v(s) = \mathbb{E}[R_{t+1} + \gamma v(S_{t+1}) | S_t = s]$$
		- 该方程表明，状态$s$的价值等于从该状态出发，获得的即时奖励$\mathbb{E}[R_{t+1}|S_t=s]$加上折扣后的下一个状态的价值的期望$\mathbb{E}[\gamma v(S_{t+1})|S_t=s]$
		- 可知，$\mathbb{E}[R_{t+1}|S_t=s]$即为奖励函数$\mathcal{R}_s$
		- $\mathbb{E}[\gamma v(S_{t+1})|S_t=s]$可由状态转移概率矩阵$\mathcal{P}$计算得到：$$\mathbb{E}[\gamma v(S_{t+1})|S_t = s] = \sum_{s' \in S} \{ \gamma v(s') \cdot \mathbb{P}[S_{t+1} = s' | S_t = s] \}$$
		- 因此，有：$$v(s)=\mathcal{R}_s+\gamma \sum_{s'\in \mathcal{S}}\mathcal{P}_{ss'}v(s') $$写成矩阵形式也即：$\pmb v = \pmb{\mathcal{R}}+\gamma\pmb{\mathcal{Pv}}$




## 马尔可夫决策过程
- 马尔可夫决策过程（Markov Decision Process, MDP）：
	- 定义：马尔可夫决策过程是一个元组$<\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma>$，其中
		- $\mathcal{S}$：状态空间，$\mathcal{S}=\{s_1,s_2,...,s_n\}$
		- $\mathcal{A}$：动作空间，包含智能体可采取的所有可能动作，$\mathcal{A}=\{a_1,a_2,...,a_m\}$
		- $\mathcal{P}$：状态转移概率矩阵，$\mathcal{P}_{ss'}^a=\mathbb{P}[S_{t+1}=s'|S_t=s,A_t=a]\in \mathbb{R}^{n\times n \times m}$
		- $\mathcal{R}$：奖励函数，$\mathcal{R}_s^a=\mathbb{E}[R_{t+1}|S_t=s,A_t=a]$
		- $\gamma$：折扣因子
	- 例：在上述例子中，添加动作维度![[Pasted image 20251201164138.png]]
- 策略（Policy）：
	- 定义：策略$\pi$定义了智能体在每个状态下选择动作的概率分布，$\pi(a|s)=\mathbb{P}[A_t=a|S_t=s]$
	- 策略决定了智能体的行为，$A_t\sim \pi(\cdot|S_t),\forall t>0$
- 价值函数（Value Function）：
	- 状态价值函数（State Value Function）：$$v_\pi(s) = \mathbb{E}_\pi[G_t | S_t = s]$$表示在状态$s$下，智能体按照策略$\pi$未来所能获得的期望累积奖励
	- 动作价值函数（Action Value Function）：$$Q_\pi(s,a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]$$表示在状态$s$下，智能体采取动作$a$后，按照策略$\pi$未来所能获得的期望累积奖励
	- 贝尔曼方程（Bellman Equation）：
		- 将两个函数分别展开，可得：$$v_{\pi}(s) = \mathbb{E}_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]$$$$q_{\pi}(s,a) = \mathbb{E}_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]$$
		- 对价值函数进行概率展开：$$v_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) Q_\pi(s,a)$$$$Q_\pi(s,a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_\pi(s')$$
		- 由此可以推出，状态价值函数的贝尔曼方程：$$v_\pi(s) = \sum_{a \in \mathcal{A}} \pi(a|s) \left[ \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_\pi(s') \right]$$动作价值函数的贝尔曼方程：$$Q_\pi(s,a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a'|s') Q_\pi(s',a')$$



## 最优化策略