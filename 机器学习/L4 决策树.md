# 决策树
- 实际情况中，数据往往是离散的或者分类的，称为数据的异质性（heterogeneity）
	- 决策树（Decision Tree）是一种处理异质数据的有效方法
- 决策树模型：
	- 决策树模型通过对人的决策过程的模拟，通过树形结构来表示数据的分类过程
	- 决策树的节点表示对某个属性的测试，分支表示测试结果，叶节点表示类别标签
		- 示意：![[Pasted image 20251117153038.png]]
	- 决策树具有良好的可解释性，能够自然处理异质数据，无需依赖传统学习模型的特征空间转换
## ID3算法
- 算法描述：参见[[L5 学习(B)#决策树#ID3算法|人工智能导论——ID3算法]]
	- 附注：
		- 熵也可以从概率的角度表示为$$H(D)=-\mathbb{E}_{p(y)}[\log p(y)]$$
		- ID3算法运行时，若某个标准划分出的子组为空，则该叶子节点的类别取父节点中样本数最多的类别（也即最大投票）
## C4.5算法
- 对ID3算法的正则化修正
	- 信息增益问题：显然，选用一组数据的主键分类，会使信息增益最大，但这种划分毫无意义
		- 解决：引入固有值，使用增益率进行划分（参见[[L5 学习(B)#^48ea3a|人工智能导论——固有值与正则化]]）
	- 数据成本问题：实际数据获取的成本不同
		- 例：医疗诊断中，不同的检查费用不同
		- 解决：为每个特征定义一个成本$c_f$，在选取划分标准时，使用$\frac{(\text{Gain Ratio})^2}{c_f}$标准进行成本惩罚
	- 数据缺失问题：在实际数据中，往往存在部分数据缺失的情况
		- 例：![[Pasted image 20251117160446.png]]
		- 方法：
			- 定义缺失比例$\rho=\frac{|\bar{\mathcal{D}}|}{|\mathcal{D}|}$
			- 按该有数据缺失的数据集进行划分，计算信息增益时，乘以$(1-\rho)$进行惩罚
			- 划分后，对于缺失数据，按各个子节点的样本比例进行分配（将节点对应的投票权重按比例分配）
			- 例：![[Pasted image 20251117160921.png]]![[Pasted image 20251117160927.png]]
	- 连续数据划分问题：数据类型有时是连续的
		- 解决：在连续数据的有序数据点中，选取一个阈值进行划分，阈值选取的标准为取得最大信息增益的阈值
		- 示意：![[Pasted image 20251117161757.png]]
		- 问题：如果仔细调整，需要较高的计算成本（复杂度变为$O(n^2d)$）
- 树的剪枝：为防止过拟合，需要对决策树进行剪枝
	- 预剪枝：每次划分前，使用验证集评估划分的必要性
		- 若划分后准确率不提升，则不进行划分
		- 缺点：贪心策略，可能导致欠拟合
	- 后剪枝：在树生长完成后，从叶节点向根节点回溯，使用验证集评估剪枝的必要性
		- 若剪枝后准确率提升，则进行剪枝
		- 优点：避免了预剪枝的贪心问题
- 决策树统合到损失函数：参见[[L5 学习(B)#^8e763f|人工智能导论——决策树的损失函数]]
	- C4.5算法等决策树算法，也即使用贪心的方式，最小化这一目标函数
	- 因此，可以使用这一损失函数来指导剪枝过程
- 决策树的解释：参见[[L5 学习(B)#理解决策树|人工智能导论——理解决策树]]
	- 多元决策树：在每个数据集内，再训练小的线性模型拟合这个分类区域的数据
		- 示意：![[Pasted image 20251117163428.png]]
		- 问题：复杂度过高，可解释性下降，且不一定提升拟合效果
## 分类和回归树（CART算法）
- 基于基尼系数的分类树：
	- 基尼系数定义：$$Gini(D)=1-\sum_{k=1}^{K}\left(\frac{|\mathcal{C}_k|}{|\mathcal{D}|}\right)^2$$
	- 基于基尼系数的二叉树：以某个数据特征$A(x)=a$和$A(x)\neq a$进行划分
		- 计算划分后基尼系数：$$Gini_A(D)=\frac{|\mathcal{D}_{A=a}|}{|\mathcal{D}|}Gini(\mathcal{D}_{A=a})+\frac{|\mathcal{D}_{A\neq a}|}{|\mathcal{D}|}Gini(\mathcal{D}_{A\neq a})$$
		- 划分标准：选择使得$Gini(D)-Gini_A(D)$最大的划分方式
		- 按这样，可以得到一棵二叉分类树
- 回归树：
	- 原理：使用平方误差作为划分标准
		- 对一组数据，若使用平方误差为待优化问题，则其最优解为平均值（若使用绝对误差，则最优解为中位数）
		- 因此，对没有离散标签而是连续数值的数据（也即回归问题），可以使用平方误差作为划分标准
	- 划分过程：
		- 选定某个特征$A$和阈值$v$，将数据划分为$A(x)\leq v$和$A(x)>v$两部分
		- 使用划分后两边数据的平均数作为预测值（标签），计算平方误差
		- 将两组数据的平方误差求和，作为该划分方式的误差
		- 数学表示：$$\ell(j,s))=\sum_{i:x_i\in R_1(j,s))}(y_i-\hat{v}_1(j,s))^2+\sum_{i:x_i\in R_2(j,s))}(y_i-\hat{v}_2(j,s))^2$$其中$\hat{v}_1(j,s))$和$\hat{v}_2(j,s))$分别为两组数据的平均值
- CART算法：
	- 使用平方误差作为划分标准，生成二叉树
	- 同样可以用C4.5算法中的剪枝方法对CART生成的树进行剪枝
	- 也是一种贪心的决策树生成算法
# 随机森林
- 将决策树学习方法集成起来的一种方法
## 装袋法
- 参见[[L5 学习(B)#随机森林#装袋法|人工智能导论——装袋法]]
## Breiman算法
- 参见[[L5 学习(B)#随机森林#Breiman算法|人工智能导论——Breiman算法]]
---
[[L5 提升方法]]