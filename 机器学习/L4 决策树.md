# 决策树
- 实际情况中，数据往往是离散的或者分类的，称为数据的异质性（heterogeneity）
	- 决策树（Decision Tree）是一种处理异质数据的有效方法
- 决策树模型：
	- 决策树模型通过对人的决策过程的模拟，通过树形结构来表示数据的分类过程
	- 决策树的节点表示对某个属性的测试，分支表示测试结果，叶节点表示类别标签
		- 示意：![[Pasted image 20251117153038.png]]
	- 决策树具有良好的可解释性，能够自然处理异质数据，无需依赖传统学习模型的特征空间转换
## ID3算法
- 算法描述：参见[[L5 学习(B)#决策树#ID3算法|人工智能导论——ID3算法]]
	- 附注：
		- 熵也可以从概率的角度表示为$$H(D)=-\mathbb{E}_{p(y)}[\log p(y)]$$
		- ID3算法运行时，若某个标准划分出的子组为空，则该叶子节点的类别取父节点中样本数最多的类别（也即最大投票）
## C4.5算法
- 对ID3算法的正则化修正
	- 信息增益问题：显然，选用一组数据的主键分类，会使信息增益最大，但这种划分毫无意义
		- 解决：引入固有值，使用增益率进行划分（参见[[L5 学习(B)#^48ea3a|人工智能导论——固有值与正则化]]）
	- 数据成本问题：实际数据获取的成本不同
		- 例：医疗诊断中，不同的检查费用不同
		- 解决：为每个特征定义一个成本$c_f$，在选取划分标准时，使用$\frac{(\text{Gain Ratio})^2}{c_f}$标准进行成本惩罚
	- 数据缺失问题：在实际数据中，往往存在部分数据缺失的情况
		- 例：![[Pasted image 20251117160446.png]]
		- 方法：
			- 定义缺失比例$\rho=\frac{|\bar{\mathcal{D}}|}{|\mathcal{D}|}$
			- 按该有数据缺失的数据集进行划分，计算信息增益时，乘以$(1-\rho)$进行惩罚
			- 划分后，对于缺失数据，按各个子节点的样本比例进行分配（将节点对应的投票权重按比例分配）
			- 例：![[Pasted image 20251117160921.png]]![[Pasted image 20251117160927.png]]
	- 连续数据划分问题：数据类型有时是lian

## 分类和回归（CART算法）



## 决策树的泛化界



# 随机森林


## 装袋法



## Breiman算法