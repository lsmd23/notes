# 决策树
- 实际情况中，数据往往是离散的或者分类的，称为数据的异质性（heterogeneity）
	- 决策树（Decision Tree）是一种处理异质数据的有效方法
- 决策树模型：
	- 决策树模型通过对人的决策过程的模拟，通过树形结构来表示数据的分类过程
	- 决策树的节点表示对某个属性的测试，分支表示测试结果，叶节点表示类别标签
		- 示意：![[Pasted image 20251117153038.png]]
	- 决策树具有良好的可解释性，能够自然处理异质数据，无需依赖传统学习模型的特征空间转换
## ID3算法
- 算法描述：参见[[L5 学习(B)#决策树#ID3算法|人工智能导论——ID3算法]]
	- 附注：
		- 熵也可以从概率的角度表示为$$H(D)=-\mathbb{E}_{p(y)}[\log p(y)]$$
		- ID3算法运行时，若某个标准划分出的子组为空，则该叶子节点的类别取父节点中样本数最多的类别（也即最大投票）
## C4.5算法
- 对ID3算法的正则化修正
	- 信息增益问题：显然，选用一组数据的主键分类，会使信息增益最大，但这种划分毫无意义
		- 解决：引入固有值，使用增益率进行划分（参见[[L5 学习(B)#^48ea3a|人工智能导论——固有值与正则化]]）
	- 数据成本问题：实际数据获取的成本不同
		- 例：医疗诊断中，不同的检查费用不同
		- 解决：为每个特征定义一个成本$c_f$，在选取划分标准时，使用$\frac{(\text{Gain Ratio})^2}{c_f}$标准进行成本惩罚
	- 数据缺失问题：在实际数据中，往往存在部分数据缺失的情况
		- 例：![[Pasted image 20251117160446.png]]
		- 方法：
			- 定义缺失比例$\rho=\frac{|\bar{\mathcal{D}}|}{|\mathcal{D}|}$
			- 按该有数据缺失的数据集进行划分，计算信息增益时，乘以$(1-\rho)$进行惩罚
			- 划分后，对于缺失数据，按各个子节点的样本比例进行分配（将节点对应的投票权重按比例分配）
			- 例：![[Pasted image 20251117160921.png]]![[Pasted image 20251117160927.png]]
	- 连续数据划分问题：数据类型有时是连续的
		- 解决：在连续数据的有序数据点中，选取一个阈值进行划分，阈值选取的标准为取得最大信息增益的阈值
		- 示意：![[Pasted image 20251117161757.png]]
		- 问题：如果仔细调整，需要较高的计算成本（复杂度变为$O(n^2d)$）
- 树的剪枝：为防止过拟合，需要对决策树进行剪枝
	- 预剪枝：每次划分前，使用验证集评估划分的必要性
		- 若划分后准确率不提升，则不进行划分
		- 缺点：贪心策略，可能导致欠拟合
	- 后剪枝：在树生长完成后，从叶节点向根节点回溯，使用验证集评估剪枝的必要性
		- 若剪枝后准确率提升，则进行剪枝
		- 优点：避免了预剪枝的贪心问题
- 决策树统合到损失函数：参见[[L5 学习(B)#^8e763f|人工智能导论——决策树的损失函数]]
	- C4.5算法等决策树算法，也即使用贪心的方式，最小化这一目标函数
	- 因此，可以使用这一损失函数来指导剪枝过程
- 决策树的解释：参见[[L5 学习(B)#理解决策树|人工智能导论——理解决策树]]
	- 多元决策树：在每个数据集内，再训练小的线性模型拟合这个分类区域的数据
		- 示意：![[Pasted image 20251117163428.png]]
		- 问题：复杂度过高，可解释性下降，且不一定提升拟合效果
## 分类和回归树（CART算法）






## 决策树的泛化界



# 随机森林
- 将决策树学习方法集成起来的一种方法
## 装袋法
- 参见[[L5 学习(B)#随机森林#装袋法|人工智能导论——装袋法]]
## Breiman算法
- 参见[[L5 学习(B)#随机森林#Breiman算法|人工智能导论——Breiman算法]]