# 支持向量机
- 使用超平面分类表示二分类的结果：
	- 超平面：$w·x + b = 0$
	- 分类结果：$y = sign(w·x + b)$，也即大于0为一类，小于0为另一类
- 分类器的要求：找到一个最具有噪声鲁棒性的超平面![[Pasted image 20251013164321.png]]
	- 支持向量机的边界间隔：
		- 优化目标：最大化所有样本点到超平面的最小距离$$\max \text{margin}(\pmb w,b),s.t. y_i(\pmb w\cdot \pmb x_i+b)\geq 1,1\leq i\leq n$$
			- 如图：![[Pasted image 20251013164622.png]]
		- 定理：点到超平面的距离为$\frac{|\pmb w\cdot \pmb x+b|}{||\pmb w||_2}$
		- 取1为一个边界，则问题转化为一个规划问题：$$\max \frac{2}{||\pmb w||_2^2},s.t. y_i(\pmb w\cdot \pmb x_i+b)\geq 1,1\leq i\leq n$$
		- 由于这个优化问题不是线性的，将其等价化为有约束的优化问题：$$\min \frac{1}{2}||\pmb w||_2^2,s.t. y_i(\pmb w\cdot \pmb x_i+b)\geq 1,1\leq i\leq n$$
	- 软间隔支持向量机：为了解决过拟合的问题
		- 允许部分样本点出现在间隔内，甚至允许部分样本点被错误分类
			- ![[Pasted image 20251013165447.png]]
		- 引入松弛变量$\xi_i$，表示第$i$个样本点到间隔的距离：$$\min \frac{1}{2}||\pmb w||_2^2+C\sum_{i=1}^n\xi_i,s.t. y_i(\pmb w\cdot \pmb x_i+b)\geq 1-\xi_i,\xi_i\geq 0,1\leq i\leq n$$
			- 其中$C$为一个超参数，控制间隔大小和误分类点数量的权衡
			- 问题为一个**线性约束的二次优化问题**
# 优化求解
## 约束优化问题
- 凸优化问题：目标函数和约束条件均为凸函数（凸集）的问题
	- 凸集：任意两点连线均在集合内的集合
	- 凸函数：定义域为凸集，且任意两点连线上的函数值不大于连线上函数值的线性插值的函数：$$f(\lambda x_1+(1-\lambda)x_2)\leq \lambda f(x_1)+(1-\lambda)f(x_2),\forall x_1,x_2\in \text{dom} (f),\lambda\in[0,1]$$
		- 如图：![[Pasted image 20251020152821.png]]
	- 性质：局部最优解即为全局最优解
- 带约束的优化问题：拉格朗日乘因子法
	- 等式约束的一般形式：$$\begin{aligned} &\min_{x\in \mathbb R^d}f(\pmb x)\\ &s.t.g(\pmb x)= 0\end{aligned}$$
		- 对满足$g(\pmb x)=0$的点，有性质：求梯度时，梯度$\nabla g(\pmb x)$对应的向量和切面是正交的
		- 对最优解$\pmb x^*$，有性质：求函数$f(\pmb x)$的梯度$\nabla f(\pmb x^*)$，这一梯度向量和切面是正交的
		- 因此，两个梯度是共线的，即存在$\lambda$使得：$$\nabla f(\pmb x^*)+\lambda \nabla g(\pmb x^*)=0$$![[Pasted image 20251020153900.png]]
		- 拉格朗日函数：$$\mathcal{L}(\pmb x,\lambda)=f(\pmb x)+\lambda g(\pmb x)$$对其求偏导则得到上述的表达式以及一个约束条件，因此，求解其偏导数为0的一个方程组，则可以得到原问题的最优解
			- **定理**：拉格朗日函数的驻点即为原问题的最优解
	- 不等式约束的一般形式：$$\begin{aligned} &\min_{x\in \mathbb R^d}f(\pmb x)\\ &s.t.g(\pmb x)\leq 0\end{aligned}$$
		- 假设最优解在可行域的内部，则无需考虑约束条件，直接求解即可，此时拉格朗日乘子$\lambda=0$
		- 假设最优解在可行域的边界上，则有性质：拉格朗日乘子需$\lambda > 0$，才能满足约束条件![[Pasted image 20251020154738.png]]
		- 因此，综合两种情况，有额外条件：$$\lambda g(\pmb x)=0,\lambda\geq 0$$才能结合上述的两种情况
		- **KKT条件**：$$\begin{cases} \nabla f(\pmb x^*)+\lambda \nabla g(\pmb x^*)=0\\ g(\pmb x^*)\leq 0\\ \lambda g(\pmb x^*)=0\\ \lambda\geq 0 \end{cases}$$
	- 一般意义的优化问题：引入多个拉格朗日乘子$$\begin{aligned} &\min_{x\in \mathbb R^d}f(\pmb x)\\ &s.t.g_i(\pmb x)\leq 0,i=1,2,...,m\\ &h_j(\pmb x)=0,j=1,2,...,p \end{aligned}$$
		- 拉格朗日函数：$$\mathcal{L}(\pmb x,\pmb \lambda,\pmb \mu)=f(\pmb x)+\sum_{i=1}^m \lambda_i g_i(\pmb x)+\sum_{j=1}^p \mu_j h_j(\pmb x)$$
		- KKT条件：$$\begin{cases} \nabla f(\pmb x^*)+\sum_{i=1}^m \lambda_i \nabla g_i(\pmb x^*)+\sum_{j=1}^p \mu_j \nabla h_j(\pmb x^*)=0\\ g_i(\pmb x^*)\leq 0,i=1,2,...,m\\ h_j(\pmb x^*)=0,j=1,2,...,p\\ \lambda_i g_i(\pmb x^*)=0,i=1,2,...,m\\ \lambda_i\geq 0,i=1,2,...,m \end{cases}$$
		- 求解：对这一新问题的最优解$\pmb x^*,\pmb \lambda^*,\pmb \mu^*$，其最优解$\pmb x^*$即为原问题的最优解
	- 本质：将原问题转换为了对偶问题，对偶问题的约束数量更少，约束更简单，且一定是一个更容易的凸优化问题。如果原问题也是一个凸优化问题，则原问题和对偶问题的最优解相同
		- 对偶函数：$$\Gamma(\pmb \lambda,\pmb \mu)=\min_{\pmb x}\mathcal{L}(\pmb x,\pmb \lambda,\pmb \mu)$$原始函数：$$\Pi(\pmb x)=\max_{\pmb \lambda,\pmb \mu}\mathcal{L}(\pmb x,\pmb \lambda,\pmb \mu)$$
		- 弱对偶条件：对任意可行解$\pmb x$和任意$\pmb \lambda\geq 0$，有$$\max_{\pmb\lambda,\pmb\mu}\Gamma(\pmb \lambda,\pmb \mu)\leq \min_{\pmb x}\Pi(\pmb x)$$
		- 对偶问题：$$\max_{\pmb \lambda,\pmb \mu}\Gamma(\pmb \lambda,\pmb \mu),s.t.\lambda_i\geq 0,i=1,2,...,m$$
## 对偶问题求解SVM
- SVM的原始问题：$$\min \frac{1}{2}||\pmb w||_2^2+C\sum_{i=1}^n\xi_i,s.t. y_i(\pmb w\cdot \pmb x_i+b)\geq 1-\xi_i,\xi_i\geq 0,1\leq i\leq n$$一个目标函数，$2n$个约束条件，算法复杂度较高
	- 拉格朗日函数：$$\mathcal{L}(\pmb w,b,\pmb \xi,\pmb \alpha,\pmb \beta)=\frac{1}{2}||\pmb w||_2^2+C\sum_{i=1}^n\xi_i-\sum_{i=1}^n \alpha_i[y_i(\pmb w\cdot \pmb x_i+b)-1+\xi_i]-\sum_{i=1}^n \beta_i \xi_i$$
		- 其中，$\alpha_i,\beta_i$为拉格朗日乘子，均需$\geq 0$
	- 对$\pmb w,b,\pmb \xi$求偏导，并令其为0，有：$$\begin{cases} \frac{\partial \mathcal{L}}{\partial \pmb w}=\pmb w-\sum_{i=1}^n \alpha_i y_i \pmb x_i=0\\ \frac{\partial \mathcal{L}}{\partial b}=-\sum_{i=1}^n \alpha_i y_i=0\\ \frac{\partial \mathcal{L}}{\partial \xi_i}=C-\alpha_i-\beta_i=0,1\leq i\leq n \end{cases}$$
	- 将上述结果代入拉格朗日函数，得到对偶函数：$$\Gamma(\pmb \alpha)=\sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j (\pmb x_i \cdot \pmb x_j)$$约束问题即为：$$\max_{\pmb \alpha}\Gamma(\pmb \alpha),s.t. \sum_{i=1}^n \alpha_i y_i=0,0\leq \alpha_i\leq C,1\leq i\leq n$$该问题为其**对偶问题**
		- 该问题为一个典型的二次规划问题，且约束转化为了一个线性约束和$n$个简单的边界约束，更**易求解**
		- 新问题的KKT条件：$$ \begin{cases} \alpha_i \geq 0,\quad \mu_i \geq 0 \\ y_i(\boldsymbol{w} \cdot \boldsymbol{x}_i + b) - 1 + \xi_i \geq 0 \\ \alpha_i(y_i(\boldsymbol{w} \cdot \boldsymbol{x}_i + b) - 1 + \xi_i) = 0 \\ \xi_i \geq 0,\quad \mu_i \xi_i = 0 \end{cases} $$
		- 解释：
			- 当$\alpha_i = 0$时，说明第$i$个样本点对分类超平面没有贡献，该样本点位于间隔之外且被正确分类
			- 当$0 < \alpha_i < C$时，说明第$i$个样本点为**支持向量**，且**位于间隔边界上**
			- 当$\alpha_i = C$时，说明第$i$个样本点为误分类点或位于间隔内的点，这些点对分类超平面的贡献达到最大值
			- 如图：![[Pasted image 20251020163334.png]]
## 原始问题求解：SGD方法
- 带线性约束问题的可微函数梯度求解法：
	- 需要问题有如下的标准形式：$$\begin{aligned} &\min_{\pmb x\in \mathbb R^d}f(\pmb x)\\ &s.t.A\pmb x=\pmb b \end{aligned}$$
	- 进行矩阵的配方，可以将问题化简为如下的形式：![[Pasted image 20251020164422.png]]
		- 精确求解至少需要$O((n+d)^2)$的时间复杂度，当$n$和$d$较大时，计算量较大
		- 解决：随机梯度下降方法
- 原始问题的SGD求解：
	- 损失函数：合页损失函数(Hinge loss) $$L(\pmb w,b,\pmb x_i,y_i)=\max(0,1-y_i(\pmb w\cdot \pmb x_i+b))$$![[Pasted image 20251020165301.png]]
	- 对其施加二范数正则化，有：$$\min_{\pmb w,b}\frac{1}{2}||\pmb w||_2^2+\frac{C}{n}\sum_{i=1}^n \max(0,1-y_i(\pmb w\cdot \pmb x_i+b))$$至此，将SVM问题统合到广义**线性模型**学习的[[L1 线性模型#线性回归|问题框架]]下，可以使用SGD等优化方法进行求解
# 核函数方法
- 前述的问题都解决的是线性可分的问题，现实中往往是线性不可分的
	- 解决：借助[[L1 线性模型#^515143|基函数]]的思路，将非线性问题作特征的映射，转化为线性问题
		- 映射函数：$\phi(\pmb x):\mathbb R^d\rightarrow \mathbb R^{d'}$，其中$d'>d$
		- 映射后的分类器：$h(\pmb x)=sign(\pmb w\cdot \phi(\pmb x)+b)$
		- 问题：
			- 特征提取的过程是基于假设的，对复杂的数据，难以设计合适的映射函数
			- 当特征空间维度较高时，计算复杂度会爆炸式增长
- 核函数：
	- 定义：对于映射函数$\phi(\pmb x)$，定义核函数$K(\pmb x_i,\pmb x_j)=\phi(\pmb x_i)\cdot \phi(\pmb x_j)$，即为**映射后**两个样本点的**内积**
		- 含义：描述了两个样本点在映射后的特征空间中的**相似度**
	- 多项式核函数：使用多项式函数作为基函数的映射$$\pmb \Phi(\pmb x)=(1,\sqrt{2}x_1,\sqrt{2}x_2,...,\sqrt{2}x_d,x_1^2,x_2^2,...,x_d^2,\sqrt{2}x_1x_2,...,\sqrt{2}x_{d-1}x_d)$$对应的核函数为：$$K(\pmb x_i,\pmb x_j)=(1+\pmb x_i\cdot \pmb x_j)^p$$
		- 





---
[[L3 学习理论]]