# 集成学习
- 定义：训练多个学习器，用多个学习器的组合来完成学习任务的方法
- 分类：
	- 并行集成（Parallel Ensemble）：多个学习器相互独立地训练，最后将它们的结果进行组合，如[[L5 学习(B)#随机森林|随机森林]]
		- 训练高效，但互补性一般
	- 串行集成（Sequential Ensemble）：多个学习器依次训练，后续的学习器依赖于前面的学习器的结果，如提升方法（Boosting）
		- 训练较慢，但互补性较强
- 理论基础：
	- 由泛化界理论，一味地增大模型复杂度会导致泛化界的增大，从而导致过拟合
	- 直观地，集成学习可以**降低组合之后模型的误差**，推导如下：
		- 对$T$个二分类器$\{h_t(\pmb x)\}_{t=1}^T$，使用最大投票的方式得到一个聚类学习器$$H(\pmb x)=\text{sign}\left(\sum_{t=1}^Th_t(\pmb x)\right)$$
		- 假设每一个学习器的误差为$\varepsilon_t=P(h_t(\pmb x)\neq y)$，且各个学习器之间相互独立
		- 则总误差可表示为：$$P(H(x) \neq y) = P\left( \sum_{t=1}^T \mathbf{1}[h_t(x) \neq y] \geq \frac{T}{2} \right)$$
		- 代入[[L3 学习理论#^0fddb4|Hoeffding不等式]]，可得：$$P(H(x) \neq y) \leq 2\exp\left(-\frac{1}{2}T(1-2\varepsilon)^2 \right)$$
	- 由表达式可以得到，误差是按指数级下降的，因此集成学习可以有效地降低误差，从而提升泛化能力
# 提升方法（Boosting）
- 弱学习器（Weak Learner）：分类误差率略低于随机猜测的学习器
	- 定义：对二分类学习器，若存在二分类学习算法$\mathcal A$，$\gamma > 0$，使得对于任意分布$D$，$\delta >0$，$c\in C$，当样本复杂度$n = \text{poly}(\frac{1}{\delta})$时，有$$P_{D_n\sim D^n}(\varepsilon_{\mathcal A}(h_{D_n})\leq \frac{1}{2}-\gamma)\geq 1-\delta$$则称$\mathcal A$为弱学习算法
	- 例：
		- 决策树桩（Decision Stump）：仅使用单个特征进行划分的决策树
		- 回归桩（Regression Stump）：仅使用单个特征进行划分的回归树
- 提升方法（Boosting）：
	- 思想：通过训练多个弱学习器，并将它们的结果进行组合，得到一个强学习器
		- 示意：![[Pasted image 20251124154505.png]]
	- 建模：
		- 第$t$步之后，聚类学习器为：$$f_t(\pmb x)=\sum_{s=1}^t\alpha_sh_s(\pmb x)$$二值化后得到分类器：$h(\pmb x)=\text{sign}(f_t(\pmb x))$
		- 转化为优化问题即为：需要找到一组$\{(\alpha_i,h_i)\}_{i=1}^t$，使得：$$(\alpha_1, ..., \alpha_t, h_1, ..., h_t) = \underset{(\alpha_1, ..., \alpha_t, h_1, ..., h_t)}{\operatorname{argmin}} \frac{1}{n} \sum_{i=1}^n \ell\left( y_i, \sum_{s=1}^t \alpha_s h_s(x_i) \right)$$
		- 由于联合优化显然是计算复杂的，因此提升方法采用贪心的方式进行优化，也即每一步优化：$$(\alpha_t, h_t) = \underset{(\alpha_t, h_t)}{\operatorname{argmin}} \frac{1}{n} \sum_{i=1}^n \ell\left( y_i, f_{t-1}(x_i) + \alpha_t h_t(x_i) \right)$$其中$f_{t-1}(x)=\sum_{s=1}^{t-1}\alpha_sh_s(x)$，也即第$t-1$步的聚类学习器
	- 训练方法：按序列优化（也即串行集成）的方式，依次训练多个弱学习器
# 自适应提升（AdaBoost）
- 常用的损失函数：
	- 类型：
		- 0-1损失函数：$\ell(y,f(x))=\mathbf{1}\{y\neq \text{sign}(f(x))\}$
		- 合页损失函数（Hinge Loss）：$\ell(y,f(x))=\max(0,1-yf(x))$
		- 逻辑斯特损失函数（Logistic Loss）：$\ell(y,f(x))=\log(1+\exp(-yf(x)))$
		- 平方损失函数（Quadratic Loss）：$\ell(y,f(x))=(1-f(x))^2_{x\leq 1}$
		- **指数损失函数**（Exponential Loss）：$\ell(y,f(x))=\exp(-yf(x))$
		- 如图：![[Pasted image 20251124160111.png]]
		- 性质：
			- 都是0-1损失函数的上界（由这一放缩过程导致的相对于0-1损失函数的误差称为**优化误差**）
			- 均为凸函数，便于优化
- 自适应提升方法：使用指数损失函数进行提升的方法
	- 算法描述：![[Pasted image 20251124160242.png]]
		- 初始化：对样本赋予均匀权重$D_1(i)=\frac{1}{n}$
		- 迭代过程：
			- 在第$t$步，使用加权数据集训练弱学习器$h_t$，计算加权错误率：$$\varepsilon_t=\sum_{i=1}^nD_t(i)\mathbf{1}\{h_t(x_i)\neq y_i\}=P_{i\sim D_t}(h_t(x_i)\neq y_i)$$
			- 计算弱学习器的权重：$$\alpha_t=\frac{1}{2}\log\frac{1-\varepsilon_t}{\varepsilon_t}$$误差越小，权重越大
			- 更新样本权重：$$D_{t+1}(i)=\frac{D_t(i)\exp(-\alpha_ty_ih_t(x_i))}{Z_t}$$其中$Z_t$为归一化因子
	- 迭代公式：
		- 分析可知，AdaBoost的迭代过程实际上是一个[[L4 动态规划|动态规划]]的过程
		- 推导：$$ \begin{align*} D_{t+1}(i) &= \frac{D_t(i) e^{-\alpha_t y_i h_t(x_i)}}{Z_t} = \frac{D_{t-1}(i) e^{-\alpha_{t-1} y_i h_{t-1}(x_i)} e^{-\alpha_t y_i h_t(x_i)}}{Z_{t-1} Z_t} \\ &= \cdots = \frac{1}{n} \frac{e^{-y_i \sum_{s=1}^t \alpha_s h_s(x_i)}}{\prod_{s=1}^t Z_s} = \frac{1}{n} \frac{e^{-y_i f_t(x_i)}}{\prod_{s=1}^t Z_s} \end{align*} $$借助指数函数的可乘性，可以将总的指数项拆分为每一步的指数项的乘积，进而使用动态规划甚至贪心的方式进行计算
	- 坐标下降优化：
		- 概念：将多变量优化问题转化为单变量优化问题，通过依次优化每一个变量来逼近全局最优解的方法
			- 对比梯度下降使用所有变量的梯度信息进行联合优化，坐标下降每次仅使用一个变量的偏导数进行优化
		- 等价性：可以证明，AdaBoost的迭代过程实际上等价于对指数损失函数，在每个假设$h_i$和权重$\alpha_i$上进行坐标下降优化
	- 运行示意：![[Pasted image 20251124180046.png]]
		- 在每次迭代时将错误的点赋予更大的权重，从而使得后续的弱学习器更加关注这些错误点
	- 性能：预排序复杂度$O(nd\log n)$，每次迭代复杂度$O(nd)$，总复杂度$O(Tnd+nd\log n)$，高效且避免过拟合
## 经验误差界



## 泛化误差界





# 梯度提升（Gradient Boosting）
