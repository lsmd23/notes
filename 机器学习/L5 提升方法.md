# 集成学习
- 定义：训练多个学习器，用多个学习器的组合来完成学习任务的方法
- 分类：
	- 并行集成（Parallel Ensemble）：多个学习器相互独立地训练，最后将它们的结果进行组合，如[[L5 学习(B)#随机森林|随机森林]]
		- 训练高效，但互补性一般
	- 串行集成（Sequential Ensemble）：多个学习器依次训练，后续的学习器依赖于前面的学习器的结果，如提升方法（Boosting）
		- 训练较慢，但互补性较强
- 理论基础：
	- 由泛化界理论，一味地增大模型复杂度会导致泛化界的增大，从而导致过拟合
	- 直观地，集成学习可以**降低组合之后模型的误差**，推导如下：
		- 对$T$个二分类器$\{h_t(\pmb x)\}_{t=1}^T$，使用最大投票的方式得到一个聚类学习器$$H(\pmb x)=\text{sign}\left(\sum_{t=1}^Th_t(\pmb x)\right)$$
		- 假设每一个学习器的误差为$\varepsilon_t=P(h_t(\pmb x)\neq y)$，且各个学习器之间相互独立
		- 则总误差可表示为：$$P(H(x) \neq y) = P\left( \sum_{t=1}^T \mathbf{1}[h_t(x) \neq y] \geq \frac{T}{2} \right)$$
		- 代入[[L3 学习理论#^0fddb4|Hoeffding不等式]]，可得：$$P(H(x) \neq y) \leq 2\exp\left(-\frac{1}{2}T(1-2\varepsilon)^2 \right)$$
	- 由表达式可以得到，误差是按指数级下降的，因此集成学习可以有效地降低误差，从而提升泛化能力
# 提升方法（Boosting）
- 弱学习器（Weak Learner）：分类误差率略低于随机猜测的学习器
	- 定义：对二分类学习器，若存在二分类学习算法$\mathcal A$，$\gamma > 0$，使得对于任意分布$D$，$\delta >0$，$c\in C$，当样本复杂度$n = \text{poly}(\frac{1}{\delta})$时，有$$P_{D_n\sim D^n}(\varepsilon_{\mathcal A}(h_{D_n})\leq \frac{1}{2}-\gamma)\geq 1-\delta$$则称$\mathcal A$为弱学习算法
	- 例：
		- 决策树桩（Decision Stump）：仅使用单个特征进行划分的决策树
		- 回归桩（Regression Stump）：仅使用单个特征进行划分的回归树
- 提升方法（Boosting）：
	- 思想：通过训练多个弱学习器，并将它们的结果进行组合，得到一个强学习器
		- 示意：![[Pasted image 20251124154505.png]]
	- 建模：
		- 第$t$步之后，聚类学习器为：$$f_t(\pmb x)=\sum_{s=1}^t\alpha_sh_s(\pmb x)$$二值化后得到分类器：$h(\pmb x)=\text{sign}(f_t(\pmb x))$
		- 转化为优化问题即为：需要找到一组$\{(\alpha_i,h_i)\}_{i=1}^t$，使得：$$(\alpha_1, ..., \alpha_t, h_1, ..., h_t) = \underset{(\alpha_1, ..., \alpha_t, h_1, ..., h_t)}{\operatorname{argmin}} \frac{1}{n} \sum_{i=1}^n \ell\left( y_i, \sum_{s=1}^t \alpha_s h_s(x_i) \right)$$
		- 由于联合优化显然是计算复杂的，因此提升方法采用贪心的方式进行优化，也即每一步优化：$$(\alpha_t, h_t) = \underset{(\alpha_t, h_t)}{\operatorname{argmin}} \frac{1}{n} \sum_{i=1}^n \ell\left( y_i, f_{t-1}(x_i) + \alpha_t h_t(x_i) \right)$$其中$f_{t-1}(x)=\sum_{s=1}^{t-1}\alpha_sh_s(x)$，也即第$t-1$步的聚类学习器
	- 训练方法：按序列优化（也即串行集成）的方式，依次训练多个弱学习器
# 自适应提升（AdaBoost）
- 常用的损失函数：
	- 类型：
		- 0-1损失函数：$\ell(y,f(x))=\mathbf{1}\{y\neq \text{sign}(f(x))\}$
		- 合页损失函数（Hinge Loss）：$\ell(y,f(x))=\max(0,1-yf(x))$
		- 逻辑斯特损失函数（Logistic Loss）：$\ell(y,f(x))=\log(1+\exp(-yf(x)))$
		- 平方损失函数（Quadratic Loss）：$\ell(y,f(x))=(1-f(x))^2_{x\leq 1}$
		- **指数损失函数**（Exponential Loss）：$\ell(y,f(x))=\exp(-yf(x))$
		- 如图：![[Pasted image 20251124160111.png]]
		- 性质：
			- 都是0-1损失函数的上界（由这一放缩过程导致的相对于0-1损失函数的误差称为**优化误差**）
			- 均为凸函数，便于优化
- 自适应提升方法：使用指数损失函数进行提升的方法
	- 算法描述：![[Pasted image 20251124160242.png]]
		- 初始化：对样本赋予均匀权重$D_1(i)=\frac{1}{n}$
		- 迭代过程：
			- 在第$t$步，使用加权数据集训练弱学习器$h_t$，计算加权错误率：$$\varepsilon_t=\sum_{i=1}^nD_t(i)\mathbf{1}\{h_t(x_i)\neq y_i\}=P_{i\sim D_t}(h_t(x_i)\neq y_i)$$
			- 计算弱学习器的权重：$$\alpha_t=\frac{1}{2}\log\frac{1-\varepsilon_t}{\varepsilon_t}$$误差越小，权重越大
			- 更新样本权重：$$D_{t+1}(i)=\frac{D_t(i)\exp(-\alpha_ty_ih_t(x_i))}{Z_t}$$其中$Z_t$为归一化因子
	- 迭代公式：
		- 分析可知，AdaBoost的迭代过程实际上是一个[[L4 动态规划|动态规划]]的过程
		- 推导：$$ \begin{align*} D_{t+1}(i) &= \frac{D_t(i) e^{-\alpha_t y_i h_t(x_i)}}{Z_t} = \frac{D_{t-1}(i) e^{-\alpha_{t-1} y_i h_{t-1}(x_i)} e^{-\alpha_t y_i h_t(x_i)}}{Z_{t-1} Z_t} \\ &= \cdots = \frac{1}{n} \frac{e^{-y_i \sum_{s=1}^t \alpha_s h_s(x_i)}}{\prod_{s=1}^t Z_s} = \frac{1}{n} \frac{e^{-y_i f_t(x_i)}}{\prod_{s=1}^t Z_s} \end{align*} $$借助指数函数的可乘性，可以将总的指数项拆分为每一步的指数项的乘积，进而使用动态规划甚至贪心的方式进行计算
	- 坐标下降优化：
		- 概念：将多变量优化问题转化为单变量优化问题，通过依次优化每一个变量来逼近全局最优解的方法
			- 对比梯度下降使用所有变量的梯度信息进行联合优化，坐标下降每次仅使用一个变量的偏导数进行优化
		- 等价性：可以证明，AdaBoost的迭代过程实际上等价于对指数损失函数，在每个假设$h_i$和权重$\alpha_i$上进行坐标下降优化
	- 运行示意：![[Pasted image 20251124180046.png]]
		- 在每次迭代时将错误的点赋予更大的权重，从而使得后续的弱学习器更加关注这些错误点
	- 性能：预排序复杂度$O(nd\log n)$，每次迭代复杂度$O(nd)$，总复杂度$O(Tnd+nd\log n)$，高效且避免过拟合
## 经验误差界
- **定理**：AdaBoost输出的经验误差满足$$\hat{\varepsilon}(h) \leq \exp\left[-2\sum_{t=1}^T \left(\frac{1}{2} - \epsilon_t\right)^2\right]$$进一步，若对所有$t\in [T]$，存在$\gamma \leq (\frac{1}{2}+\epsilon_t)$，则有$$\hat{\varepsilon}(h) \leq \exp(-2T\gamma^2)$$
	- 该定理说明了AdaBoost的经验误差随着弱学习器数量的增加而**指数级下降**
	- 经验误差界表明，在数据集良好的情况下，AdaBoost可以通过增加弱学习器的数量来显著**降低经验误差**，从而提升模型的拟合能力
	- *证明过程*：
		- 由指示函数与指数函数的关系 $\mathbf{1}[y_i f(x_i) \leq 0] \leq e^{-y_i f(x_i)}$，结合样本分布的迭代公式 $D_{t+1}(i) = \frac{1}{n} \cdot \frac{e^{-y_i f_t(x_i)}}{\prod_{s=1}^t Z_s}$，可推导出，经验误差 $\hat{\varepsilon}(h) \leq \prod_{t=1}^T Z_t$，其中$Z_t$为归一化因子
		- 由：$$Z_t = \sum_{i=1}^n D_t(i) e^{-\alpha_t y_i h_t(x_i)} = (1 - \epsilon_t)e^{-\alpha_t} + \epsilon_t e^{\alpha_t}$$对 $\alpha_t$ 求导并令导数为0，可得最优$\alpha_t = \frac{1}{2}\log\frac{1 - \epsilon_t}{\epsilon_t}$，代入后 $Z_t$ 的最小值为 $2\sqrt{\epsilon_t(1 - \epsilon_t)}$ 
		- 利用 $\sqrt{1 - 4\left(\frac{1}{2} - \epsilon_t\right)^2} \leq \exp\left[-2\left(\frac{1}{2} - \epsilon_t\right)^2\right]$（由$1 - x \leq e^{-x}$ 推导），最终得到经验误差的指数上界
## 泛化误差界
- 基于[[L3 学习理论#VC维度(Vapnik-Chervonenkis Dimension)|VC维]]的推导：
	- 假设基学习器$\mathcal H$的VC维为$d$，则提升后的学习器$\mathcal{F}_T=\text{AdaBoost}(\mathcal H, T)$的VC维满足：$$\text{VCdim}(\mathcal{F}_T) \leq 2(T+1)(d+1)\log((T+1)e)$$
	- 可以看出，当迭代次数$T$增大时，其VC维也会增大，理应导致过拟合
	- 实际实验中，即使反复迭代，依然没有观察到过拟合现象![[Pasted image 20251124214114.png]]因此，需要不同于由0-1损失函数推导出的泛化界理论的新理论来解释这一现象
- 边际理论：
	- 边际损失函数：对置信边际$\rho > 0$，$\rho$-边际损失函数为$L_\rho:\mathbb R \rightarrow \mathbb R,L_\rho(y, \hat{y}) = \Phi_\rho(y\hat{y})$，其中$\Phi_\rho(x)$是分段函数： $$ \Phi_\rho(x) = \begin{cases} 1, & x \leq 0 \\ 1 - \frac{x}{\rho}, & 0 < x \leq \rho \\ 0, & \rho < x \end{cases} $$
		- 函数图像：![[Pasted image 20251124214529.png]]
	- 经验边际损失：定义经验边际损失为：$$ \hat{\varepsilon}_\rho(h) = \frac{1}{n} \sum_{i=1}^n \Phi_\rho(y_i h(x_i)) $$
		- 性质：相比0-1损失，经验边际损失在$[0, \rho]$区间内对分类结果进行了线性放缩
	- 定理：设$\mathcal{H}$是实值函数类，固定$\rho > 0$，对任意$\delta > 0$，以至少$1-\delta$的概率，对所有$h \in \mathcal{H}$，有：$$ \varepsilon_D(h) \leq \hat{\varepsilon}_{\mathcal{D}_n}(h) + \frac{2}{\rho}\mathcal{R}_n(\mathcal{H}) + \sqrt{\frac{\log(1/\delta)}{2n}} $$且$$ \varepsilon_D(h) \leq \hat{\varepsilon}_{\mathcal{D}_n}(h) + \frac{2}{\rho}\hat{\mathcal{R}}_n(\mathcal{H}) + 3\sqrt{\frac{\log(2/\delta)}{2n}} $$其中$\mathcal{R}_n(\mathcal{H})$是期望拉德马赫复杂度，$\hat{\mathcal{R}}_n(\mathcal{H})$是经验拉德马赫复杂度
	- 由此，可以看到，其泛化性能由边际大小$\rho$和基学习器类的复杂度$\mathcal{R}_n(\mathcal{H})$共同决定，而不是单纯的迭代次数$T$
# 梯度提升（Gradient Boosting）
- AdaBoost使用指数损失函数，对离群点不鲁棒
- 梯度提升方法：
	- 推导：
		- 由前文可知，序列优化目标为：$$(\alpha_t, h_t) = \underset{(\alpha_t, h_t)}{\operatorname{argmin}} \frac{1}{n} \sum_{i=1}^n \ell\left( y_i, f_{t-1}(x_i) + \alpha_t h_t(x_i) \right)$$
		- 参数：$\pmb u_t = (\alpha_th_t(\pmb x_1),\cdots, \alpha_th_t(\pmb x_n))$表示新增的带权的弱学习器在所有样本上的预测值
		- 参数：$\pmb v_t = (f_{t-1}(\pmb x_1),\cdots, f_{t-1}(\pmb x_n))$表示当前聚类学习器在所有样本上的预测值
		- 定义目标函数$F(\pmb u_t,\pmb v_t) = \frac{1}{n}\sum_{i=1}^n\ell(y_i,v_{ti}+u_{ti})$，则优化目标为选择一个$\pmb u_t$使得$F(\pmb u_t,\pmb v_t)$最小
		- 视函数$F(\pmb u_t,\pmb v_t) = \frac{1}{n}\sum_{i=1}^n\ell(y_i,v_{ti}+u_{ti})$为只关于$\pmb v_t$的函数$G(\pmb v_t) = F(\pmb u_t,\pmb v_t)|_{\pmb{u}_t}$，为使其最小化，可以使用梯度下降的方法进行优化：$$\pmb {v}_t - \eta \nabla_{\pmb{v}_t} \sum_{i=1}^n \ell(y_i, v_{ti})$$
		- 因此，只需要使得$\pmb u_t$与$-\nabla_{\pmb{v}_t} \sum_{i=1}^n \ell(y_i, v_{ti})$尽可能接近即可
	- 定义：对$u_{ti}=\alpha_th_t(\pmb x_i)$，令第$t$次迭代后的最佳的基学习器取值为：$$h_t(\pmb{x}_i) \approx -\nabla_{v_{ti}} \sum_{i=1}^n \ell(y_i, v_{ti}) = -\nabla_{v_{ti}} \ell(y_i, v_{ti})$$同时，$\alpha_t$取梯度下降的学习率
	- 和梯度优化的对比：
		- 示意图：![[Pasted image 20251124220939.png]]
		- 梯度下降优化直接在假设空间函数上优化，依照损失函数对参数的梯度优化
		- 梯度提升方法则是求损失函数对第$t-1$次聚类学习器的输出结果的梯度，然后训练一个弱学习器去拟合这个梯度，从而更新聚类学习器
- 前向分布加法建模（Forward Stagewise Additive Modeling）：梯度提升方法的具体实现
	- 流程：
		1. 初始化：$f_0\leftarrow 0$
		2. 迭代更新（第$t$轮）：
			- 前$t-1$轮后模型：$f_{t-1} = \sum_{s=1}^{t-1} \alpha_s h_s$
			- 定义预测向量：$\pmb{v}_t = (f_{t-1}(x_1), ..., f_{t-1}(x_n))$
			- 步方向（弱学习器）：$h_t \approx -\nabla_{\pmb{v}_t} \ell$（拟合损失函数的负梯度，即伪残差）
			- 步长：$\alpha_t > 0$（学习率）
			- 模型更新：$f_t = f_{t-1} + \alpha_t h_t$
	- 条件：要求损失函数对输出结果是可微的，不要求基学习器的特点
## 梯度提升机(GBM)
- L2损失函数下的梯度提升方法
	- 损失函数：$\ell(y,f(x))=(y-f(x))^2$
	- 聚类后的损失函数：$$\ell(\pmb v_t)=\sum_{i=1}^n(y_i-v_{ti})^2$$
	- 求梯度：$$-\partial_{v_{ti}} \ell(\pmb{v}_t) = -\partial_{v_{ti}} (v_{ti} - y_i)^2 = 2(y_i - v_{ti}) = 2(y_i - f_{t-1}(\pmb{x}_i))$$ 
	- 故可以得到：$$h_t = \underset{h}{\operatorname{argmin}} \sum_{i=1}^n \left( h(\pmb{x}_i) - 2(y_i - f_{t-1}(\pmb{x}_i)) \right)^2$$
	- 示例：![[Pasted image 20251124221935.png]]
- 二分类逻辑回归下的梯度提升方法
	- 损失函数：$\ell(y,f(x))=\log(1+e^{-yf(x)})$
	- 聚类后的损失函数：$$\ell(\pmb v_t)=\sum_{i=1}^n\log(1+\exp(-y_iv_{ti}))$$
	- 求梯度：$$-\partial_{v_{ti}} \ell(\pmb{v}_t) = -\partial_{v_{ti}} \log(1+e^{-y_iv_{ti}}) = \frac{y_i}{1+e^{y_iv_{ti}}} = \frac{y_i}{1+e^{y_if_{t-1}(\pmb{x}_i)}}$$ 
	- 故可以得到：$$h_t = \underset{h}{\operatorname{argmin}} \sum_{i=1}^n \left( h(\pmb{x}_i) - \frac{y_i}{1+e^{y_if_{t-1}(\pmb{x}_i)}} \right)^2$$
	- 示例：![[Pasted image 20251124222210.png]]
---
[[L6 强化学习]]