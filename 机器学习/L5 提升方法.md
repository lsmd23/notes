# 集成学习
- 定义：训练多个学习器，用多个学习器的组合来完成学习任务的方法
- 分类：
	- 并行集成（Parallel Ensemble）：多个学习器相互独立地训练，最后将它们的结果进行组合，如[[L5 学习(B)#随机森林|随机森林]]
		- 训练高效，但互补性一般
	- 串行集成（Sequential Ensemble）：多个学习器依次训练，后续的学习器依赖于前面的学习器的结果，如提升方法（Boosting）
		- 训练较慢，但互补性较强
- 理论基础：
	- 由泛化界理论，一味地增大模型复杂度会导致泛化界的增大，从而导致过拟合
	- 直观地，集成学习可以**降低组合之后模型的误差**，推导如下：
		- 对$T$个二分类器$\{h_t(\pmb x)\}_{t=1}^T$，使用最大投票的方式得到一个聚类学习器$$H(\pmb x)=\text{sign}\left(\sum_{t=1}^Th_t(\pmb x)\right)$$
		- 假设每一个学习器的误差为$\varepsilon_t=P(h_t(\pmb x)\neq y)$，且各个学习器之间相互独立
		- 则总误差可表示为：$$P(H(x) \neq y) = P\left( \sum_{t=1}^T \mathbf{1}[h_t(x) \neq y] \geq \frac{T}{2} \right)$$
		- 代入[[L3 学习理论#^0fddb4|Hoeffding不等式]]，可得：$$P(H(x) \neq y) \leq 2\exp\left(-\frac{1}{2}T(1-2\varepsilon)^2 \right)$$
	- 由表达式可以得到，误差是按指数级下降的，因此集成学习可以有效地降低误差，从而提升泛化能力
# 提升方法（Boosting）
- 弱学习器（Weak Learner）：分类误差率略低于随机猜测的学习器
	- 定义：对二分类学习器，若存在二分类学习算法$\mathcal A$，$\gamma > 0$，使得对于任意分布$D$，$\delta >0$，$c\in C$，当样本复杂度$n = \text{poly}(\frac{1}{\delta})$时，有$$P_{D_n\sim D^n}(\varepsilon_{\mathcal A}(h_{D_n})\leq \frac{1}{2}-\gamma)\geq 1-\delta$$则称$\mathcal A$为弱学习算法
	- 例：
		- 决策树桩（Decision Stump）：仅使用单个特征进行划分的决策树
		- 回归桩（Regression Stump）：仅使用单个特征进行划分的回归树
- 提升方法（Boosting）：
	- 思想：通过训练多个弱学习器，并将它们的结果进行组合，得到一个强学习器
		- 示意：![[Pasted image 20251124154505.png]]
	- 建模：
		- 第$t$步之后，聚类学习器为：$$f_t(\pmb x)=\sum_{s=1}^t\alpha_sh_s(\pmb x)$$二值化后得到分类器：$h(\pmb x)=\text{sign}(f_t(\pmb x))$
		- 转化为优化问题即为：需要找到一组$\{(\alpha_i,h_i)\}_{i=1}^t$，使得：$$(\alpha_1, ..., \alpha_t, h_1, ..., h_t) = \underset{(\alpha_1, ..., \alpha_t, h_1, ..., h_t)}{\operatorname{argmin}} \frac{1}{n} \sum_{i=1}^n \ell\left( y_i, \sum_{s=1}^t \alpha_s h_s(x_i) \right)$$
	- 训练方法：按序列优化（也即串行集成）的方式，依次训练多个弱学习器





# 自适应提升（AdaBoost）
- 损失函数：
	- 类型：
		- 如图：![[Pasted image 20251124160111.png]]
	- 优化误差：
- 自适应提升方法：
	- 算法描述：![[Pasted image 20251124160242.png]]
## 经验误差界



## 泛化误差界





# 梯度提升（Gradient Boosting）
