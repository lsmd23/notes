# 集成学习
- 定义：训练多个学习器，用多个学习器的组合来完成学习任务的方法
- 分类：
	- 并行集成（Parallel Ensemble）：多个学习器相互独立地训练，最后将它们的结果进行组合，如[[L5 学习(B)#随机森林|随机森林]]
		- 训练高效，但互补性一般
	- 串行集成（Sequential Ensemble）：多个学习器依次训练，后续的学习器依赖于前面的学习器的结果，如提升方法（Boosting）
		- 训练较慢，但互补性较强
- 理论基础：
	- 由泛化界理论，一味地增大模型复杂度会导致泛化界的增大，从而导致过拟合
	- 直观地，集成学习可以**降低组合之后模型的误差**，推导如下：
		- 对$T$个二分类器$\{h_t(\pmb x)\}_{t=1}^T$，使用最大投票的方式得到一个聚类学习器$$H(\pmb x)=\text{sign}\left(\sum_{t=1}^Th_t(\pmb x)\right)$$


# 提升方法（Boosting）
- 弱学习器（Weak Learner）：分类误差率略低于随机猜测的学习器
	- 定义：对二分类学习器，若存在二分类学习算法$\mathcal A$，$\gamma > 0$，使得对于任意分布$D$，
	- 例：
		- 决策树桩（Decision Stump）：仅使用单个特征进行划分的决策树
		- 回归桩（Regression Stump）：仅使用单个特征进行划分的回归树
- 提升方法（Boosting）：
	- 思想：通过训练多个弱学习器，并将它们的结果进行组合，得到一个强学习器
		- 示意：![[Pasted image 20251124154505.png]]
	- 建模：
	- 训练方法：按序列优化（也即串行集成）的方式，依次训练多个弱学习器





# 自适应提升（AdaBoost）
- 损失函数：
	- 类型：
		- 如图：![[Pasted image 20251124160111.png]]
	- 优化误差：
- 自适应提升方法：
	- 算法描述：![[Pasted image 20251124160242.png]]
## 经验误差界



## 泛化误差界





# 梯度提升（Gradient Boosting）
