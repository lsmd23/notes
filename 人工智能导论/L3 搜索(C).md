# 计算机博弈问题
- 存在多于一个的智能体对同一个问题进行交互的问题
- 分类标准：问题的随机性，信息的完备性，博弈的智能体数目，博弈是否同时进行，博弈是否是零和博弈……
- 算法给出对于当前局面的一个即时性的博弈策略
- 标准博弈问题：确定的，信息完备的，轮流的，只有两方参与的，零和的博弈问题
	- 这类问题可以形式化为搜索问题：![[Pasted image 20250310152947.png]]
		- **效用函数**：在博弈问题中用于评估结果对当前玩家的情况，效用函数越大，对当前决策的智能体越有利
	- 典例：各种棋类问题![[Pasted image 20250310152930.png]]
# 对抗搜索
- 树搜索与价值函数：
	- 对一个标准的博弈问题，建立一个搜索树
	- **价值函数**：衡量当前节点在整个决策路径中的价值大小
		- 对叶节点（即终局节点），其效用函数即为其价值函数
		- 对非叶节点，其所有子节点的价值函数的**最大值**即为当前节点的价值函数
		- 例：吃豆人![[Pasted image 20250310155348.png]]
- 博弈树：
	- 在上述的问题中，引入第二个智能体，对树节点按照当前决策的智能体交替染色
	- 对零和博弈，对手智能体的决策路线（价值函数）与当前智能体是相反的，即对手应当尽可能选取小的价值函数决策
	- 例：![[Pasted image 20250310171644.png]]
## 极小极大算法
- 思想：按照最小最大的思想，选取合适的节点进行决策
	- 例：![[Pasted image 20250310171744.png]]
- 算法实现：
	- 使用深度优先搜索计算节点的价值函数
	- 按照极小极大的思路选取当前的执行策略
	- 伪代码实现：![[Pasted image 20250310154406.png]]
	- 性质：
		- 对两边的最优决策，分别记为$\pi_{max},\pi_{min}$，实际决策记为$\pi_{agent},\pi_{opp}$
		- $\forall \pi_{opp},V(\pi_{max},\pi_{min})\leq V(\pi_{max},\pi_{opp})$
		- $\forall \pi_{agent},V(\pi_{max},\pi_{min})\geq V(\pi_{agent},\pi_{min})$
	- 推广化的极小极大算法：可以推广到多方博弈的非零和博弈问题——只需要考虑对每个智能体的价值函数的最大值的节点即可
	- 复杂度：类似深度搜索的穷举法搜索，时间$O(b^m)$，空间$O(bm)$
## Alpha-Beta剪枝
- 对上述极小极大算法，可以做复杂度上的优化
	- 当搜素进行到某层时，考虑其孙节点的遍历情况。若能得到孙节点的情况，则根据最大最小的搜索逻辑，其限制了子节点的上/下界，进而可以进行剪枝
	- 例：![[Pasted image 20250310172249.png]]
		- 当搜索到第二个子节点的孙节点，计算得到价值函数2时，其父节点的价值函数不可能超过2，因而该子节点分支不会被当前节点的max算法所考虑
		- 从第三个子节点的孙节点搜索情况来看，搜索的顺序会决定剪枝发生的可能，因而是一种优化方式
- $\alpha-\beta$剪枝算法：
	- 伪码实现：![[Pasted image 20250310172536.png]]
	- 说明：
		- 对每个节点，维护一组$(\alpha,\beta)$值
		- **α 是“底线”**：对 Max 方来说，“我至少要拿到 α 的收益，否则不如另寻他路”。
		- **β 是“天花板”**：对 Min 方来说，“我最多只能让对手拿到 β 的收益，否则必须阻止这条路”。
		- **传递方向**：父节点的 α/β 会约束子节点的搜索范围，而子节点的结果反过来又影响父节点的 α/β，形成动态调整。也即，在整个搜索时，会自下而上地不断调整这一对数的值，而传递时，是自上而下地传递这组值
	- 优化：节点搜索的顺序影响剪枝的数目，进而影响搜索的结果
		- 最差情况下，不会降低复杂度
		- 最好情况下，复杂度可以降低到$O(b^{0.5m})$
		- 采用随机的策略进行，平均而言将复杂度降低到$O(b^{0.75m})$
## 实时决策搜索
- 实际问题中，仅仅作$\alpha-\beta$剪枝优化依然远远不够解决实际的搜索问题，需要在搜索深度上做进一步的优化（即截断搜索）
- **评估函数**：采取类似启发推理的方式，对节点做前向的推理，从而大大减少搜索的深度
	- 对每个状态，抽取其特征，按照某种方式进行计算，可以得到一个数据，即为评估函数
	- 评估函数以推理的方式评估目前局面的好坏
	- 实际问题中，评估函数可以通过学习的方式获取到比较好的评估函数
	- 例：对国际象棋，可以按子力价值、移动空间、王的安全性和中心控制程度计算评估函数![[Pasted image 20250310173243.png]]
- 截断搜索：
	- 结合评估函数，设定搜索深度的阈值，在达到最大的搜索深度时，采用评估函数决定搜索的结果
	- 算法：![[Pasted image 20250310174516.png]]
		- 特点：需要在前向评估推理和实际的搜索之间做权衡
	- 优化——静态搜索：
		- 前向搜索只在静态的位置，也即向下的评估函数没有重大改变的位置停止
		- 例：国际象棋中，不能在可能有大子在下一步丢失的位置停止搜索
	- 优化——图搜索与末端优化：
		- 避免对同一局面的多次重复搜索，可以引入探索集变为图搜索
		- 对末端的情况，可以进行逆向的枚举，从而提高递归的边界
# 蒙特卡洛树搜索
- 问题：对围棋问题，由于分支数和深度十分庞大，且难以找到合适的评估函数，用传统的树搜索难以进行优化搜索
	- 思路：纵向上，想办法找到合适的评估策略；横向上，做选择性的剪枝
- 蒙特卡洛树搜索：
	- 是生成式的树搜索，每次由一个当前盘面出发，不断向下伸展决策树，最终找到最合适的解决策略
	- 步骤：选择，扩展，模拟，反向传播![[Pasted image 20250310175112.png]]
		1. 选择：策略——老虎机模型
			- 实际问题中，对一个情况的信息随着试验的次数增多而增多，但盲目扩展试验的对象会大大增大计算量，产生冗余计算
			- UCB公式：$$value=v_i+C\times\sqrt{\frac{\ln(N)}{n_i}}$$其中 $v_i$ 指当前节点的评估值，$n_i$ 指当前节点的试验次数，$N$ 指父节点的试验次数
			- 这一选择评估指标倾向于选择高价值的节点，同时倾向于选择未被探索过的节点，属于横向的剪枝算法
			- 例：![[Pasted image 20250310175612.png]]
		2. 扩展：选择到叶子节点后，向下按照交替染色的逻辑，延伸出一个未被探索过的节点
			- 例：![[Pasted image 20250310175705.png]]
		3. 模拟：按照某种策略，进行一定深度的模拟对弈（可以采用随机的策略进行，也可以利用学习的方式进行更高效更好的启发式对弈）
			- 例：![[Pasted image 20250310175827.png]]
		4. 反向传播：将对弈的结果向上传播到当前的节点，更新数据
			- ![[Pasted image 20250310175900.png]]
	- 决策思路：选取遍历次数多的节点进行决策，次数差不多时可能需要一定程度的权衡
- 性质：
	- 搜索树的结果是非对称的，符合现实世界的解的规律
	- 不受分支的影响，易于适配新的问题
	- 可以继承学习的启发式的函数辅助搜索
	- 可以随时停止探索进行决策
---
[[L4 学习(A)]]