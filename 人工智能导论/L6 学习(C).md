# 强化学习
- 基本框架：环境，状态转移与动作，奖励收益。学习的目标是最大化奖励收益的值。
## 马尔可夫决策过程(MDP)
* 定义： 五元组$<S,P,A,R,\gamma>$
	* 状态集合$S$：刻画所有的状态
	* 动作集合$A$：刻画所有的动作
	* 奖励因子$R$：对一个确定的状态和动作$(s,a)$，所能获得的奖励因子
	* 转移概率矩阵$P$：对确定的转移动作和状态，刻画转移到不同状态的概率的矩阵$P_{SAS'}=\mathbb P[S_{t+1}=s'|S_{t}=s,A_t=a]$
	* 折扣因子$\gamma$：用于对更靠后的奖励值的修正，值越小，表示越不重视靠后的奖励值
	* 行为：
		* $t=0$时，对应一个初始状态$s_0$
		* 直到终止时刻，进行如下的迭代：
			1. 选择一个动作$a_t$
			2. 根据奖励函数，返回奖励值$r_t$
			3. 根据转移概率矩阵，返回次态$s_{t+1}$
			4. 进行状态变换并收到奖励值
- 强化学习的策略：
	* 策略$\pmb \pi$：$S\rightarrow A$，刻画选择动作的偏好，强化学习的目标是使得某个策略$\pmb \pi^*$的累积奖励函数$\sum_{t\geq 0}\gamma^tr_t$取得最大值
	* 由于概率矩阵的存在，整个过程是一个随机过程，因此应当用期望的方式评估奖励函数$$\pi^*=\arg\max \mathbb E[\sum_{t\geq 0}\gamma^tr_t|\pi]$$
	* 强化学习在所有可能的轨迹上进行取样，计算，得到一组奖励函数，从而在全局上评估出最优的策略
* 价值函数：刻画动作的价值大小从而决定策略的偏好程度
	* 状态价值函数：定义在状态$s$和策略$\pi$上的奖励值$$V^\pi (s)=\mathbb E_{s_1,a_1,\cdots,s_t,a_t,\cdots}[\sum_{t\geq 0}\gamma^tr_t|s_0=s,\pi]$$
	* 状态-动作价值函数：定义在状态$s$，动作$a$和策略$\pi$上的奖励值$$Q^\pi(s,a)=\mathbb E_{s_1:T,a_1:T}[\sum_{t\geq 0}\gamma^tr_t|s_0=s,a_0=a,\pi]$$
	* 最优动作：使得$Q$函数达到最大的动作对应的奖励值$$Q^*(s,a)=\max_\pi Q^\pi(s,a)=\arg\max_a Q^*(s,a)$$
		* 最优动作的集合即最优策略
	* 贝尔曼方程：转移函数有动态转移方程$$Q^\pi(s,a)=\mathbb E_{s',a',}[r+\gamma Q^\pi(s',a')|s,a]$$满足最优子结构的性质。同样，最优动作也有最优子结构的性质。$$Q^*(s,a)=\mathbb E_{s'}[r+\gamma\max_{a'} Q^*(s',a')|s,a]$$
# 基于价值的学习
- Q-学习：
	- 构造带参数的学习器$Q(s,a,\pmb w)\approx Q^*(s,a)$，其中$\pmb w$为参数
		- 学习器会以输入的$s,a$或$s$并枚举$a$的方式，学习得到好的参数$\pmb w$对应一个好的策略方法
	- 蒙特卡洛估计：
		- 数理基础：利用多次随机采样的平均值进行估计
		- 步骤：多次对轨迹采样，调用对应的$Q$函数，在每次采样后，更新轨迹采样次数以及奖励值从而计算平均
		- 本质：用估计值得到输出函数，将其转化为监督学习
		- 缺点：依赖大量的采样过程，**样本复杂**度较高，试错次数太慢
	- **时间差分学习**：
		- 数理基础：贝尔曼方程提供了动态估计$Q$函数的方式
		- 步骤：
			- 采用线性回归的最小二乘方式，得到一个损失函数$$l=(r+\gamma \max_{a'}Q^*(s',a',w)-Q(s,a,w))^2$$其中，用$r+\gamma \max_{a'}Q^*(s',a',w)$这一项代表Q函数的估计值
			- 写成递推式如下：$$Q_{t+1}(s_t,a_t)=Q_t(s_t,a_t)+\alpha(r_{t+1}+\gamma \max_{a}Q_t(s_{t+1},a)-Q_t(s_t,a_t))$$其中，$Q_t(s_t,a_t)$指代前一状态，参数$\alpha$表示学习率
			- 数学证明可以得到，当学习率是递减且时间$t\rightarrow \infty$时，递推式以概率1收敛到最优的$Q^*$函数
			- 例：![[Pasted image 20250407174414.png]]
	- 探索-利用窘境：在试图探索更多的收益与利用已经探索到的信息之间存在平衡
- Q-学习神经网络：
	- 问题：上述的数学证明只对线性的模型成立，对非线性的神经网络模型，其不一定成立
	- 自举式学习：在线的进行学习，每次通过递推式得到$Q^*$函数的估计值，之后用这个估计值即使的学习出参数$\pmb w$，反复迭代
	- 经验重放：当学习的动作轨迹过长时，学习往往会忘记以前的内容。因此，维护一个学习过的经验动作集，每次在取max时，改为从所有的动作集中选取
# 基于策略的学习
- 基于价值的学习的问题：
	- 策略函数往往比Q函数更加简单
	- 对连续的动作空间和状态空间，Q函数往往难以定义，而策略函数好定义
	- Q函数对动作a没有刻画其随机性
- 策略学习：
	- 相对于完全监督学习，策略学习每次的输出动作$a_t$并非完全最优的输出数据$y_i$，而只是基于当前不完美的策略所执行的输出
	- 引入$J$函数：$J(\tau;\theta)$，引入参数$\theta$用于刻画策略的参数，该函数用于刻画损失（奖励）函数的情况，这一学习实质是通过使得如下函数达到最大值来进行学习$$\sum_tJ(\tau;\theta)\log p(a_t|s_t)$$
	- 可以使用梯度上升的方式，得到参数$\theta$的取值
	- 实现方式：![[Pasted image 20250407175527.png]]



# 强化学习的应用——AlphaGo




