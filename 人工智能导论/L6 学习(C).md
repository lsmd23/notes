# 强化学习
- 基本框架：环境，状态转移与动作，奖励收益。学习的目标是最大化奖励收益的值。
## 马尔可夫决策过程(MDP)
* 定义： 五元组$<S,P,A,R,\gamma>$
	* 状态集合$S$：刻画所有的状态
	* 动作集合$A$：刻画所有的动作
	* 奖励因子$R$：对一个确定的状态和动作$(s,a)$，所能获得的奖励因子
	* 转移概率矩阵$P$：对确定的转移动作和状态，刻画转移到不同状态的概率的矩阵$P_{SAS'}=\mathbb P[S_{t+1}=s'|S_{t}=s,A_t=a]$
	* 折扣因子$\gamma$：用于对更靠后的奖励值的修正，值越小，表示越不重视靠后的奖励值
	* 行为：
		* $t=0$时，对应一个初始状态$s_0$
		* 直到终止时刻，进行如下的迭代：
			1. 选择一个动作$a_t$
			2. 根据奖励函数，返回奖励值$r_t$
			3. 根据转移概率矩阵，返回次态$s_{t+1}$
			4. 进行状态变换并收到奖励值
- 强化学习的策略：
	* 策略$\pmb \pi$：$S\rightarrow A$，刻画选择动作的偏好，强化学习的目标是使得某个策略$\pmb \pi^*$的累积奖励函数$\sum_{t\geq 0}\gamma^tr_t$取得最大值
	* 由于概率矩阵的存在，整个过程是一个随机过程，因此应当用期望的方式评估奖励函数$$\pi^*=\arg\max \mathbb E[\sum_{t\geq 0}\gamma^tr_t|\pi]$$
	* 强化学习在所有可能的轨迹上进行取样，计算，得到一组奖励函数，从而在全局上评估出最优的策略
* 价值函数：刻画动作的价值大小从而决定策略的偏好程度
	* 状态价值函数：定义在状态$s$和策略$\pi$上的奖励值$$V^\pi (s)=\mathbb E_{s_1,a_1,\cdots,s_t,a_t,\cdots}[\sum_{t\geq 0}\gamma^tr_t|s_0=s,\pi]$$
	* 状态-动作价值函数：定义在状态$s$，动作$a$和策略$\pi$上的奖励值$$Q^\pi(s,a)=\mathbb E_{s_1:T,a_1:T}[\sum_{t\geq 0}\gamma^tr_t|s_0=s,a_0=a,\pi]$$
	* 最优动作：使得$Q$函数达到最大的动作对应的奖励值$$Q^*(s,a)=\max_\pi Q^\pi(s,a)=\arg\max_a Q^*(s,a)$$
		* 最优动作的集合即最优策略
	* 贝尔曼方程：价值函数有动态转移方程$$Q^\pi(s,a)=\mathbb E_{s',a'}[r+\gamma Q^\pi(s',a')|s,a]$$满足最优子结构的性质。同样，最优动作也有最优子结构的性质。$$Q^*(s,a)=\mathbb E_{s'}[r+\gamma\max_{a'} Q^*(s',a')|s,a]$$
# 基于价值的学习
- Q-学习：
	- 构造带参数的学习器$Q(s,a,\pmb w)\approx Q^*(s,a)$，其中$\pmb w$为参数
		- 学习器会以输入的$s,a$或$s$并枚举$a$的方式，学习得到好的参数$\pmb w$对应一个好的策略方法
	- 蒙特卡洛估计：
		- 数理基础：利用多次随机采样的平均值进行估计
		- 步骤：多次对轨迹采样，调用对应的$Q$函数，在每次采样后，更新轨迹采样次数以及奖励值从而计算平均
		- 本质：用估计值得到输出函数，将其转化为监督学习
		- 缺点：依赖大量的采样过程，**样本复杂**度较高，试错次数太慢
	- **时间差分学习**：
		- 数理基础：贝尔曼方程提供了动态估计$Q$函数的方式
		- 步骤：
			- 采用线性回归的最小二乘方式，得到一个损失函数$$l=(r+\gamma \max_{a'}Q^*(s',a',w)-Q(s,a,w))^2$$其中，用$r+\gamma \max_{a'}Q^*(s',a',w)$这一项代表Q函数的估计值
			- 写成递推式如下：$$Q_{t+1}(s_t,a_t)=Q_t(s_t,a_t)+\alpha(r_{t+1}+\gamma \max_{a}Q_t(s_{t+1},a)-Q_t(s_t,a_t))$$其中，$Q_t(s_t,a_t)$指代前一状态，参数$\alpha$表示学习率
			- 数学证明可以得到，当学习率是递减且时间$t\rightarrow \infty$时，递推式以概率1收敛到最优的$Q^*$函数
			- 例：![[Pasted image 20250407174414.png]]
	- 探索-利用窘境：在试图探索更多的收益与利用已经探索到的信息之间存在平衡
		- 解决：引入概率参数，使得其有向价值函数并非最大的方向探索的概率
- Q-学习神经网络：
	- 问题：上述的数学证明只对线性的模型成立，对非线性的神经网络模型，其不一定成立
	- 自举式学习：在线的进行学习，每次通过递推式得到$Q^*$函数的估计值，之后用这个估计值即使的学习出参数$\pmb w$，反复迭代
		- 问题：在相邻的两次状态之间，价值函数可能会差异很大，导致学习的结果可能会在某个节点上有剧烈的变化，学习结果不稳定
			- 解决：记录一些时间上更早得到的参数结果，用这些结果取代简单的代入上一个状态的学习结果，一定程度规避学习不稳定的现象
		- 问题：样本可能本身会相关（也即相邻的状态之间是严重依赖的），样本的非独立性会对学习的效率造成影响
			- 解决：经验重放——当学习的动作轨迹过长时，学习往往会忘记以前的内容。因此，维护一个学习过的经验动作集（且该集合无序），每次在取max时，改为从所有的动作集中选取（降低了样本之间的相关性）
			- 经验重放涉及一部分的超参数，影响学习的效率
	- 对神经网络的改进：
		- 双重Q-学习：
			- 学习过程中，最优的价值函数$Q^*$是未知的，因此使用当前的$Q$函数进行替代
			- 一般而言，这样的估计会**高估**最优的价值函数的具体值（因为每次都是取最大的值进行迭代）
			- 解决：引入目标$Q$函数网络及其参数$w'$，该参数选取自迭代时间戳上更老的数据，进而**控制其迭代的幅度**
			- 选取的动作来自与目标$Q$函数网络，借此取平衡
			- 公式：$$L=(r+\gamma Q(s',{\arg\max}_{a'}Q(s',a',w),w')-Q(s,a,w))^2$$
		- 优先级经验回放：
			- 对经验回访的数组，以时间差分学习的**损失函数**作为其优先级成对存放
			- 采样时，优先采样优先级大的数据，从而获得最应该学习的样本去降低误差
		- 竞争网络：对Q网络可以进行分解，将其分解为平均值项和动作方差项$$Q(s,a)=V(s,v)+A(s,a,w)$$分别用两个网络逼近这两个函数，从而降低拟合的方差（分离了参数，分别估计）
# 基于策略的学习
- 基于价值的学习的问题：
	- 策略函数往往比Q函数更加简单
	- 对连续的动作空间和状态空间，Q函数往往难以定义，而策略函数好定义
	- Q函数对动作a没有刻画其随机性
- 策略学习：
	- 相对于完全监督学习，策略学习每次的输出动作$a_t$并非完全最优的输出数据$y_i$，而只是基于当前不完美的策略所执行的输出
	- 引入$J$函数：$J(\tau;\theta)$，引入参数$\theta$用于刻画策略的参数，该函数用于刻画损失（奖励）函数的情况，这一学习实质是通过使得如下函数达到最大值来进行学习$$\sum_tJ(\tau;\theta)\log p(a_t|s_t)$$
	- 可以使用梯度上升的方式，得到参数$\theta$的取值，称为**策略梯度学习**
		- 数学推导可以得知，策略梯度的值和结果的对数似然估计有关，在对数似然估计前以$J$函数加权
	- 实现方式：![[Pasted image 20250407175527.png]]
- 综合的解决——演员-评委模型：
	- 问题：
		- 对单纯的策略梯度学习，由于$J$函数的设计不具有通用性（大多数情况，简单设计得到的$J$函数很难刻画整条决策轨迹中每个决策的价值大小），故其方差很高
		- 简单的$Q$学习也有自己的问题
	- 解决：使用$Q$函数替代策略梯度中的$J$函数，进而计算得到策略梯度：$$Q(s_t,a_t,w)\nabla_\theta\log(a_t|s_t;\theta) $$
		- $Q$函数的部分称为“评委”模型
		- 对数似然估计的部分称为“演员”模型
	- 实现：![[Pasted image 20250414153416.png]]


# 强化学习的应用——AlphaGo
- 问题：MCTS搜索依然难以应对围棋极大的规模
- 网络的训练：
	- 策略网络：
		- 监督学习：从已有的棋谱中学习其每一步的概率，得到一个初步的策略网络![[Pasted image 20250414154639.png]]
		- 强化学习：用确定结果的自我对弈进行基于策略的强化学习![[Pasted image 20250414154659.png]]
	- 价值网络：
- AlphaGo的实现：
- 
---
[[L7 学习(D)]]


