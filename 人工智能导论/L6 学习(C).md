# 强化学习
- 基本框架：环境，状态转移与动作，奖励收益。学习的目标是最大化奖励收益的值。
## 马尔可夫决策过程(MDP)
* 定义： 五元组$<S,P,A,R,\gamma>$
	* 状态集合$S$：刻画所有的状态
	* 动作集合$A$：刻画所有的动作
	* 奖励因子$R$：对一个确定的状态和动作$(s,a)$，所能获得的奖励因子
	* 转移概率矩阵$P$：对确定的转移动作和状态，刻画转移到不同状态的概率的矩阵
	* 折扣因子$\gamma$：
	* 行为：
		* $t=0$时，对应一个初始状态$s_0$
		* 直到终止时刻，进行如下的迭代：
			1. 选择一个动作$a_t$
- 强化学习的策略：
	* 策略$\pmb \pi$：$S\rightarrow A$，刻画选择动作的偏好，强化学习的目标是使得某个策略$\pmb \pi^*$的累积奖励函数$\sum_{t\geq 0}\gamma^tr_t$取得最大值
	* 由于概率矩阵的存在，整个过程是一个随机过程，因此应当用期望的方式评估奖励函数$$\pi^*=\arg\max \mathbb E[\sum_{t\geq 0}\gamma^tr_t|\pi]$$
	* 强化学习在所有可能的轨迹上进行取样，计算，得到一组奖励函数，从而在全局上评估出最优的策略
* 价值函数：刻画动作的价值大小从而决定策略的偏好程度
	* 状态价值函数：定义在状态$s$和策略$\pi$上的奖励值$$V^\pi (s)=\mathbb E_{s_1,a_1,\cdots,s_t,a_t,\cdots}[\sum_{t\geq 0}\gamma^tr_t|s_0=s,\pi]$$
	* 状态-动作价值函数：定义在状态$s$，动作$a$和策略$\pi$上的奖励值$$Q^\pi(s,a)=\mathbb E_{s_1:T,a_1:T}[\sum_{t\geq 0}\gamma^tr_t|s_0=s,a_0=a,\pi]$$
	* 最优动作：使得$Q$函数达到最大的动作对应的奖励值$$Q^*(s,a)=\max_\pi Q^\pi(s,a)$$
		* 最优动作的集合即最优策略
	* 贝尔曼方程：转移函数有动态转移方程$$Q^\pi(s,a)=\mathbb E_{s',a',}[r+\gamma Q^\pi(s',a')|s,a]$$满足最优子结构的性质。同样，最优动作也有最优子结构的性质。$$Q^*(s,a)=\mathbb E_{s'}[r+\gamma\max_{a'} Q^*(s',a')|s,a]$$
# 基于价值的学习
- 模型：
	- 构造带参数的学习器$Q(s,a,\pmb w)\=Q^*(s,a)$
- 蒙特卡洛估计：
	- 




# 基于策略的学习




# 强化学习的应用——AlphaGo




