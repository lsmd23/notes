# 深度学习
- 问题：前述所有模型中，输入到输出都是简单的映射，而和现实中复杂的信息加工有很大区别
- 深度学习解决的是实际问题中的特征提取问题，试图用学习的方式去模拟人类对某些复杂数据的特征提取过程（如图像处理、语音识别等等）
# 多层感知机
- 感知机：
	- 模型：模拟神经元的功能![[Pasted image 20250414163223.png]]数学模型：$\hat{y}=g(\theta_0+\sum_{i=1}^dx_i\theta_i)$
	- 简单应用：表达基本布尔运算![[Pasted image 20250414163724.png]]
- 多层感知机：将感知机前后相连，即可得到多层感知机
	- 拥有中间的隐藏层，进而可以表达更加复杂的表达式（例如布尔函数）
	- 模型：![[Pasted image 20250415204631.png]]最后一层表示一个损失函数
- 激活函数：要求必须是非线性的以实现多层感知机的拟合效果
	- 中间层的激活函数：要尽可能减少梯度消失和数值爆炸现象，取中间值可以得到比较好的一些函数![[Pasted image 20250415204824.png]]实际常用ReLU函数
	- 最后一层的激活函数：Softmax函数及其修正
		- Softmax函数：$$g(\pmb z)_i=\frac{e^{z_i}}{\sum_{j=1}^ke^{z_j}}$$用于评估神经网络的结果的情况
			- 缺点：由于指数存在，可能会极大的放大概率，使得结果的随机性降低
			- 解决：上下同时除一个参数，将指数项降低到负值$$g(\pmb z)_i=\frac{e^{z_i-z_m}}{\sum_{j=1}^ke^{z_j-z_m}}$$
	- 损失函数：交叉熵损失函数以及其它的损失函数都常见（熵损失函数、相关熵损失函数）
	- 价值函数：综合两者（采用Softmax函数和交叉熵损失函数），可以得到如下形式的以$\theta$为参数的价值函数$$\min J(\theta)=-\frac{1}{m}\sum_{i=1}^m[\sum_{j=1}^k\pmb 1\{y^{(i)}=j\}\log\frac{\exp(z_j^{(n_l)})}{\sum_{j'=1}^k\exp(z_{j'}^{(n_l)})}]$$
## 反向传播


## 训练策略
- 问题：数学证明发现，对深度学习的损失函数，其局部极值的个数是指数量级的。因此，希望能有更多的手段，逃离局部极值，得到泛化性更好的局部极值点
- 方案：
	- 随机梯度下降(SGD)：随机选取一部分的维度计算梯度
		- 数学公式：$$$$
		- 优势：计算快，且**逃离鞍点**
	- 动量随机梯度下降：借鉴物理中的冲量与动量概念，引入一个冲量值
		- 数学公式：$$\Delta = \beta\Delta+(1-\beta)\frac{\partial}{\partial }$$
		- 优势：可以逃离比较浅的局部极值
		- 实际应用：ADM优化
	- 学习率（衰减）策略：
		- 学习率（也即梯度更新的步长）会影响其跳出局部极值的高度大小，也会影响训练的稳定程度
		- 常用的学习率衰减：
			- 随迭代步数指数衰减$\eta=\eta_0 e^{-kt}$
			- 简单阶梯式衰减：
	- 权重衰减策略：正则化损失函数
		- 加入正则项以控制损失函数的参数
		- 正则函数可以控制参数的范围，从而不免数值溢出带来的训练不稳定
	- 参数丢弃：
		- 由于神经网络的参数（感知机）数量过多，可以在训练时在层内丢弃掉一些神经元
		- 优势：
			- 通过多次丢弃训练，可以保证每个神经元的工作效率都较高
			- 类似于随机森林的思想
	- 权重初始化：
		- 
- 总结：
	- 实际过程中，数据集
	- 学习率、学习的程度与结果的相关性：![[Pasted image 20250421154724.png]]
# 卷积神经网络(CNN)
- 问题：解决处理图像等有内在规则的高维向量与矩阵的输入数据的问题
	- 人类的视角：人类在观察时，每个视神经细胞都只复杂观察的一部分，而不是负责全部的部分。全部信息的处理交给更高级的神经系统中进行
	- 机器的视角：图像也有一些关键性的特征（例如边缘特征），且特征之间也是层次化由低到高的，类似上述的局部注意机制
		- 局部性假设：每个神经元可以只负责图像的一小部分，再通过神经网络结合到一起
		- 保持空间先验：这样的机制保持了原图像中物体的未知
		- 参数共享：提取同一个特征的神经元，应当使用同一个共享的参数。对不同的特征，可以增加同层级的神经元的数目（增加通道数目）
- 卷积神经网络(CNN)：
	- 卷积运算：
		- 内积运算：刻画两个函数的相似程度
		- 对内积运算引入两函数的差距的参数$\tau$，使其成为参数$\tau$的函数，即为卷积函数$$(f\ast g)(t)=\int_{-\infty}^\infty f(\tau)g(t-\tau)\mathrm d\tau$$如图所示：![[Pasted image 20250421160423.png]]
		- 交叉相关函数：处理复变函数的卷积运算$$(f\star g)(n)=$$
		- 和图像处理中的局部神经元参数共享扫描机制是对应的
	- 实现：
		- 输入：多通道的图像，形式化为高维矩阵或张量
		- 卷积核：通道数相同但大小更小的卷积核，按空间关系，对矩阵的局部作内积运算，处理得到下一层的数据
			- 形式化：
			- 实现时，用多神经元共享参数的方式，实现并行高效的卷积计算
		- 多层卷积：用不同的$d$个卷积核，得到新的通道数为$d$的处理后图像（**特征图**）
			- 例：![[Pasted image 20250421162032.png]]

