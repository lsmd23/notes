# 深度学习
- 问题：前述所有模型中，输入到输出都是简单的映射，而和现实中复杂的信息加工有很大区别
- 深度学习解决的是实际问题中的特征提取问题，试图用学习的方式去模拟人类对某些复杂数据的特征提取过程（如图像处理、语音识别等等）
# 多层感知机
- 感知机：
	- 模型：模拟神经元的功能![[Pasted image 20250414163223.png]]数学模型：$\hat{y}=g(\theta_0+\sum_{i=1}^dx_i\theta_i)$
	- 简单应用：表达基本布尔运算![[Pasted image 20250414163724.png]]
- 多层感知机：将感知机前后相连，即可得到多层感知机
	- 拥有中间的隐藏层，进而可以表达更加复杂的表达式（例如布尔函数）
	- 模型：![[Pasted image 20250415204631.png]]最后一层表示一个损失函数
- 激活函数：要求必须是非线性的以实现多层感知机的拟合效果
	- 中间层的激活函数：要尽可能减少梯度消失和数值爆炸现象，取中间值可以得到比较好的一些函数![[Pasted image 20250415204824.png]]实际常用ReLU函数
	- 最后一层的激活函数：Softmax函数及其修正
		- Softmax函数：$$g(\pmb z)_i=\frac{e^{z_i}}{\sum_{j=1}^ke^{z_j}}$$用于评估神经网络的结果的情况
			- 缺点：由于指数存在，可能会极大的放大概率，使得结果的随机性降低
			- 解决：上下同时除一个参数，将指数项降低到负值$$g(\pmb z)_i=\frac{e^{z_i-z_m}}{\sum_{j=1}^ke^{z_j-z_m}}$$
	- 损失函数：交叉熵损失函数以及其它的损失函数都常见（熵损失函数、相关熵损失函数）
	- 价值函数：综合两者（采用Softmax函数和交叉熵损失函数），可以得到如下形式的以$\theta$为参数的价值函数$$\min J(\theta)=-\frac{1}{m}\sum_{i=1}^m[\sum_{j=1}^k\pmb 1\{y^{(i)}=j\}\log\frac{\exp(z_j^{(n_l)})}{\sum_{j'=1}^k\exp(z_{j'}^{(n_l)})}]$$
## 反向传播




#

