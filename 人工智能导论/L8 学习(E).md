# Transformer
- 语言模型：
	- 抽象成数学模型，即预测一离散的条件概率分布：$P(\text{word}_i|\text{word}_1,\cdots,\text{word}_{i-1})$。也即已出现词条件下某词的概率分布情况
	- Transformer架构最早被用来处理语言模型
- 注意力机制：
	- 词向量与上下文：对每一个词，建模成一个向量。不同词向量之间可以计算“相似度”，表示词之间的各种关联（反义、主谓、关联……），由此体现词在上下文中的关系
	- 注意力机制：对一句话中的词进行识别时，会选择性的将有限的资源有区别的分配
- Transformer模块：
	- 可伸缩内积注意力：计算“相似度”的方式
		- 简单的求内积：$a(q,k)=q^Tk$
		- 数学研究发现，内积运算使得结果的方差变小，因此用标准差进行修正$a(q,k)=\frac{q^Tk}{\sqrt{d_k}}$（$d_k$表示向量的维度），使得结果的数值更加稳定
		- 工作方式：
			- 先抽象词向量，将每个词抽象成向量（利用词嵌入技术）
			- 对每个词向量，利用注意力机制，用变换矩阵$W^q,W^k,W^v$将每个向量映射成其分别作为Query、Key、Value时的向量
			- 之后，对每个$q_i$，都和所有的$k_j$作相似内积计算，之后以$v_j$为权进行加和，计算得到新的特征$y_i$
				- 这一返回值特征考虑了上下文的特征，上下文中的关键语素会修正单个词的含义
			- 数学公式：$$\text{Attention}(Q,K,V)=\text{Softmax}(\frac{QK^T}{\sqrt{d_k}})V$$展开后：$$y_i=\sum_j\alpha_{ij}v_j,\alpha_{ij}\frac{\exp(q_ik_j)}{\sum_j\exp(q_ik_j)}$$
			- 这一计算是$O(n^2)$复杂度的，效率相对较低，但充分考虑了语言的上下文性质
	- 多头注意力机制：
		- 上述的可伸缩内积注意力机制，研究发现其提取的特征会过少
		- 优化方式：将一个词向量切割成多个向量，对应于**词向量空间切割为数个子空间**，每个子空间分别计算注意力，对应不同情况下词与词之间可能的关系
		- 工作方式：
			- 在不同的子空间（$k$子空间）上，分别计算注意力值$y_{ik}$，之后将不同的$y_{ik}$综合起来得到总的注意力
			- 在结合时，简单的连接效果不好，还需要对连接后的向量进行线性映射，也即乘以一个矩阵$W^O$
			- 数学表示：$$\begin{align}&\text{head}_i=\text{Attention}(QW_i^Q,KW_i^K,VW_i^V) \\&  \text{Multihead}(Q,K,V)=\text{Concat}(\text{head}_1,\cdots,\text{head}_h )W^O \\ \end{align}$$
			- 超参数：头的个数$h$，也即切割的份数
	- 基于位置的前馈网络(FFN)：
		- 将直接得到的$y_i$进行输入，特征依然不够明显，因为整个过程仅仅经过了线性的映射
		- 使用两层的感知机，以ReLU为激活函数，对其进行非线性映射：$$\text{FFN}(x)=\max(0,xW_1+b_1)W_2+b_2$$
		- 特点：每个词在进行映射后，保留了其独立的提取出的和上下文之间的关系特征，简单的进行粘连反而会失去这些特征，经过映射后，保证特征不会在粘连后丢失
	- 残差连接：
	- 层归一化：
	- 位置编码：
		- 置换不变性：前述的建模过程，对词向量的位置是没有要求的，与实际的语言是不相符的
		- 对每个位置，引入位置向量$e_i$，与原有的词向量进行Concat操作，从而在词向量中反映其位置
		- 位置编码：分别为奇数、偶数维分量编码
		- 位置编码的设计：
			- 不同位置编码唯一
			- 编码应当有界从而可以处理大的长度的编码
			- 任意两个位置之间的编码的结果在整个序列中具有**滑动不变性**（也即相差k个的两个位置编码应当可以平移）
- Transformer架构：
	- 编码器：将输入编码成特征
	- 解码器：根据已知以及特征，预测下一个词的概率分布
		- 自回归：每次生成时，是将上一次的输出采样结果作为下一次估计的输入，进行下一次的采样
		- KV缓存：
		- 损失函数：
	- 掩码多头注意力机制：
		- 每次注意力计算时，只考虑前面的部分，而不考虑后面的部分
		- 对应于数学模型，使用掩码将注意力矩阵转化为上三角的矩阵，也即对每个词只有其前面的词是有注意力的
		- 解决并行训练的问题：