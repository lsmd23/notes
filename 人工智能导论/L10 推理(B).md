# 概率学基础
- 贝叶斯估计：利用观测得到的数据，采用贝叶斯公式修正变量的先验分布进而得到后验分布
	- 贝叶斯公式：$$p(\theta|D)=\frac{p(D|\theta)p(\theta)}{p(D)}$$
	- 贝叶斯定律：$$p(\theta|D)∝ {p(D|\theta)p(\theta)}$$
	- 贝叶斯决策准则：也即取后验众数作为点估计的结果
- 概率分布：
	- 伯努利分布：也即概率为$q$的两点分布
	- 多伯努利分布：多点分布，对应一个参数集合$\{\pmb \Phi\}$，$P(y=l|\pmb \phi)=\phi_l$
	- 高斯分布：也即正态分布，可以拓展到多维的高斯分布，对应参数有均值和协方差矩阵$\{\pmb{\mu,\Sigma}\}$，其PDF为：$$p(\pmb x|\pmb \mu,\pmb \Sigma)=\frac{1}{\sqrt{|2\pi\pmb \Sigma|}}\exp(-\frac{1}{2}(\pmb x-\pmb \mu)^T\pmb \Sigma^{-1}(\pmb x-\pmb \mu))$$
- 最大似然估计：
	- 似然函数与对数似然函数：$$p(D;\theta)=\prod_{n=1}^Np(z_N;\theta),\log p(D;\theta)=\sum_{n=1}^N\log p(z_N;\theta)$$
	- 最大似然估计也即使得似然函数（或对数似然函数）取得最大值的参数为参数的估计量$$\hat\theta=\arg\max\sum_{n=1}^N\log p(z_N;\theta)$$对应于损失函数，这一估计也即所谓的学习过程
	- 对高斯分布的估计：样本均值为总体均值的最大似然估计，样本二阶矩为总体方差的最大似然估计
# 判别式模型
- 思想：建模贝叶斯估计中的条件概率$p(Y|X)$，使用含参数的模型逼近该条件概率：$q(Y|X;\theta)$
- 线性回归：
	- 做建模假设：所有样本的噪声都服从同方差的高斯分布：$y\sim N(\pmb w^T\pmb x,\sigma^2)$
	- 使用线性模型建模一个条件期望：$E(y|\pmb x,\pmb w,\sigma^2)=\pmb w^T\pmb x$
	- 求最大似然估计：单样本似然函数：$$p(y_n|\pmb w, \pmb x_n) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left\{-\frac{1}{2\sigma^2}(y_n - \pmb w^T\pmb x_n)^2\right\}$$似然函数：$$log p(\mathcal{D}_n; \pmb w)  = \sum_{n=1}^{N} \log p(y_n|\pmb w, \pmb x_n)=\frac{N}{2} \log \frac{1}{2\pi\sigma^2} - \frac{1}{2\sigma^2} \sum_{n=1}^{N} \left(y_n - \pmb{w}^T \pmb{x}_n\right)^2$$
		- 存在假设：经验分布假设，也即$x$的边际分布均为$1/N$，从而分离变量
		- 结论：最大似然估计使得均方误差最小，即线性模型损失函数的来源
	- 最大后验估计：采用贝叶斯的点估计准则
		- 先验分布：采用相互不相关的同方差假设：$$p(\pmb w)=N(\pmb w|0,\alpha^{-1}\pmb I)$$
		- 推导：可以得到，$\hat{w} = \arg \min_{w} \|\pmb{X w} - y\|_2^2 + \frac{\alpha}{\beta} \|\pmb w\|_2^2$
		- 结论：在原来的参数估计的均方误差后，增加了样本的二范数正则化条件
		- 贝叶斯模型平均：可以采用不同的概率模型$p(\tilde y|\theta)$，根据数据上权重的大小$p(\theta|y)$进行综合多个模型的估计，也即公式：$$p(\tilde{y}|y) = \sum_{h=1}^{H} p(\tilde{y}|M_h, y) p(M_h|y)$$
# 生成式模型
- 问题：判别式模型只建模参数，不建模数据，因此假设数据集中有一个完全错误的样本（对抗样本），其同样会给出确切的结果，和实际不符
## 朴素贝叶斯分类器
- 贝叶斯准则：$$p(X=\pmb x|Y=y)=\frac{p(Y=y|X=\pmb x)p(Y=y)}{p(X=\pmb x)}$$建模得到$p(X=\pmb x|Y=y)$，即可用于已知模型参数时，生成样本数据
	- 维数灾难：样本数据往往是一个高维变量，建模一个高维的PMF往往是困难的
- 朴素贝叶斯分类器：
	- 条件独立假设：假设在给定参数$y$时，样本参数的各维度之间是条件独立的，则概率模型大大的被简化，即为朴素贝叶斯分类器$$p(X=\pmb x|Y=y)=\prod_{j=1}^dp(x_{\cdot j}|y)$$
	- 例：伯努利分布的朴素贝叶斯分类器：建模参数$y$的取值对应的概率$\phi$，以及对每个样本维度，其在给定参数$y$的取值后($\pm 1$)对应的伯努利概率参数$\phi_j^+,\phi_j^-$
		- 计算得到：$$\phi_j^+ = \frac{\sum_{i=1}^n \pmb{1}\{x_{ij} = 1 \land y_i = +1\}}{\sum_{i=1}^n \pmb{1}\{y_i = +1\}}$$$$\phi = \frac{\sum_{i=1}^n \pmb{1}\{y_i = +1\}}{n}$$
		- 例：![[Pasted image 20250512175344.png]]
		- 拉普拉斯平滑条件：若某一概率值$p(x_{\cdot j}=r_j|Y=+1)$计算为0，根据条件独立假设，会导致全概率函数$p(\pmb x=r_j|Y=+1)$计算均为0，因此加入一个均匀的修正项：$$p(x_{\cdot j} = r_j \mid Y = +1) = \frac{\sum_{i=1}^{n} \pmb{1}(x_{\cdot j} = r_j \wedge y_i = +1)+1}{\sum_{i=1}^{n} \pmb{1}(y_i = +1)+ k_j} $$

## 高斯判别分析
- 建模连续模型变量的分布
- 聚类假设：对某个分类，数据集应当服从同类的样本聚集，不同类样本分离的假设，在同一类样本中假设为高斯分布$$p(X = \pmb x|Y = +1) \propto \exp\left(-\frac{1}{2}( \pmb x - \pmb \mu_+)^T \pmb \Sigma^{-1}(\pmb x - \pmb \mu_+)\right)$$$$p(X = \pmb x|Y = -1) \propto \exp\left(-\frac{1}{2}(\pmb x - \pmb \mu_-)^T \pmb \Sigma^{-1}(\pmb x - \pmb \mu_-)\right).$$注：共享协方差假设：建模各个数据的维度遵循相同的协方差矩阵，防止过拟合
- 似然函数：$$\begin{align}l(\phi, \pmb \mu_{+}, \pmb \mu_{-}, \pmb \Sigma) &= \log \prod_{i=1}^{n} p(\pmb x_{i}, y_{i}; \phi,\pmb  \mu_{+}, \pmb \mu_{-}, \pmb \Sigma)\\ &= \log \prod_{i=1}^{n} p(\pmb x_{i}|y_{i}; \pmb \mu_{+}, \pmb \mu_{-}, \pmb \Sigma) + \log \prod_{i=1}^{n} p(y_{i}|\phi)\\\end{align}$$得到其估计：$$\begin{align}\phi = \frac{\sum_{i=1}^n \pmb {1}\{y_i = +1\}}{n}, \quad
&\pmb \mu_+ = \frac{\sum_{i=1}^n \pmb {1}\{y_i = +1\} \pmb x_i}{\sum_{i=1}^n \pmb {1}\{y_i = +1\}}, \quad
\pmb \mu_- = \frac{\sum_{i=1}^n \pmb {1}\{y_i = -1\}\pmb  x_i}{\sum_{i=1}^n \pmb {1}\{y_i = -1\}}\\ &\pmb \Sigma = \frac{1}{n} \sum_{i=1}^n (\pmb x_i - \pmb \mu_{y_i})(\pmb x_i - \pmb \mu_{y_i})^T\\\end{align}$$
# 混合模型与EM算法
## 高斯混合模型
- 问题：
	- 现实问题往往有多个分布族，单一的高斯分布难以建模，因此采用多个高斯分布建模的方式
	- 在生成式模型中，预先的标签值未知，无法预测判断其概率分布
- 混合模型：
	- 混合高斯模型：使用对不同的模型分布加权平均的方式建模一个未知标签的变量的分布
		- 例：$$p(\pmb x)=\frac{1}{2}p(\pmb x|\pmb \mu_{+1},\pmb\Sigma)+\frac{1}{2}p(\pmb x|\pmb \mu_{-1},\pmb\Sigma)$$
	- 参数：
		- 不同簇对应的取值概率$\pi=(\pi_1,\pi_2,\cdots,\pi_k)$
		- 不同的簇对应的高斯分布参数：$(\pmb \mu_i,\pmb\Sigma_i)$
		- 则建模出的概率为：$$p(\pmb x)=\sum_{z=1}^k\pi_z \mathscr N(\pmb x|\pmb \mu_Z,\pmb\Sigma_Z)$$
		- 更一般的，不使用高斯分布，则为
		  ：$$p(\pmb x)=\sum_{i=1}^kw_i p_i(x)$$也即随机变量可以用如下的方式产生：先按权重取样得到其分布簇，在从分布簇中按固定的分布生成一个值
- 学习与混合高斯分布：
	- 对无监督学习，未知其标签，可以用混合高斯分布建模，认为每个数据都是通过上述的方式采样得到的
	- 将参数$\pi$加入到似然函数中，带入高斯分布，可以得到似然函数的数学表达式：$$\ell(\pmb \pi, \pmb \mu,\pmb  \Sigma) = \sum_{i=1}^n \log \left[ \sum_{z=1}^k \frac{\pi_z}{\sqrt{|2\pi\pmb \Sigma_z|}} \exp\left(-\frac{1}{2}(\pmb x_i - \pmb \mu_z)^T \pmb \Sigma_z^{-1}(\pmb x_i - \pmb \mu_z)\right) \right]$$可以看到表达式中出现对数内项求和，该式的精确求解是np难的，因此采用近似的求解算法计算
	- 无监督学习是在这三个（两组）参数间作迭代计算，也即：
		- 对已知的样本参数$(\pmb x,z)$，求解最大似然估计$(\pmb \pi, \pmb \mu,\pmb  \Sigma)$
		- 再回头带入这组参数$(\pmb \pi, \pmb \mu,\pmb  \Sigma)$，对每个样本的自变量$\pmb x$，计算对应的$z$
## 期望最大化算法
- 隐变量模型： ^fe0a10
	- 对变量，存在隐变量集合$z$（不可观测变量）与可观测变量集合$x$（可观测变量）
	- 建模含参概率模型，同时表示隐变量和可观测变量的概率$p(x,z|\theta)$
	- 只观察到$x$的数据集为不完备数据集，反之则为完备数据集
	- 问题类型：
		- 学习问题：也即求最大似然估计$\hat \theta$
		- 推断问题：已知参数$\theta$，利用可观测变量集合推断隐变量集合的概率$p(z|x,\theta)$
- 期望最大化算法(EM)：
	- 问题：对上述的含有对数内求和的似然函数，直接计算似然$\max_\theta\log p (x|\theta)$是不可行的（是无监督学习的过程），因此，引入参数的分布函数$q(z)$，从而将标签$z$的似然函数边际化，即求$\max_\theta\sum_zq(z)\log p(x,z|\theta)$（转换为有监督学习过程）
	- 证据下界：选定一个变分分布参数$q(z)$：$$\begin{align}\log p(x|\theta)&=\log [\sum_zp(x,z|\theta)]\\ &=\log[\sum_zq(z)(\frac{p(x,z|\theta)}{q(z)})]\\ &\geq \sum_zq(z)\log(\frac{p(x,z|\theta)}{q(z)})=L(q,\theta)\\ \end{align}$$求得的$L(q,\theta)$是原函数的一个下界，称为证据下界
		- 该下界是可以通过算法迭代求解的函数，因此通过求下界的最大值，可以求出原似然函数的最大值
	- 算法：选定初始的模型参数$\theta^{\text{old}}$，先求得一个使得证据下界最大化的$q^*=\arg\max_qL(q,\theta^{\text{old}})$，再迭代求解新的模型参数$\theta^{\text{new}}=\arg\max_\theta(q^*,\theta)$
		- $KL$散度：衡量隐变量的变分分布（先验分布）和后验分布之间的距离：$KL[q(z)||p(z)]=\sum_zq(z)\log(\frac{q(z)}{p(z)})$，$KL$散度具有非负性
		- 可以证明，$\log p(x|\theta)=L(q,\theta)+KL[q(z)||p(z|x,\theta)]$，也即当KL散度取得最小值时，得到的最大化的证据下界函数一定是似然函数的值
		- 计算最大后验估计和最大似然估计的过程是类似的，都可以用EM算法求解
	- 高斯混合模型的EM算法：
		- 参数：$\pmb{\pi, \mu, \Sigma}$；隐变量：单个数据的归属$\pmb x_i$，定义向量$(\gamma_i^1,\gamma_i^2,\cdots,\gamma_i^k)$为先验的变分分布
		- E步骤：可以计算得到最大的变分分布为：$$\gamma_i^j=\frac{\pi_jN(\pmb x_i|\pmb \mu_j,\pmb \Sigma_j)}{\sum_{c=1}^k\pi_cN(\pmb x_i|\pmb \mu_c,\pmb \Sigma_c)}$$
		- M步骤：用最大似然估计的数学方式可以求得使其最大化的参数为：$$\pi_c^{\text{new}}=\frac{n_C}{n},\pmb \mu_C^{\text{new}}=\frac{1}{n_C}\sum_{i=1}^n\gamma_i^C\pmb x_i,\pmb\Sigma_C^{\text{new}}=\frac{1}{n_C}\sum_{i=1}^n\gamma_i^c(\pmb x_i-\pmb\mu_c^{\text{new}})(\pmb x_i-\pmb\mu_c^{\text{new}})^T$$
		- 例：![[Pasted image 20250519161552.png]]
---
[[L11 推理(C)]]