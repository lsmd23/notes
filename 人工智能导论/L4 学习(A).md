# 机器学习
- 通过已有数据自动学习的一段程序，而不是直接进行显式的编程
- 多学科的交叉技术，有广泛的应用场景
## 基本框架
- 例：垃圾信息过滤——将文本信息分类
	- 特征提取：基于学习或非学习的方式，将文本信息的关键特征进行提取建模，生成**特征向量**(feature vector)
	- 问题形式化：
		- 输入一个特征向量，值域为分类的标准（例如{0,1}域），存在一个由特征向量到值域的目标函数
		- 机器学习需要用已有的一些数据，提出一个尽可能拟合目标函数的**假设函数$h:X\rightarrow y$**
		- 核心模块：假设空间与学习算法
- 假设空间：各个假设函数组成的空间$\mathcal{H}$
	- 希望假设空间内的函数具有相对较好的性质，例如连续性，光滑性，简单性等等，统称为假设函数的正则性
	- 损失函数：用于评估假设函数和实际结果的符合程度，寻找最符合的假设函数$h$
		- 拟合误差：用于评估一组参数对应的拟合函数的好坏，对应的是这组参数的函数![[Pasted image 20250317160102.png]]
		- 拟合结果：
			- 拟合的结果可能有过拟合或者欠拟合的情况
			- 欠拟合的结果可能导致模型不能适配多数的任务需求，难以应用
			- 过拟合的结果导致模型难以进行任务的扩展，仅能适配训练集的结果，也会导致算力的浪费等
			- ![[Pasted image 20250317174802.png]]
- 学习的分类：
	- 监督学习：每组数据的输出都完全已知
	- 无监督学习：训练数据的输出未知
		- 一般而言，无监督学习常用于预训练阶段，监督学习用于后训练阶段
	- 强化学习：利用与环境的交互反馈进行学习，向符合结果的最好方向移动
		- ![[Pasted image 20250317155802.png]]
		- 需要定义状态，状态转移与奖励函数
## 评估指标
- 对二分类问题，存在四种结果：![[Pasted image 20250317160431.png]]
	- 准确率：$\frac{TP+TN}{TP+TN+FP+FN}$表示结果的准确程度
	- 查准率（精确率）：$\frac{TP}{TP+FP}$表示预测正确的结果中，有多少是真正正确的结果
	- 查全率（召回率）：$\frac{TP}{TP+FN}$表示有多少真正正确的结果被预测为正确的
	- 混淆矩阵：将二分类问题推广到n分类问题，可以得到相关的n阶混淆矩阵
- ROC曲线与AUC:
	- ROC曲线展示分类模型在不同分类阈值下的性能，通过绘制真正率（TPR）和假正率（FPR）的关系来反映模型的权衡能力。
		- $TPR=\frac{TP}{TP+FN}$，$FPR=\frac{FP}{FP+TN}$
		- AUC：​ROC曲线下的面积，用于衡量模型区分正负样本的能力。AUC值越大，模型性能越好
		- 如图：![[Pasted image 20250317183615.png]]
			- 曲线越靠近左上角，表示模型性能越好
			- 对角线（TPR=FPR）表示随机猜测模型，AUC=0.5
- 回归评价指标：
	- 平方指标：$(y_i-\hat{y_i})^2$
	- 绝对值指标：$|y_i-\hat{y_i}|$
	- 对数指标：$(\log(1+y_i)-\log(1+\hat{y_i}))^2$
	- 相对绝对值指标：$|\frac{y_i-\hat{y_i}}{y_i}|$
	- 根据离群点的性质，数据的性质选择合适的连续的回归指标作为分析依据
## 模型选择
- 模型与现实结果依然存在误差，对不同的问题，需要选择不同的学习方式对应不同的模型![[Pasted image 20250317162847.png]]
- 模型的测试：
	- 将数据集分为三类：训练集，验证集，测试集![[Pasted image 20250317163514.png]]
	- 训练集用于训练模型，找到一组对应的参数，验证集用于验证过程，调整得到更好的参数
	- 对应的，测试集对模型完全不可见，用于模型训练后对模型的测试
	- 交叉验证：将训练集和测试集混合，按不同方式分割，进行训练和拟合，从而得到一组参数。将不同分割方式相互平均，得到一组训练结果![[Pasted image 20250317163723.png]]
	- 自动化参数调试：将调参的过程写成一组自动化的代码：![[Pasted image 20250317175349.png]]
		- 复杂度过高，没有实际应用价值
		- 结合实际的调参经验，可以加速这一过程
# 线性回归模型
- 线性函数是正则性极好的一组函数
- 训练数据：一组$d+1$维向量：$$\begin{pmatrix}1\\x_1\\\vdots\\x_d\end{pmatrix}$$
	- 增加的一个维度用于形式化线性假设空间与假设函数的截距数据
- 假设空间与假设函数：
	- 假设函数可以表示成两个向量的内积：$$h(x)=\sum_{j=0}^d{w_jx_j}=\pmb{w\cdot x}$$
	- 使用最小平方估计：$$\hat{\epsilon}=\sum_{i=1}^n{(h(\pmb{x}_i)-y_i)^2}$$
	- 需要找到一组参数$w$，使得这组估计值最小，即求：$$\min_w\sum_{i=1}^n{(h_w(\pmb{x}_i)-y_i)^2}$$
## 优化算法
- **梯度下降算法**——局部贪心搜索算法 ^d205b8
	- 类似牛顿法的过程
	- 每次求某点的梯度值，向梯度下降的方向移动一个步长，得到新的参数值
		- 数学表达式：$$\pmb{w}^{t+1}\leftarrow \pmb{w}^{t}-2\eta \pmb{X}^T(\pmb{Xw}^t-\pmb{y})$$其中$\eta$为梯度下降步长，$\pmb{X}$为对应的数据向量所构成的d行n列矩阵
		- 梯度下降步长$\eta$影响结果是否收敛，因此这个数据影响算法的正确性
	- 收敛速度：$O(1/\sqrt{T})$
	- 复杂度分析：$O(dnT)$
	- 优化——**随机梯度下降算法**:
		- 当样本数据规模过大时，一个对n线性的复杂度是难以接受的
		- 每次在n个点中，选取m个点作为计算标准，将d行n列矩阵缩小为d行m列，复杂度降低到$O(dmT)$ $$\pmb{w}^{t+1}\leftarrow \pmb{w}^{t}-2\eta \pmb{X}_m^T(\pmb{X}_m\pmb{w}^t-\pmb{y})$$
		- 性质：
			- 梯度下降的速度受取样数值m的影响，m取样越大，迭代一轮需要的时间越小，但迭代一次需要的时间较大
			- 逃离鞍点：可以逃离梯度为零的鞍点值，更好的寻找最小值
## 非线性化
- 基函数与特征映射：
	- 背景：实际问题中输出往往和输入不是简单的线性关系
	- 基函数：定义在样本向量$\pmb{x}$上的函数$\phi_j(\pmb{x})$，用于对线性的结果进行扩展
	- 特征映射与空间：一组这样的函数$\Phi$将原来的$d$维样本向量映射为$\tilde{d}$维的向量$\pmb{z}$
		- 例：$\pmb{z}'=(1,x_2,x_1^2,x_1x_2)$
		- 径向基函数：以高斯函数的方式描述的基函数$$\phi_j(x)=\{\exp{(\frac{-||\pmb{x}-\pmb{\mu}_j||_2^2}{2\sigma^2}}):j=1,...,\tilde{d}\}$$描述的是一个向量在某个分量上的均值附近的集中程度
- 利用基函数的线性拟合：
	- 利用特征函数，将非线性问题线性化
		- 选取假设函数：$$h(\pmb{x})=\tilde{\pmb{w}}\cdot\pmb{z}=\tilde{\pmb{w}}\cdot\Phi(\pmb{x})=\sum_{j=1}^{\tilde{d}}\tilde{w_j}\phi_j(\pmb{x})$$
		- 这样的函数拟合了非线性的结果
	- 问题：当特征函数选取不合适，映射的特征空间过大时，容易发生过拟合（尤其在使用多项式函数做拟合时）
## 正则化
- 拟合的最佳值：![[Pasted image 20250324160157.png]]
	- 当模型复杂度低时，发生欠拟合，测试误差和训练误差同步增大
	- 当模型复杂度高时，发生过拟合，训练误差减小，测试误差增大，二者差距增大
- 二范数正则化线性回归：在损失函数中添加二范数项
	- 参数向量：$$\pmb{\hat{w}}=\arg \min_{\pmb{w}\in \mathbb{R}^d}\sum_{i=1}^n(\pmb{w}^T\pmb{x}_i-y_i)^2+\lambda ||\pmb{w}||_2^2$$
	- 解释：$||\pmb{w}||_2^2$项指在线性空间中到原点的欧氏距离，当$\lambda$越大时，这一距离的考量权值越大，对解的约束性越强，解分布越靠近原点
		- 在二维平面上：表示对应的椭圆等值线和圆心为原点的圆的交点![[Pasted image 20250324194505.png]]
- 一范数正则化线性回归：将二范数改为一范数即可，即为绝对值之和
	- 性质：解的稀疏性——由于一范数对应的空间是一个存在尖点的空间，因此，用一范数正则化的结果中，会有很多维度的参数为零
		- 特征筛选：筛选出了关键性的特征维度，排除掉了无用的维度
		- 运算效率提升：稀疏的解向量提高了后续运算的效率和速度
# 线性分类
- 分类问题的输出是离散的点，而不是线性回归那样的连续的值。因此，如果选用线性回归的方式进行分类问题，虽然可以解决，但并不是最佳的解决方式
- 阶跃函数与离散化：
	- 选取阈值边界，按照阈值将连续的输出映射到离散的域上，从而符合分类问题的要求
	- 但此时，复合后的函数成为不可微的函数，难以使用梯度下降的方法进行拟合
	- 激活函数——sigmoid函数：优化阶跃函数，将其转化为可求导的函数
		- 形式：$$\sigma(t)=\frac{1}{1+e^{-t}}$$其中$t$表示温度参数，用于控制函数的阶跃特性的大小
		- 图像：用蓝色曲线拟合红色曲线![[Pasted image 20250324161755.png]]
- 交叉熵损失函数：
	- 上述的建模过程是难以直接解释的
	- 引入概率的方法，建模条件概率密度函数，试图用激活函数与回归函数的复合值拟合这样的概率密度函数
	- 交叉熵损失函数：定义：$$l(h(\pmb{x}_i),y_i)=\begin{cases}-\log[\sigma(\pmb w^T\pmb x_i)]  & y_i=1 \\-\log[1-\sigma(\pmb w^T\pmb x_i)]& y_i=0\\ \end{cases}$$
- “逻辑”回归模型与正则化：
	- 和连续变量类似，用交叉熵损失函数替换原来的平方损失函数
		- 基本原理来自最大似然估计的推导过程，此处省略
	- 正则化：同样可以加入二范数的正则项，不仅可以避免过拟合，同时可以避免损失函数失去全局最小值的性质![[Pasted image 20250324163042.png]]
- 多分类问题：
	- 拓展多个维度的函数，每一项的输出值作为概率分布的权值
	- Softmax函数：对多分类问题的条件概率的激活函数，使之连续可微化
		- 形式：$$p(y=i|\pmb x)=\frac{\exp(\pmb w_i^T\pmb x)}{\sum_{r=1}^C\exp(\pmb w_r^T\pmb x)}$$
		- 解释：将选取的概率用连续函数的方式给出，适合线性模型学习
	- 交叉熵损失函数：
		- 形式：$$l(h(\pmb{x}_i),y_i)=\begin{cases}-\log[\frac{\exp(\pmb w_1^T\pmb x)}{\sum_{r=1}^C\exp(\pmb w_r^T\pmb x)}]  & y_i=1 \\-\log[\frac{\exp(\pmb w_2^T\pmb x)}{\sum_{r=1}^C\exp(\pmb w_r^T\pmb x)}]& y_i=2\\ \vdots\\ -\log[\frac{\exp(\pmb w_C^T\pmb x)}{\sum_{r=1}^C\exp(\pmb w_r^T\pmb x)}]& y_i=C \end{cases}$$
		- 缺点：多解性——解为一个向量族
			- 解决方式：范数正则化，可以选取出最优化的解
# 总结
![[Pasted image 20250324163918.png]]

---
[[L5 学习(B)]]