# 决策树
- 问题：实际的样本中，会出现数据的异构性，即：有些分量是连续的，另一些分量却是离散的，而离散的变量也分有序和无序等等很多类，一般的深度学习模型难以处理异构性高的数据
- 背景：人在决策分类时，往往依照不同特征，按照树状结构进行分类，而不是一次将所有的特征纳入考虑之中
- 决策树模型：![[Pasted image 20250324165539.png]]
## ID3算法
- 数据集的划分：
	- 根据某一特征，可以向下延伸决策树，将数据集划分成对应的子集
	- 在决策树模型中，存在对**更均匀的划分**与**更纯粹的划分**的倾向程度，称为**归纳偏好**
	- 度量划分的纯粹度：
		- 错误率：$$\text{Err}(D)=1-\max_{1\leq k\leq K}(\frac{C_k}{D})$$
		- 熵函数：$$H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log\frac{|C_k|}{|D|}$$
		- 基尼系数：$$\text{Gini}(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2$$
	- 划分的信息增益：表示一次划分对整个分类问题的效益
		- 定义：父数据集和子数据集熵函数加权平均的差值$$IG=H(D_1\cup D_2)-\frac{|D_1|}{|D|}H(D_1)-\frac{|D_2|}{|D|}H(D_2)$$
		- 刻画一次划分的熵减程度，可以根据划分的结果以及数据集的原状态计算得到
- ID3算法：
	- 基于贪心思想实现的划分算法
	- 描述：
		- 创建代表全体数据的根节点
		- 遍历所有划分方式，选择使得信息增益最大的划分方式
		- 按这样的划分方式生长决策树，创建对应的叶子节点
		- 停止条件：叶子节点内的数据归为同一类，或所有的划分标准已经用完
	- 伪码实现：![[Pasted image 20250331154611.png]]
	- 复杂度：
		- 对每层，复杂度为$O(dn)$
		- 树的深度为划分维度$d$和$\log n$的最小值
	- 问题：
		- 无法处理特征之间的不同重要性
		- 信息增益函数在某些时候无法正确刻画分组的正确性，容易出现过拟合（即分组过多的情况）
## C4.5算法
- 正则化方法：为处理信息增益函数的过拟合（分组过多）情况，增加惩罚函数
	- 固有值：$$\text{IV}(f)=-\sum_{i=1}^{|V|}\frac{|D_i|}{|D|}\log\frac{|D_i|}{|D|}$$用于衡量划分的多少
	- 增益率：$\text{Gain Ratio}=\frac{IG}{IV}$
- 连续分布特征：经典的决策树模型划分只能用离散的取值对应的特征进行划分
	- 阈值划分：在有序的连续型数据的离散数据点中，取一个值进行划分
	- 传统上习惯取中位数据的中值阈值进行划分，将整个数据划分成尽量相等的两部分
- 树的剪枝：
	- 对决策树模型，树的子节点过多，意味着过拟合的情况。因此，及时对树剪枝可以防止过拟合的发生
	- 预剪枝：在树的生长过程中剪枝
		- 对于每次分裂，使用验证集上准确率的变化来衡量这次分裂的必要性
		- 如果准确率增长不超过L甚至下降，则不进行这次分裂
		- 缺点：是一种基于贪心的策略，不一定适用（无法预知后面的分裂）
	- 后剪枝：预剪枝可能导致过度剪枝引起欠拟合
		- 后剪枝在训练后对树进行修剪
		- 过程：从叶节点到根节点回溯，使用验证集上准确性的变化来衡量对该节点进行修剪的必要性
- 决策树的损失函数：
	- $$\begin{align}C_\alpha(T)=&\hat{\epsilon}(T)+\alpha|T|=\sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|\\=&-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}\log\frac{N_{tk}}{N_t}+\alpha|T|\end{align}$$
	- 说明：$\hat{\epsilon}(T)$——验证集上错误率，$|T|$叶子节点数，$N_{tk}$在叶子节点中的分类的最大投票
	- 剪枝过程中，可以采用损失函数进行评估，若损失函数可以减小则剪枝，反之则不剪枝
## 理解决策树
- 对由不同维度上特征构成的特征向量，其组成一个高维的假设空间
- 决策树的生长过程，就是对假设空间进行划分的过程，由于其每次只在一个特征上做决策，因此其是在平行于坐标轴的方向上做划分
- 相比线性的回归模型，决策树模型是典型的非线性模型；且由于决策树是可生长的，假设空间的划分可以做到比较细致；通过合适的正则化，可以控制其不发生过拟合
- 假设函数：
	- 表达式：$$h(\pmb x)\sum_{i=1}^mc_i\pmb 1\{\pmb x\in R_i\}$$
	- 基函数：$$\pmb x\rightarrow \pmb{\Phi(x)} =(\pmb1\{\pmb x\in R_1\},\cdots,\pmb1\{\pmb x\in R_m\})$$
	- 决策树可以视为对这样的基函数的线性拟合，由于这组基函数显然不可微，故无法使用梯度下降的学习方法
- 与线性模型的对比：
	- 可解释性一般更强（在树经过合适的剪枝且拟合程度较好下）
	- 拟合性视情况而定（当数据集和决策树对应的基函数的情况更加符合时拟合情况更好）
	- 线性模型对数据特征的量纲十分敏感，很难处理不同量纲数据之间的情况，当数据维度很高时，严重限制了模型的应用情况
# 随机森林
- 单棵决策树的表达能力有限，因此试图采取多棵决策树的方式对数据进行学习
- 随机地选取出数棵决策树的方式，即为随机森林
## 装袋法
- 自助采样法：
	- 思想：对一个大的数据集，每次进行n次的有放回采样，得到多个子数据集
	- 采样的概率分析：由概率论知识可得，对大规模的数据进行$n$次采样，即使$n\rightarrow \infty$，依然有$(1-\frac{1}{n})^n=\frac{1}{e}=0.368$的数据可能不被选取
	- 自助采样法（自举法）：对一组规模很大的数据集$P$，选取规模为$n$的$B$组数据$D_n^1,\cdots,D_n^B$，计算对应的函数学习与验证
- 自助聚合法（装袋法）：
	- 对上述自举法的B个数据集，可以分别学习得到B个假设函数，记为$h_{D_n^1},\cdots,h_{D_n^B}$
	- 装袋后的假设函数为：$h_{\text{bag}}(x)=\text{Combine}(h_{D_n^1}(\pmb x),\cdots,h_{D_n^B}(\pmb x))$
	- 统计意义上，装袋函数降低了统计估计的方差
- 袋外验证(OOB)：
	- 由于恰好有大约37%的数据是不会被装袋的，可以用于对结果的验证
	- 袋外验证的假设函数为在不含数据点上的平均值：$$h_{OOB}(\pmb x_i) = \frac{1}{|S_i|} \sum_{b \in S_i} h_{D_n^b}(\pmb x_i)$$
## Breiman算法
- 问题：如果每棵决策树都在同样的特征维度上生长，其随机程度依然不高
	- 解决：对特征维度也随机选取出$K$个用于决策树的生长，经验表明$K=\sqrt{d}$时效果较好
- 算法：
	- 伪码实现：![[Pasted image 20250331164450.png]]
- 偏差方差分解：![[Pasted image 20250401210357.png]]
## 总结
- 一般的决策树方法：构建非常深的树
	- 问题：验证误差低，测试误差高。也即出现过拟合程度高
- 鉴于单个决策树的多样性来自于自助采样与随机树构建的过程，因此在这两个方向增强随机化处理，也即随机森林的方式，可以解决这一问题
---
[[L6 学习(C)]]