# 决策树
- 问题：实际的样本中，会出现数据的异构性，即：有些分量是连续的，另一些分量却是离散的，而离散的变量也分有序和无序等等很多类，一般的深度学习模型难以处理异构性高的数据
- 背景：人在决策分类时，往往依照不同特征，按照树状结构进行分类，而不是一次将所有的特征纳入考虑之中
- 决策树模型：![[Pasted image 20250324165539.png]]
## ID3算法
- 数据集的划分：
	- 根据某一特征，可以向下延伸决策树，将数据集划分成对应的子集
	- 在决策树模型中，存在对**更均匀的划分**与**更纯粹的划分**的倾向程度，称为**归纳偏好**
	- 度量划分的纯粹度：
		- 错误率
		- 熵函数
		- 基尼系数
	- 划分的信息增益：表示一次划分对整个分类问题的效益
		- 定义：
		- 刻画一次划分的熵减程度，可以根据划分的结果以及数据集的原状态计算得到
- ID3算法：
	- 基于贪心思想实现的划分算法
	- 描述：
		- 创建代表全体数据的根节点
		- 遍历所有划分方式，选择使得信息增益最大的划分方式
		- 按这样的划分方式生长决策树，创建对应的叶子节点
		- 停止条件：叶子节点内的数据归为同一类，或所有的划分标准已经用完
	- 伪码实现：![[Pasted image 20250331154611.png]]
	- 复杂度：
		- 
	- 问题：
		- 无法处理特征之间的不同重要性
		- 信息增益函数在某些时候无法正确刻画分组的正确性，容易出现过拟合
## C4.5算法
- 正则化方法：为处理信息增益函数的不足，增加惩罚函数使之更符合实际情况
	- 固有值：
	- 增益率：
- 连续分布特征：经典的决策树模型划分只能用离散的取值对应的特征进行划分
	- 阈值划分：
- 树的剪枝：
	- 预剪枝：
	- 后剪枝：
- 决策树的损失函数：
	- $$C_\alpha(T)$$
	- 说明：
	- 剪枝过程中，可以采用损失函数进行评估，若损失函数可以减小则剪枝，反之则不剪枝
## 理解决策树
- 对由不同维度上特征构成的特征向量，其组成一个高维的假设空间
- 决策树的生长过程，就是对假设空间进行划分的过程，由于其每次只在一个特征上做决策，因此其是在平行于坐标轴的方向上做划分
- 相比线性的回归模型，决策树模型是典型的非线性模型；且由于决策树是可生长的，假设空间的划分可以做到比较细致；通过合适的正则化，可以控制其不发生过拟合
- 假设函数：
	- 表达式：$$h(\pmb x)\sum_{i=1}^mc_i\pmb 1\{\pmb x\in R_i\}$$
- 与线性模型的对比：
	- 可解释性一般更强（在树经过合适的剪枝且拟合程度较好下）
	- 拟合性视情况而定（当数据集和决策树对应的基函数的情况更加符合时拟合情况更好）
	- 线性模型对数据特征的量纲十分敏感，很难处理不同量纲数据之间的情况，当数据维度很高时，严重限制了模型的应用情况
# 随机森林
- 单棵决策树的表达能力有限，
## 装袋法
- 自助采样法：
	- 思想：对一个大的数据集，每次进行n次的有放回采样，得到多个子数据集
	- 采样的概率分析：
- 自助聚合法：
	- dd
- 袋外验证(OOB)：
	- 
## Breiman算法
- 问题：
- 算法：
	- 伪码实现：![[Pasted image 20250331164450.png]]
- 偏差方差分解：